#loc = loc(unknown)
#loc1 = loc("params_and_buffers['vllm_model.language_model.model.embed_tokens.weight']")
#loc2 = loc("params_and_buffers['vllm_model.language_model.model.layers.0.input_layernorm.weight']")
#loc3 = loc("params_and_buffers['vllm_model.language_model.model.layers.0.mlp.experts.w13_weight']")
#loc4 = loc("params_and_buffers['vllm_model.language_model.model.layers.0.mlp.experts.w2_weight']")
#loc5 = loc("params_and_buffers['vllm_model.language_model.model.layers.0.mlp.gate.weight']")
#loc6 = loc("params_and_buffers['vllm_model.language_model.model.layers.0.post_attention_layernorm.weight']")
#loc7 = loc("params_and_buffers['vllm_model.language_model.model.layers.0.self_attn.k_norm.weight']")
#loc8 = loc("params_and_buffers['vllm_model.language_model.model.layers.0.self_attn.o_proj.weight']")
#loc9 = loc("params_and_buffers['vllm_model.language_model.model.layers.0.self_attn.q_norm.weight']")
#loc10 = loc("params_and_buffers['vllm_model.language_model.model.layers.0.self_attn.qkv_proj.weight']")
#loc11 = loc("params_and_buffers['vllm_model.language_model.model.layers.0.self_attn.rotary_emb.cos_sin_cache']")
#loc12 = loc("params_and_buffers['vllm_model.language_model.model.layers.1.input_layernorm.weight']")
#loc13 = loc("params_and_buffers['vllm_model.language_model.model.layers.1.mlp.experts.w13_weight']")
#loc14 = loc("params_and_buffers['vllm_model.language_model.model.layers.1.mlp.experts.w2_weight']")
#loc15 = loc("params_and_buffers['vllm_model.language_model.model.layers.1.mlp.gate.weight']")
#loc16 = loc("params_and_buffers['vllm_model.language_model.model.layers.1.post_attention_layernorm.weight']")
#loc17 = loc("params_and_buffers['vllm_model.language_model.model.layers.1.self_attn.k_norm.weight']")
#loc18 = loc("params_and_buffers['vllm_model.language_model.model.layers.1.self_attn.o_proj.weight']")
#loc19 = loc("params_and_buffers['vllm_model.language_model.model.layers.1.self_attn.q_norm.weight']")
#loc20 = loc("params_and_buffers['vllm_model.language_model.model.layers.1.self_attn.qkv_proj.weight']")
#loc21 = loc("params_and_buffers['vllm_model.language_model.model.layers.10.input_layernorm.weight']")
#loc22 = loc("params_and_buffers['vllm_model.language_model.model.layers.10.mlp.experts.w13_weight']")
#loc23 = loc("params_and_buffers['vllm_model.language_model.model.layers.10.mlp.experts.w2_weight']")
#loc24 = loc("params_and_buffers['vllm_model.language_model.model.layers.10.mlp.gate.weight']")
#loc25 = loc("params_and_buffers['vllm_model.language_model.model.layers.10.post_attention_layernorm.weight']")
#loc26 = loc("params_and_buffers['vllm_model.language_model.model.layers.10.self_attn.k_norm.weight']")
#loc27 = loc("params_and_buffers['vllm_model.language_model.model.layers.10.self_attn.o_proj.weight']")
#loc28 = loc("params_and_buffers['vllm_model.language_model.model.layers.10.self_attn.q_norm.weight']")
#loc29 = loc("params_and_buffers['vllm_model.language_model.model.layers.10.self_attn.qkv_proj.weight']")
#loc30 = loc("params_and_buffers['vllm_model.language_model.model.layers.11.input_layernorm.weight']")
#loc31 = loc("params_and_buffers['vllm_model.language_model.model.layers.11.mlp.experts.w13_weight']")
#loc32 = loc("params_and_buffers['vllm_model.language_model.model.layers.11.mlp.experts.w2_weight']")
#loc33 = loc("params_and_buffers['vllm_model.language_model.model.layers.11.mlp.gate.weight']")
#loc34 = loc("params_and_buffers['vllm_model.language_model.model.layers.11.post_attention_layernorm.weight']")
#loc35 = loc("params_and_buffers['vllm_model.language_model.model.layers.11.self_attn.k_norm.weight']")
#loc36 = loc("params_and_buffers['vllm_model.language_model.model.layers.11.self_attn.o_proj.weight']")
#loc37 = loc("params_and_buffers['vllm_model.language_model.model.layers.11.self_attn.q_norm.weight']")
#loc38 = loc("params_and_buffers['vllm_model.language_model.model.layers.11.self_attn.qkv_proj.weight']")
#loc39 = loc("params_and_buffers['vllm_model.language_model.model.layers.12.input_layernorm.weight']")
#loc40 = loc("params_and_buffers['vllm_model.language_model.model.layers.12.mlp.experts.w13_weight']")
#loc41 = loc("params_and_buffers['vllm_model.language_model.model.layers.12.mlp.experts.w2_weight']")
#loc42 = loc("params_and_buffers['vllm_model.language_model.model.layers.12.mlp.gate.weight']")
#loc43 = loc("params_and_buffers['vllm_model.language_model.model.layers.12.post_attention_layernorm.weight']")
#loc44 = loc("params_and_buffers['vllm_model.language_model.model.layers.12.self_attn.k_norm.weight']")
#loc45 = loc("params_and_buffers['vllm_model.language_model.model.layers.12.self_attn.o_proj.weight']")
#loc46 = loc("params_and_buffers['vllm_model.language_model.model.layers.12.self_attn.q_norm.weight']")
#loc47 = loc("params_and_buffers['vllm_model.language_model.model.layers.12.self_attn.qkv_proj.weight']")
#loc48 = loc("params_and_buffers['vllm_model.language_model.model.layers.13.input_layernorm.weight']")
#loc49 = loc("params_and_buffers['vllm_model.language_model.model.layers.13.mlp.experts.w13_weight']")
#loc50 = loc("params_and_buffers['vllm_model.language_model.model.layers.13.mlp.experts.w2_weight']")
#loc51 = loc("params_and_buffers['vllm_model.language_model.model.layers.13.mlp.gate.weight']")
#loc52 = loc("params_and_buffers['vllm_model.language_model.model.layers.13.post_attention_layernorm.weight']")
#loc53 = loc("params_and_buffers['vllm_model.language_model.model.layers.13.self_attn.k_norm.weight']")
#loc54 = loc("params_and_buffers['vllm_model.language_model.model.layers.13.self_attn.o_proj.weight']")
#loc55 = loc("params_and_buffers['vllm_model.language_model.model.layers.13.self_attn.q_norm.weight']")
#loc56 = loc("params_and_buffers['vllm_model.language_model.model.layers.13.self_attn.qkv_proj.weight']")
#loc57 = loc("params_and_buffers['vllm_model.language_model.model.layers.14.input_layernorm.weight']")
#loc58 = loc("params_and_buffers['vllm_model.language_model.model.layers.14.mlp.experts.w13_weight']")
#loc59 = loc("params_and_buffers['vllm_model.language_model.model.layers.14.mlp.experts.w2_weight']")
#loc60 = loc("params_and_buffers['vllm_model.language_model.model.layers.14.mlp.gate.weight']")
#loc61 = loc("params_and_buffers['vllm_model.language_model.model.layers.14.post_attention_layernorm.weight']")
#loc62 = loc("params_and_buffers['vllm_model.language_model.model.layers.14.self_attn.k_norm.weight']")
#loc63 = loc("params_and_buffers['vllm_model.language_model.model.layers.14.self_attn.o_proj.weight']")
#loc64 = loc("params_and_buffers['vllm_model.language_model.model.layers.14.self_attn.q_norm.weight']")
#loc65 = loc("params_and_buffers['vllm_model.language_model.model.layers.14.self_attn.qkv_proj.weight']")
#loc66 = loc("params_and_buffers['vllm_model.language_model.model.layers.15.input_layernorm.weight']")
#loc67 = loc("params_and_buffers['vllm_model.language_model.model.layers.15.mlp.experts.w13_weight']")
#loc68 = loc("params_and_buffers['vllm_model.language_model.model.layers.15.mlp.experts.w2_weight']")
#loc69 = loc("params_and_buffers['vllm_model.language_model.model.layers.15.mlp.gate.weight']")
#loc70 = loc("params_and_buffers['vllm_model.language_model.model.layers.15.post_attention_layernorm.weight']")
#loc71 = loc("params_and_buffers['vllm_model.language_model.model.layers.15.self_attn.k_norm.weight']")
#loc72 = loc("params_and_buffers['vllm_model.language_model.model.layers.15.self_attn.o_proj.weight']")
#loc73 = loc("params_and_buffers['vllm_model.language_model.model.layers.15.self_attn.q_norm.weight']")
#loc74 = loc("params_and_buffers['vllm_model.language_model.model.layers.15.self_attn.qkv_proj.weight']")
#loc75 = loc("params_and_buffers['vllm_model.language_model.model.layers.16.input_layernorm.weight']")
#loc76 = loc("params_and_buffers['vllm_model.language_model.model.layers.16.mlp.experts.w13_weight']")
#loc77 = loc("params_and_buffers['vllm_model.language_model.model.layers.16.mlp.experts.w2_weight']")
#loc78 = loc("params_and_buffers['vllm_model.language_model.model.layers.16.mlp.gate.weight']")
#loc79 = loc("params_and_buffers['vllm_model.language_model.model.layers.16.post_attention_layernorm.weight']")
#loc80 = loc("params_and_buffers['vllm_model.language_model.model.layers.16.self_attn.k_norm.weight']")
#loc81 = loc("params_and_buffers['vllm_model.language_model.model.layers.16.self_attn.o_proj.weight']")
#loc82 = loc("params_and_buffers['vllm_model.language_model.model.layers.16.self_attn.q_norm.weight']")
#loc83 = loc("params_and_buffers['vllm_model.language_model.model.layers.16.self_attn.qkv_proj.weight']")
#loc84 = loc("params_and_buffers['vllm_model.language_model.model.layers.17.input_layernorm.weight']")
#loc85 = loc("params_and_buffers['vllm_model.language_model.model.layers.17.mlp.experts.w13_weight']")
#loc86 = loc("params_and_buffers['vllm_model.language_model.model.layers.17.mlp.experts.w2_weight']")
#loc87 = loc("params_and_buffers['vllm_model.language_model.model.layers.17.mlp.gate.weight']")
#loc88 = loc("params_and_buffers['vllm_model.language_model.model.layers.17.post_attention_layernorm.weight']")
#loc89 = loc("params_and_buffers['vllm_model.language_model.model.layers.17.self_attn.k_norm.weight']")
#loc90 = loc("params_and_buffers['vllm_model.language_model.model.layers.17.self_attn.o_proj.weight']")
#loc91 = loc("params_and_buffers['vllm_model.language_model.model.layers.17.self_attn.q_norm.weight']")
#loc92 = loc("params_and_buffers['vllm_model.language_model.model.layers.17.self_attn.qkv_proj.weight']")
#loc93 = loc("params_and_buffers['vllm_model.language_model.model.layers.18.input_layernorm.weight']")
#loc94 = loc("params_and_buffers['vllm_model.language_model.model.layers.18.mlp.experts.w13_weight']")
#loc95 = loc("params_and_buffers['vllm_model.language_model.model.layers.18.mlp.experts.w2_weight']")
#loc96 = loc("params_and_buffers['vllm_model.language_model.model.layers.18.mlp.gate.weight']")
#loc97 = loc("params_and_buffers['vllm_model.language_model.model.layers.18.post_attention_layernorm.weight']")
#loc98 = loc("params_and_buffers['vllm_model.language_model.model.layers.18.self_attn.k_norm.weight']")
#loc99 = loc("params_and_buffers['vllm_model.language_model.model.layers.18.self_attn.o_proj.weight']")
#loc100 = loc("params_and_buffers['vllm_model.language_model.model.layers.18.self_attn.q_norm.weight']")
#loc101 = loc("params_and_buffers['vllm_model.language_model.model.layers.18.self_attn.qkv_proj.weight']")
#loc102 = loc("params_and_buffers['vllm_model.language_model.model.layers.19.input_layernorm.weight']")
#loc103 = loc("params_and_buffers['vllm_model.language_model.model.layers.19.mlp.experts.w13_weight']")
#loc104 = loc("params_and_buffers['vllm_model.language_model.model.layers.19.mlp.experts.w2_weight']")
#loc105 = loc("params_and_buffers['vllm_model.language_model.model.layers.19.mlp.gate.weight']")
#loc106 = loc("params_and_buffers['vllm_model.language_model.model.layers.19.post_attention_layernorm.weight']")
#loc107 = loc("params_and_buffers['vllm_model.language_model.model.layers.19.self_attn.k_norm.weight']")
#loc108 = loc("params_and_buffers['vllm_model.language_model.model.layers.19.self_attn.o_proj.weight']")
#loc109 = loc("params_and_buffers['vllm_model.language_model.model.layers.19.self_attn.q_norm.weight']")
#loc110 = loc("params_and_buffers['vllm_model.language_model.model.layers.19.self_attn.qkv_proj.weight']")
#loc111 = loc("params_and_buffers['vllm_model.language_model.model.layers.2.input_layernorm.weight']")
#loc112 = loc("params_and_buffers['vllm_model.language_model.model.layers.2.mlp.experts.w13_weight']")
#loc113 = loc("params_and_buffers['vllm_model.language_model.model.layers.2.mlp.experts.w2_weight']")
#loc114 = loc("params_and_buffers['vllm_model.language_model.model.layers.2.mlp.gate.weight']")
#loc115 = loc("params_and_buffers['vllm_model.language_model.model.layers.2.post_attention_layernorm.weight']")
#loc116 = loc("params_and_buffers['vllm_model.language_model.model.layers.2.self_attn.k_norm.weight']")
#loc117 = loc("params_and_buffers['vllm_model.language_model.model.layers.2.self_attn.o_proj.weight']")
#loc118 = loc("params_and_buffers['vllm_model.language_model.model.layers.2.self_attn.q_norm.weight']")
#loc119 = loc("params_and_buffers['vllm_model.language_model.model.layers.2.self_attn.qkv_proj.weight']")
#loc120 = loc("params_and_buffers['vllm_model.language_model.model.layers.20.input_layernorm.weight']")
#loc121 = loc("params_and_buffers['vllm_model.language_model.model.layers.20.mlp.experts.w13_weight']")
#loc122 = loc("params_and_buffers['vllm_model.language_model.model.layers.20.mlp.experts.w2_weight']")
#loc123 = loc("params_and_buffers['vllm_model.language_model.model.layers.20.mlp.gate.weight']")
#loc124 = loc("params_and_buffers['vllm_model.language_model.model.layers.20.post_attention_layernorm.weight']")
#loc125 = loc("params_and_buffers['vllm_model.language_model.model.layers.20.self_attn.k_norm.weight']")
#loc126 = loc("params_and_buffers['vllm_model.language_model.model.layers.20.self_attn.o_proj.weight']")
#loc127 = loc("params_and_buffers['vllm_model.language_model.model.layers.20.self_attn.q_norm.weight']")
#loc128 = loc("params_and_buffers['vllm_model.language_model.model.layers.20.self_attn.qkv_proj.weight']")
#loc129 = loc("params_and_buffers['vllm_model.language_model.model.layers.21.input_layernorm.weight']")
#loc130 = loc("params_and_buffers['vllm_model.language_model.model.layers.21.mlp.experts.w13_weight']")
#loc131 = loc("params_and_buffers['vllm_model.language_model.model.layers.21.mlp.experts.w2_weight']")
#loc132 = loc("params_and_buffers['vllm_model.language_model.model.layers.21.mlp.gate.weight']")
#loc133 = loc("params_and_buffers['vllm_model.language_model.model.layers.21.post_attention_layernorm.weight']")
#loc134 = loc("params_and_buffers['vllm_model.language_model.model.layers.21.self_attn.k_norm.weight']")
#loc135 = loc("params_and_buffers['vllm_model.language_model.model.layers.21.self_attn.o_proj.weight']")
#loc136 = loc("params_and_buffers['vllm_model.language_model.model.layers.21.self_attn.q_norm.weight']")
#loc137 = loc("params_and_buffers['vllm_model.language_model.model.layers.21.self_attn.qkv_proj.weight']")
#loc138 = loc("params_and_buffers['vllm_model.language_model.model.layers.22.input_layernorm.weight']")
#loc139 = loc("params_and_buffers['vllm_model.language_model.model.layers.22.mlp.experts.w13_weight']")
#loc140 = loc("params_and_buffers['vllm_model.language_model.model.layers.22.mlp.experts.w2_weight']")
#loc141 = loc("params_and_buffers['vllm_model.language_model.model.layers.22.mlp.gate.weight']")
#loc142 = loc("params_and_buffers['vllm_model.language_model.model.layers.22.post_attention_layernorm.weight']")
#loc143 = loc("params_and_buffers['vllm_model.language_model.model.layers.22.self_attn.k_norm.weight']")
#loc144 = loc("params_and_buffers['vllm_model.language_model.model.layers.22.self_attn.o_proj.weight']")
#loc145 = loc("params_and_buffers['vllm_model.language_model.model.layers.22.self_attn.q_norm.weight']")
#loc146 = loc("params_and_buffers['vllm_model.language_model.model.layers.22.self_attn.qkv_proj.weight']")
#loc147 = loc("params_and_buffers['vllm_model.language_model.model.layers.23.input_layernorm.weight']")
#loc148 = loc("params_and_buffers['vllm_model.language_model.model.layers.23.mlp.experts.w13_weight']")
#loc149 = loc("params_and_buffers['vllm_model.language_model.model.layers.23.mlp.experts.w2_weight']")
#loc150 = loc("params_and_buffers['vllm_model.language_model.model.layers.23.mlp.gate.weight']")
#loc151 = loc("params_and_buffers['vllm_model.language_model.model.layers.23.post_attention_layernorm.weight']")
#loc152 = loc("params_and_buffers['vllm_model.language_model.model.layers.23.self_attn.k_norm.weight']")
#loc153 = loc("params_and_buffers['vllm_model.language_model.model.layers.23.self_attn.o_proj.weight']")
#loc154 = loc("params_and_buffers['vllm_model.language_model.model.layers.23.self_attn.q_norm.weight']")
#loc155 = loc("params_and_buffers['vllm_model.language_model.model.layers.23.self_attn.qkv_proj.weight']")
#loc156 = loc("params_and_buffers['vllm_model.language_model.model.layers.24.input_layernorm.weight']")
#loc157 = loc("params_and_buffers['vllm_model.language_model.model.layers.24.mlp.experts.w13_weight']")
#loc158 = loc("params_and_buffers['vllm_model.language_model.model.layers.24.mlp.experts.w2_weight']")
#loc159 = loc("params_and_buffers['vllm_model.language_model.model.layers.24.mlp.gate.weight']")
#loc160 = loc("params_and_buffers['vllm_model.language_model.model.layers.24.post_attention_layernorm.weight']")
#loc161 = loc("params_and_buffers['vllm_model.language_model.model.layers.24.self_attn.k_norm.weight']")
#loc162 = loc("params_and_buffers['vllm_model.language_model.model.layers.24.self_attn.o_proj.weight']")
#loc163 = loc("params_and_buffers['vllm_model.language_model.model.layers.24.self_attn.q_norm.weight']")
#loc164 = loc("params_and_buffers['vllm_model.language_model.model.layers.24.self_attn.qkv_proj.weight']")
#loc165 = loc("params_and_buffers['vllm_model.language_model.model.layers.25.input_layernorm.weight']")
#loc166 = loc("params_and_buffers['vllm_model.language_model.model.layers.25.mlp.experts.w13_weight']")
#loc167 = loc("params_and_buffers['vllm_model.language_model.model.layers.25.mlp.experts.w2_weight']")
#loc168 = loc("params_and_buffers['vllm_model.language_model.model.layers.25.mlp.gate.weight']")
#loc169 = loc("params_and_buffers['vllm_model.language_model.model.layers.25.post_attention_layernorm.weight']")
#loc170 = loc("params_and_buffers['vllm_model.language_model.model.layers.25.self_attn.k_norm.weight']")
#loc171 = loc("params_and_buffers['vllm_model.language_model.model.layers.25.self_attn.o_proj.weight']")
#loc172 = loc("params_and_buffers['vllm_model.language_model.model.layers.25.self_attn.q_norm.weight']")
#loc173 = loc("params_and_buffers['vllm_model.language_model.model.layers.25.self_attn.qkv_proj.weight']")
#loc174 = loc("params_and_buffers['vllm_model.language_model.model.layers.26.input_layernorm.weight']")
#loc175 = loc("params_and_buffers['vllm_model.language_model.model.layers.26.mlp.experts.w13_weight']")
#loc176 = loc("params_and_buffers['vllm_model.language_model.model.layers.26.mlp.experts.w2_weight']")
#loc177 = loc("params_and_buffers['vllm_model.language_model.model.layers.26.mlp.gate.weight']")
#loc178 = loc("params_and_buffers['vllm_model.language_model.model.layers.26.post_attention_layernorm.weight']")
#loc179 = loc("params_and_buffers['vllm_model.language_model.model.layers.26.self_attn.k_norm.weight']")
#loc180 = loc("params_and_buffers['vllm_model.language_model.model.layers.26.self_attn.o_proj.weight']")
#loc181 = loc("params_and_buffers['vllm_model.language_model.model.layers.26.self_attn.q_norm.weight']")
#loc182 = loc("params_and_buffers['vllm_model.language_model.model.layers.26.self_attn.qkv_proj.weight']")
#loc183 = loc("params_and_buffers['vllm_model.language_model.model.layers.27.input_layernorm.weight']")
#loc184 = loc("params_and_buffers['vllm_model.language_model.model.layers.27.mlp.experts.w13_weight']")
#loc185 = loc("params_and_buffers['vllm_model.language_model.model.layers.27.mlp.experts.w2_weight']")
#loc186 = loc("params_and_buffers['vllm_model.language_model.model.layers.27.mlp.gate.weight']")
#loc187 = loc("params_and_buffers['vllm_model.language_model.model.layers.27.post_attention_layernorm.weight']")
#loc188 = loc("params_and_buffers['vllm_model.language_model.model.layers.27.self_attn.k_norm.weight']")
#loc189 = loc("params_and_buffers['vllm_model.language_model.model.layers.27.self_attn.o_proj.weight']")
#loc190 = loc("params_and_buffers['vllm_model.language_model.model.layers.27.self_attn.q_norm.weight']")
#loc191 = loc("params_and_buffers['vllm_model.language_model.model.layers.27.self_attn.qkv_proj.weight']")
#loc192 = loc("params_and_buffers['vllm_model.language_model.model.layers.28.input_layernorm.weight']")
#loc193 = loc("params_and_buffers['vllm_model.language_model.model.layers.28.mlp.experts.w13_weight']")
#loc194 = loc("params_and_buffers['vllm_model.language_model.model.layers.28.mlp.experts.w2_weight']")
#loc195 = loc("params_and_buffers['vllm_model.language_model.model.layers.28.mlp.gate.weight']")
#loc196 = loc("params_and_buffers['vllm_model.language_model.model.layers.28.post_attention_layernorm.weight']")
#loc197 = loc("params_and_buffers['vllm_model.language_model.model.layers.28.self_attn.k_norm.weight']")
#loc198 = loc("params_and_buffers['vllm_model.language_model.model.layers.28.self_attn.o_proj.weight']")
#loc199 = loc("params_and_buffers['vllm_model.language_model.model.layers.28.self_attn.q_norm.weight']")
#loc200 = loc("params_and_buffers['vllm_model.language_model.model.layers.28.self_attn.qkv_proj.weight']")
#loc201 = loc("params_and_buffers['vllm_model.language_model.model.layers.29.input_layernorm.weight']")
#loc202 = loc("params_and_buffers['vllm_model.language_model.model.layers.29.mlp.experts.w13_weight']")
#loc203 = loc("params_and_buffers['vllm_model.language_model.model.layers.29.mlp.experts.w2_weight']")
#loc204 = loc("params_and_buffers['vllm_model.language_model.model.layers.29.mlp.gate.weight']")
#loc205 = loc("params_and_buffers['vllm_model.language_model.model.layers.29.post_attention_layernorm.weight']")
#loc206 = loc("params_and_buffers['vllm_model.language_model.model.layers.29.self_attn.k_norm.weight']")
#loc207 = loc("params_and_buffers['vllm_model.language_model.model.layers.29.self_attn.o_proj.weight']")
#loc208 = loc("params_and_buffers['vllm_model.language_model.model.layers.29.self_attn.q_norm.weight']")
#loc209 = loc("params_and_buffers['vllm_model.language_model.model.layers.29.self_attn.qkv_proj.weight']")
#loc210 = loc("params_and_buffers['vllm_model.language_model.model.layers.3.input_layernorm.weight']")
#loc211 = loc("params_and_buffers['vllm_model.language_model.model.layers.3.mlp.experts.w13_weight']")
#loc212 = loc("params_and_buffers['vllm_model.language_model.model.layers.3.mlp.experts.w2_weight']")
#loc213 = loc("params_and_buffers['vllm_model.language_model.model.layers.3.mlp.gate.weight']")
#loc214 = loc("params_and_buffers['vllm_model.language_model.model.layers.3.post_attention_layernorm.weight']")
#loc215 = loc("params_and_buffers['vllm_model.language_model.model.layers.3.self_attn.k_norm.weight']")
#loc216 = loc("params_and_buffers['vllm_model.language_model.model.layers.3.self_attn.o_proj.weight']")
#loc217 = loc("params_and_buffers['vllm_model.language_model.model.layers.3.self_attn.q_norm.weight']")
#loc218 = loc("params_and_buffers['vllm_model.language_model.model.layers.3.self_attn.qkv_proj.weight']")
#loc219 = loc("params_and_buffers['vllm_model.language_model.model.layers.30.input_layernorm.weight']")
#loc220 = loc("params_and_buffers['vllm_model.language_model.model.layers.30.mlp.experts.w13_weight']")
#loc221 = loc("params_and_buffers['vllm_model.language_model.model.layers.30.mlp.experts.w2_weight']")
#loc222 = loc("params_and_buffers['vllm_model.language_model.model.layers.30.mlp.gate.weight']")
#loc223 = loc("params_and_buffers['vllm_model.language_model.model.layers.30.post_attention_layernorm.weight']")
#loc224 = loc("params_and_buffers['vllm_model.language_model.model.layers.30.self_attn.k_norm.weight']")
#loc225 = loc("params_and_buffers['vllm_model.language_model.model.layers.30.self_attn.o_proj.weight']")
#loc226 = loc("params_and_buffers['vllm_model.language_model.model.layers.30.self_attn.q_norm.weight']")
#loc227 = loc("params_and_buffers['vllm_model.language_model.model.layers.30.self_attn.qkv_proj.weight']")
#loc228 = loc("params_and_buffers['vllm_model.language_model.model.layers.31.input_layernorm.weight']")
#loc229 = loc("params_and_buffers['vllm_model.language_model.model.layers.31.mlp.experts.w13_weight']")
#loc230 = loc("params_and_buffers['vllm_model.language_model.model.layers.31.mlp.experts.w2_weight']")
#loc231 = loc("params_and_buffers['vllm_model.language_model.model.layers.31.mlp.gate.weight']")
#loc232 = loc("params_and_buffers['vllm_model.language_model.model.layers.31.post_attention_layernorm.weight']")
#loc233 = loc("params_and_buffers['vllm_model.language_model.model.layers.31.self_attn.k_norm.weight']")
#loc234 = loc("params_and_buffers['vllm_model.language_model.model.layers.31.self_attn.o_proj.weight']")
#loc235 = loc("params_and_buffers['vllm_model.language_model.model.layers.31.self_attn.q_norm.weight']")
#loc236 = loc("params_and_buffers['vllm_model.language_model.model.layers.31.self_attn.qkv_proj.weight']")
#loc237 = loc("params_and_buffers['vllm_model.language_model.model.layers.32.input_layernorm.weight']")
#loc238 = loc("params_and_buffers['vllm_model.language_model.model.layers.32.mlp.experts.w13_weight']")
#loc239 = loc("params_and_buffers['vllm_model.language_model.model.layers.32.mlp.experts.w2_weight']")
#loc240 = loc("params_and_buffers['vllm_model.language_model.model.layers.32.mlp.gate.weight']")
#loc241 = loc("params_and_buffers['vllm_model.language_model.model.layers.32.post_attention_layernorm.weight']")
#loc242 = loc("params_and_buffers['vllm_model.language_model.model.layers.32.self_attn.k_norm.weight']")
#loc243 = loc("params_and_buffers['vllm_model.language_model.model.layers.32.self_attn.o_proj.weight']")
#loc244 = loc("params_and_buffers['vllm_model.language_model.model.layers.32.self_attn.q_norm.weight']")
#loc245 = loc("params_and_buffers['vllm_model.language_model.model.layers.32.self_attn.qkv_proj.weight']")
#loc246 = loc("params_and_buffers['vllm_model.language_model.model.layers.33.input_layernorm.weight']")
#loc247 = loc("params_and_buffers['vllm_model.language_model.model.layers.33.mlp.experts.w13_weight']")
#loc248 = loc("params_and_buffers['vllm_model.language_model.model.layers.33.mlp.experts.w2_weight']")
#loc249 = loc("params_and_buffers['vllm_model.language_model.model.layers.33.mlp.gate.weight']")
#loc250 = loc("params_and_buffers['vllm_model.language_model.model.layers.33.post_attention_layernorm.weight']")
#loc251 = loc("params_and_buffers['vllm_model.language_model.model.layers.33.self_attn.k_norm.weight']")
#loc252 = loc("params_and_buffers['vllm_model.language_model.model.layers.33.self_attn.o_proj.weight']")
#loc253 = loc("params_and_buffers['vllm_model.language_model.model.layers.33.self_attn.q_norm.weight']")
#loc254 = loc("params_and_buffers['vllm_model.language_model.model.layers.33.self_attn.qkv_proj.weight']")
#loc255 = loc("params_and_buffers['vllm_model.language_model.model.layers.34.input_layernorm.weight']")
#loc256 = loc("params_and_buffers['vllm_model.language_model.model.layers.34.mlp.experts.w13_weight']")
#loc257 = loc("params_and_buffers['vllm_model.language_model.model.layers.34.mlp.experts.w2_weight']")
#loc258 = loc("params_and_buffers['vllm_model.language_model.model.layers.34.mlp.gate.weight']")
#loc259 = loc("params_and_buffers['vllm_model.language_model.model.layers.34.post_attention_layernorm.weight']")
#loc260 = loc("params_and_buffers['vllm_model.language_model.model.layers.34.self_attn.k_norm.weight']")
#loc261 = loc("params_and_buffers['vllm_model.language_model.model.layers.34.self_attn.o_proj.weight']")
#loc262 = loc("params_and_buffers['vllm_model.language_model.model.layers.34.self_attn.q_norm.weight']")
#loc263 = loc("params_and_buffers['vllm_model.language_model.model.layers.34.self_attn.qkv_proj.weight']")
#loc264 = loc("params_and_buffers['vllm_model.language_model.model.layers.35.input_layernorm.weight']")
#loc265 = loc("params_and_buffers['vllm_model.language_model.model.layers.35.mlp.experts.w13_weight']")
#loc266 = loc("params_and_buffers['vllm_model.language_model.model.layers.35.mlp.experts.w2_weight']")
#loc267 = loc("params_and_buffers['vllm_model.language_model.model.layers.35.mlp.gate.weight']")
#loc268 = loc("params_and_buffers['vllm_model.language_model.model.layers.35.post_attention_layernorm.weight']")
#loc269 = loc("params_and_buffers['vllm_model.language_model.model.layers.35.self_attn.k_norm.weight']")
#loc270 = loc("params_and_buffers['vllm_model.language_model.model.layers.35.self_attn.o_proj.weight']")
#loc271 = loc("params_and_buffers['vllm_model.language_model.model.layers.35.self_attn.q_norm.weight']")
#loc272 = loc("params_and_buffers['vllm_model.language_model.model.layers.35.self_attn.qkv_proj.weight']")
#loc273 = loc("params_and_buffers['vllm_model.language_model.model.layers.36.input_layernorm.weight']")
#loc274 = loc("params_and_buffers['vllm_model.language_model.model.layers.36.mlp.experts.w13_weight']")
#loc275 = loc("params_and_buffers['vllm_model.language_model.model.layers.36.mlp.experts.w2_weight']")
#loc276 = loc("params_and_buffers['vllm_model.language_model.model.layers.36.mlp.gate.weight']")
#loc277 = loc("params_and_buffers['vllm_model.language_model.model.layers.36.post_attention_layernorm.weight']")
#loc278 = loc("params_and_buffers['vllm_model.language_model.model.layers.36.self_attn.k_norm.weight']")
#loc279 = loc("params_and_buffers['vllm_model.language_model.model.layers.36.self_attn.o_proj.weight']")
#loc280 = loc("params_and_buffers['vllm_model.language_model.model.layers.36.self_attn.q_norm.weight']")
#loc281 = loc("params_and_buffers['vllm_model.language_model.model.layers.36.self_attn.qkv_proj.weight']")
#loc282 = loc("params_and_buffers['vllm_model.language_model.model.layers.37.input_layernorm.weight']")
#loc283 = loc("params_and_buffers['vllm_model.language_model.model.layers.37.mlp.experts.w13_weight']")
#loc284 = loc("params_and_buffers['vllm_model.language_model.model.layers.37.mlp.experts.w2_weight']")
#loc285 = loc("params_and_buffers['vllm_model.language_model.model.layers.37.mlp.gate.weight']")
#loc286 = loc("params_and_buffers['vllm_model.language_model.model.layers.37.post_attention_layernorm.weight']")
#loc287 = loc("params_and_buffers['vllm_model.language_model.model.layers.37.self_attn.k_norm.weight']")
#loc288 = loc("params_and_buffers['vllm_model.language_model.model.layers.37.self_attn.o_proj.weight']")
#loc289 = loc("params_and_buffers['vllm_model.language_model.model.layers.37.self_attn.q_norm.weight']")
#loc290 = loc("params_and_buffers['vllm_model.language_model.model.layers.37.self_attn.qkv_proj.weight']")
#loc291 = loc("params_and_buffers['vllm_model.language_model.model.layers.38.input_layernorm.weight']")
#loc292 = loc("params_and_buffers['vllm_model.language_model.model.layers.38.mlp.experts.w13_weight']")
#loc293 = loc("params_and_buffers['vllm_model.language_model.model.layers.38.mlp.experts.w2_weight']")
#loc294 = loc("params_and_buffers['vllm_model.language_model.model.layers.38.mlp.gate.weight']")
#loc295 = loc("params_and_buffers['vllm_model.language_model.model.layers.38.post_attention_layernorm.weight']")
#loc296 = loc("params_and_buffers['vllm_model.language_model.model.layers.38.self_attn.k_norm.weight']")
#loc297 = loc("params_and_buffers['vllm_model.language_model.model.layers.38.self_attn.o_proj.weight']")
#loc298 = loc("params_and_buffers['vllm_model.language_model.model.layers.38.self_attn.q_norm.weight']")
#loc299 = loc("params_and_buffers['vllm_model.language_model.model.layers.38.self_attn.qkv_proj.weight']")
#loc300 = loc("params_and_buffers['vllm_model.language_model.model.layers.39.input_layernorm.weight']")
#loc301 = loc("params_and_buffers['vllm_model.language_model.model.layers.39.mlp.experts.w13_weight']")
#loc302 = loc("params_and_buffers['vllm_model.language_model.model.layers.39.mlp.experts.w2_weight']")
#loc303 = loc("params_and_buffers['vllm_model.language_model.model.layers.39.mlp.gate.weight']")
#loc304 = loc("params_and_buffers['vllm_model.language_model.model.layers.39.post_attention_layernorm.weight']")
#loc305 = loc("params_and_buffers['vllm_model.language_model.model.layers.39.self_attn.k_norm.weight']")
#loc306 = loc("params_and_buffers['vllm_model.language_model.model.layers.39.self_attn.o_proj.weight']")
#loc307 = loc("params_and_buffers['vllm_model.language_model.model.layers.39.self_attn.q_norm.weight']")
#loc308 = loc("params_and_buffers['vllm_model.language_model.model.layers.39.self_attn.qkv_proj.weight']")
#loc309 = loc("params_and_buffers['vllm_model.language_model.model.layers.4.input_layernorm.weight']")
#loc310 = loc("params_and_buffers['vllm_model.language_model.model.layers.4.mlp.experts.w13_weight']")
#loc311 = loc("params_and_buffers['vllm_model.language_model.model.layers.4.mlp.experts.w2_weight']")
#loc312 = loc("params_and_buffers['vllm_model.language_model.model.layers.4.mlp.gate.weight']")
#loc313 = loc("params_and_buffers['vllm_model.language_model.model.layers.4.post_attention_layernorm.weight']")
#loc314 = loc("params_and_buffers['vllm_model.language_model.model.layers.4.self_attn.k_norm.weight']")
#loc315 = loc("params_and_buffers['vllm_model.language_model.model.layers.4.self_attn.o_proj.weight']")
#loc316 = loc("params_and_buffers['vllm_model.language_model.model.layers.4.self_attn.q_norm.weight']")
#loc317 = loc("params_and_buffers['vllm_model.language_model.model.layers.4.self_attn.qkv_proj.weight']")
#loc318 = loc("params_and_buffers['vllm_model.language_model.model.layers.40.input_layernorm.weight']")
#loc319 = loc("params_and_buffers['vllm_model.language_model.model.layers.40.mlp.experts.w13_weight']")
#loc320 = loc("params_and_buffers['vllm_model.language_model.model.layers.40.mlp.experts.w2_weight']")
#loc321 = loc("params_and_buffers['vllm_model.language_model.model.layers.40.mlp.gate.weight']")
#loc322 = loc("params_and_buffers['vllm_model.language_model.model.layers.40.post_attention_layernorm.weight']")
#loc323 = loc("params_and_buffers['vllm_model.language_model.model.layers.40.self_attn.k_norm.weight']")
#loc324 = loc("params_and_buffers['vllm_model.language_model.model.layers.40.self_attn.o_proj.weight']")
#loc325 = loc("params_and_buffers['vllm_model.language_model.model.layers.40.self_attn.q_norm.weight']")
#loc326 = loc("params_and_buffers['vllm_model.language_model.model.layers.40.self_attn.qkv_proj.weight']")
#loc327 = loc("params_and_buffers['vllm_model.language_model.model.layers.41.input_layernorm.weight']")
#loc328 = loc("params_and_buffers['vllm_model.language_model.model.layers.41.mlp.experts.w13_weight']")
#loc329 = loc("params_and_buffers['vllm_model.language_model.model.layers.41.mlp.experts.w2_weight']")
#loc330 = loc("params_and_buffers['vllm_model.language_model.model.layers.41.mlp.gate.weight']")
#loc331 = loc("params_and_buffers['vllm_model.language_model.model.layers.41.post_attention_layernorm.weight']")
#loc332 = loc("params_and_buffers['vllm_model.language_model.model.layers.41.self_attn.k_norm.weight']")
#loc333 = loc("params_and_buffers['vllm_model.language_model.model.layers.41.self_attn.o_proj.weight']")
#loc334 = loc("params_and_buffers['vllm_model.language_model.model.layers.41.self_attn.q_norm.weight']")
#loc335 = loc("params_and_buffers['vllm_model.language_model.model.layers.41.self_attn.qkv_proj.weight']")
#loc336 = loc("params_and_buffers['vllm_model.language_model.model.layers.42.input_layernorm.weight']")
#loc337 = loc("params_and_buffers['vllm_model.language_model.model.layers.42.mlp.experts.w13_weight']")
#loc338 = loc("params_and_buffers['vllm_model.language_model.model.layers.42.mlp.experts.w2_weight']")
#loc339 = loc("params_and_buffers['vllm_model.language_model.model.layers.42.mlp.gate.weight']")
#loc340 = loc("params_and_buffers['vllm_model.language_model.model.layers.42.post_attention_layernorm.weight']")
#loc341 = loc("params_and_buffers['vllm_model.language_model.model.layers.42.self_attn.k_norm.weight']")
#loc342 = loc("params_and_buffers['vllm_model.language_model.model.layers.42.self_attn.o_proj.weight']")
#loc343 = loc("params_and_buffers['vllm_model.language_model.model.layers.42.self_attn.q_norm.weight']")
#loc344 = loc("params_and_buffers['vllm_model.language_model.model.layers.42.self_attn.qkv_proj.weight']")
#loc345 = loc("params_and_buffers['vllm_model.language_model.model.layers.43.input_layernorm.weight']")
#loc346 = loc("params_and_buffers['vllm_model.language_model.model.layers.43.mlp.experts.w13_weight']")
#loc347 = loc("params_and_buffers['vllm_model.language_model.model.layers.43.mlp.experts.w2_weight']")
#loc348 = loc("params_and_buffers['vllm_model.language_model.model.layers.43.mlp.gate.weight']")
#loc349 = loc("params_and_buffers['vllm_model.language_model.model.layers.43.post_attention_layernorm.weight']")
#loc350 = loc("params_and_buffers['vllm_model.language_model.model.layers.43.self_attn.k_norm.weight']")
#loc351 = loc("params_and_buffers['vllm_model.language_model.model.layers.43.self_attn.o_proj.weight']")
#loc352 = loc("params_and_buffers['vllm_model.language_model.model.layers.43.self_attn.q_norm.weight']")
#loc353 = loc("params_and_buffers['vllm_model.language_model.model.layers.43.self_attn.qkv_proj.weight']")
#loc354 = loc("params_and_buffers['vllm_model.language_model.model.layers.44.input_layernorm.weight']")
#loc355 = loc("params_and_buffers['vllm_model.language_model.model.layers.44.mlp.experts.w13_weight']")
#loc356 = loc("params_and_buffers['vllm_model.language_model.model.layers.44.mlp.experts.w2_weight']")
#loc357 = loc("params_and_buffers['vllm_model.language_model.model.layers.44.mlp.gate.weight']")
#loc358 = loc("params_and_buffers['vllm_model.language_model.model.layers.44.post_attention_layernorm.weight']")
#loc359 = loc("params_and_buffers['vllm_model.language_model.model.layers.44.self_attn.k_norm.weight']")
#loc360 = loc("params_and_buffers['vllm_model.language_model.model.layers.44.self_attn.o_proj.weight']")
#loc361 = loc("params_and_buffers['vllm_model.language_model.model.layers.44.self_attn.q_norm.weight']")
#loc362 = loc("params_and_buffers['vllm_model.language_model.model.layers.44.self_attn.qkv_proj.weight']")
#loc363 = loc("params_and_buffers['vllm_model.language_model.model.layers.45.input_layernorm.weight']")
#loc364 = loc("params_and_buffers['vllm_model.language_model.model.layers.45.mlp.experts.w13_weight']")
#loc365 = loc("params_and_buffers['vllm_model.language_model.model.layers.45.mlp.experts.w2_weight']")
#loc366 = loc("params_and_buffers['vllm_model.language_model.model.layers.45.mlp.gate.weight']")
#loc367 = loc("params_and_buffers['vllm_model.language_model.model.layers.45.post_attention_layernorm.weight']")
#loc368 = loc("params_and_buffers['vllm_model.language_model.model.layers.45.self_attn.k_norm.weight']")
#loc369 = loc("params_and_buffers['vllm_model.language_model.model.layers.45.self_attn.o_proj.weight']")
#loc370 = loc("params_and_buffers['vllm_model.language_model.model.layers.45.self_attn.q_norm.weight']")
#loc371 = loc("params_and_buffers['vllm_model.language_model.model.layers.45.self_attn.qkv_proj.weight']")
#loc372 = loc("params_and_buffers['vllm_model.language_model.model.layers.46.input_layernorm.weight']")
#loc373 = loc("params_and_buffers['vllm_model.language_model.model.layers.46.mlp.experts.w13_weight']")
#loc374 = loc("params_and_buffers['vllm_model.language_model.model.layers.46.mlp.experts.w2_weight']")
#loc375 = loc("params_and_buffers['vllm_model.language_model.model.layers.46.mlp.gate.weight']")
#loc376 = loc("params_and_buffers['vllm_model.language_model.model.layers.46.post_attention_layernorm.weight']")
#loc377 = loc("params_and_buffers['vllm_model.language_model.model.layers.46.self_attn.k_norm.weight']")
#loc378 = loc("params_and_buffers['vllm_model.language_model.model.layers.46.self_attn.o_proj.weight']")
#loc379 = loc("params_and_buffers['vllm_model.language_model.model.layers.46.self_attn.q_norm.weight']")
#loc380 = loc("params_and_buffers['vllm_model.language_model.model.layers.46.self_attn.qkv_proj.weight']")
#loc381 = loc("params_and_buffers['vllm_model.language_model.model.layers.47.input_layernorm.weight']")
#loc382 = loc("params_and_buffers['vllm_model.language_model.model.layers.47.mlp.experts.w13_weight']")
#loc383 = loc("params_and_buffers['vllm_model.language_model.model.layers.47.mlp.experts.w2_weight']")
#loc384 = loc("params_and_buffers['vllm_model.language_model.model.layers.47.mlp.gate.weight']")
#loc385 = loc("params_and_buffers['vllm_model.language_model.model.layers.47.post_attention_layernorm.weight']")
#loc386 = loc("params_and_buffers['vllm_model.language_model.model.layers.47.self_attn.k_norm.weight']")
#loc387 = loc("params_and_buffers['vllm_model.language_model.model.layers.47.self_attn.o_proj.weight']")
#loc388 = loc("params_and_buffers['vllm_model.language_model.model.layers.47.self_attn.q_norm.weight']")
#loc389 = loc("params_and_buffers['vllm_model.language_model.model.layers.47.self_attn.qkv_proj.weight']")
#loc390 = loc("params_and_buffers['vllm_model.language_model.model.layers.5.input_layernorm.weight']")
#loc391 = loc("params_and_buffers['vllm_model.language_model.model.layers.5.mlp.experts.w13_weight']")
#loc392 = loc("params_and_buffers['vllm_model.language_model.model.layers.5.mlp.experts.w2_weight']")
#loc393 = loc("params_and_buffers['vllm_model.language_model.model.layers.5.mlp.gate.weight']")
#loc394 = loc("params_and_buffers['vllm_model.language_model.model.layers.5.post_attention_layernorm.weight']")
#loc395 = loc("params_and_buffers['vllm_model.language_model.model.layers.5.self_attn.k_norm.weight']")
#loc396 = loc("params_and_buffers['vllm_model.language_model.model.layers.5.self_attn.o_proj.weight']")
#loc397 = loc("params_and_buffers['vllm_model.language_model.model.layers.5.self_attn.q_norm.weight']")
#loc398 = loc("params_and_buffers['vllm_model.language_model.model.layers.5.self_attn.qkv_proj.weight']")
#loc399 = loc("params_and_buffers['vllm_model.language_model.model.layers.6.input_layernorm.weight']")
#loc400 = loc("params_and_buffers['vllm_model.language_model.model.layers.6.mlp.experts.w13_weight']")
#loc401 = loc("params_and_buffers['vllm_model.language_model.model.layers.6.mlp.experts.w2_weight']")
#loc402 = loc("params_and_buffers['vllm_model.language_model.model.layers.6.mlp.gate.weight']")
#loc403 = loc("params_and_buffers['vllm_model.language_model.model.layers.6.post_attention_layernorm.weight']")
#loc404 = loc("params_and_buffers['vllm_model.language_model.model.layers.6.self_attn.k_norm.weight']")
#loc405 = loc("params_and_buffers['vllm_model.language_model.model.layers.6.self_attn.o_proj.weight']")
#loc406 = loc("params_and_buffers['vllm_model.language_model.model.layers.6.self_attn.q_norm.weight']")
#loc407 = loc("params_and_buffers['vllm_model.language_model.model.layers.6.self_attn.qkv_proj.weight']")
#loc408 = loc("params_and_buffers['vllm_model.language_model.model.layers.7.input_layernorm.weight']")
#loc409 = loc("params_and_buffers['vllm_model.language_model.model.layers.7.mlp.experts.w13_weight']")
#loc410 = loc("params_and_buffers['vllm_model.language_model.model.layers.7.mlp.experts.w2_weight']")
#loc411 = loc("params_and_buffers['vllm_model.language_model.model.layers.7.mlp.gate.weight']")
#loc412 = loc("params_and_buffers['vllm_model.language_model.model.layers.7.post_attention_layernorm.weight']")
#loc413 = loc("params_and_buffers['vllm_model.language_model.model.layers.7.self_attn.k_norm.weight']")
#loc414 = loc("params_and_buffers['vllm_model.language_model.model.layers.7.self_attn.o_proj.weight']")
#loc415 = loc("params_and_buffers['vllm_model.language_model.model.layers.7.self_attn.q_norm.weight']")
#loc416 = loc("params_and_buffers['vllm_model.language_model.model.layers.7.self_attn.qkv_proj.weight']")
#loc417 = loc("params_and_buffers['vllm_model.language_model.model.layers.8.input_layernorm.weight']")
#loc418 = loc("params_and_buffers['vllm_model.language_model.model.layers.8.mlp.experts.w13_weight']")
#loc419 = loc("params_and_buffers['vllm_model.language_model.model.layers.8.mlp.experts.w2_weight']")
#loc420 = loc("params_and_buffers['vllm_model.language_model.model.layers.8.mlp.gate.weight']")
#loc421 = loc("params_and_buffers['vllm_model.language_model.model.layers.8.post_attention_layernorm.weight']")
#loc422 = loc("params_and_buffers['vllm_model.language_model.model.layers.8.self_attn.k_norm.weight']")
#loc423 = loc("params_and_buffers['vllm_model.language_model.model.layers.8.self_attn.o_proj.weight']")
#loc424 = loc("params_and_buffers['vllm_model.language_model.model.layers.8.self_attn.q_norm.weight']")
#loc425 = loc("params_and_buffers['vllm_model.language_model.model.layers.8.self_attn.qkv_proj.weight']")
#loc426 = loc("params_and_buffers['vllm_model.language_model.model.layers.9.input_layernorm.weight']")
#loc427 = loc("params_and_buffers['vllm_model.language_model.model.layers.9.mlp.experts.w13_weight']")
#loc428 = loc("params_and_buffers['vllm_model.language_model.model.layers.9.mlp.experts.w2_weight']")
#loc429 = loc("params_and_buffers['vllm_model.language_model.model.layers.9.mlp.gate.weight']")
#loc430 = loc("params_and_buffers['vllm_model.language_model.model.layers.9.post_attention_layernorm.weight']")
#loc431 = loc("params_and_buffers['vllm_model.language_model.model.layers.9.self_attn.k_norm.weight']")
#loc432 = loc("params_and_buffers['vllm_model.language_model.model.layers.9.self_attn.o_proj.weight']")
#loc433 = loc("params_and_buffers['vllm_model.language_model.model.layers.9.self_attn.q_norm.weight']")
#loc434 = loc("params_and_buffers['vllm_model.language_model.model.layers.9.self_attn.qkv_proj.weight']")
#loc435 = loc("params_and_buffers['vllm_model.language_model.model.norm.weight']")
#loc436 = loc("kv_caches[0]")
#loc437 = loc("kv_caches[1]")
#loc438 = loc("kv_caches[2]")
#loc439 = loc("kv_caches[3]")
#loc440 = loc("kv_caches[4]")
#loc441 = loc("kv_caches[5]")
#loc442 = loc("kv_caches[6]")
#loc443 = loc("kv_caches[7]")
#loc444 = loc("kv_caches[8]")
#loc445 = loc("kv_caches[9]")
#loc446 = loc("kv_caches[10]")
#loc447 = loc("kv_caches[11]")
#loc448 = loc("kv_caches[12]")
#loc449 = loc("kv_caches[13]")
#loc450 = loc("kv_caches[14]")
#loc451 = loc("kv_caches[15]")
#loc452 = loc("kv_caches[16]")
#loc453 = loc("kv_caches[17]")
#loc454 = loc("kv_caches[18]")
#loc455 = loc("kv_caches[19]")
#loc456 = loc("kv_caches[20]")
#loc457 = loc("kv_caches[21]")
#loc458 = loc("kv_caches[22]")
#loc459 = loc("kv_caches[23]")
#loc460 = loc("kv_caches[24]")
#loc461 = loc("kv_caches[25]")
#loc462 = loc("kv_caches[26]")
#loc463 = loc("kv_caches[27]")
#loc464 = loc("kv_caches[28]")
#loc465 = loc("kv_caches[29]")
#loc466 = loc("kv_caches[30]")
#loc467 = loc("kv_caches[31]")
#loc468 = loc("kv_caches[32]")
#loc469 = loc("kv_caches[33]")
#loc470 = loc("kv_caches[34]")
#loc471 = loc("kv_caches[35]")
#loc472 = loc("kv_caches[36]")
#loc473 = loc("kv_caches[37]")
#loc474 = loc("kv_caches[38]")
#loc475 = loc("kv_caches[39]")
#loc476 = loc("kv_caches[40]")
#loc477 = loc("kv_caches[41]")
#loc478 = loc("kv_caches[42]")
#loc479 = loc("kv_caches[43]")
#loc480 = loc("kv_caches[44]")
#loc481 = loc("kv_caches[45]")
#loc482 = loc("kv_caches[46]")
#loc483 = loc("kv_caches[47]")
#loc484 = loc("input_ids")
#loc485 = loc("attn_metadata.input_positions")
#loc486 = loc("attn_metadata.block_tables")
#loc487 = loc("attn_metadata.seq_lens")
#loc488 = loc("attn_metadata.query_start_loc")
#loc489 = loc("attn_metadata.request_distribution")
#loc499 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/custom_op.py":76:15 to :51)
#loc506 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/custom_op.py":46:15 to :52)
#loc596 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/quantization/unquantized.py":256:17 to 261:9)
#loc597 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/fused_moe/layer.py":2239:34 to 2260:13)
#loc598 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/fused_moe/layer.py":2023:31 to :78)
#loc608 = loc("shard_map")
#loc630 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":397:15 to 399:63)
#loc643 = loc("scatter-add")
#loc645 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":96:17 to 102:28)
#loc646 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":334:17 to 340:48)
#loc655 = loc("psum")
#loc662 = loc("sort")
#loc666 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":384:37 to 391:3)
#loc668 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":126:15 to :38)
#loc686 = loc("scatter")
#loc699 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":232:15 to 236:3)
#loc713 = loc("reduce_window_sum")
#loc723 = loc("CustomOp.forward_tpu"(#loc499))
#loc730 = loc("CustomOp.forward"(#loc506))
#loc820 = loc("VllmUnquantizedFusedMoEMethod.apply"(#loc596))
#loc821 = loc("FusedMoE.forward_impl"(#loc597))
#loc822 = loc("FusedMoE.forward_native"(#loc598))
#loc853 = loc("jax_fused_moe_func_padded"(#loc630))
#loc867 = loc("tensor_sharded_gmm_merged_column_parallel"(#loc645))
#loc868 = loc("jax_fused_moe_func"(#loc646))
#loc886 = loc("gmm"(#loc666))
#loc888 = loc("make_group_metadata"(#loc668))
#loc918 = loc("make_group_metadata"(#loc699))
#loc943 = loc(callsite(#loc723 at #loc730))
#loc980 = loc(callsite(#loc822 at #loc943))
#loc1012 = loc(callsite(#loc821 at #loc980))
#loc1052 = loc(callsite(#loc820 at #loc1012))
#loc1144 = loc(callsite(#loc853 at #loc1052))
#loc1236 = loc(callsite(#loc868 at #loc1144))
#loc1330 = loc(callsite(#loc867 at #loc1236))
#loc1427 = loc(callsite(#loc886 at #loc1330))
#loc1579 = loc(callsite(#loc888 at #loc1427))
#loc1609 = loc(callsite(#loc918 at #loc1427))
module @jit_step_fun attributes {mhlo.num_partitions = 4 : i32, mhlo.num_replicas = 1 : i32} {
  sdy.mesh @mesh = <["data"=1, "model"=4]> loc(#loc)
  func.func public @main(%arg0: tensor<152064x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.embed_tokens.weight']"), %arg1: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.0.input_layernorm.weight']"), %arg2: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.0.mlp.experts.w13_weight']"), %arg3: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.0.mlp.experts.w2_weight']"), %arg4: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.0.mlp.gate.weight']"), %arg5: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.0.post_attention_layernorm.weight']"), %arg6: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.0.self_attn.k_norm.weight']"), %arg7: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.0.self_attn.o_proj.weight']"), %arg8: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.0.self_attn.q_norm.weight']"), %arg9: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.0.self_attn.qkv_proj.weight']"), %arg10: tensor<262144x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.0.self_attn.rotary_emb.cos_sin_cache']"), %arg11: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.1.input_layernorm.weight']"), %arg12: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.1.mlp.experts.w13_weight']"), %arg13: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.1.mlp.experts.w2_weight']"), %arg14: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.1.mlp.gate.weight']"), %arg15: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.1.post_attention_layernorm.weight']"), %arg16: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.1.self_attn.k_norm.weight']"), %arg17: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.1.self_attn.o_proj.weight']"), %arg18: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.1.self_attn.q_norm.weight']"), %arg19: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.1.self_attn.qkv_proj.weight']"), %arg20: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.10.input_layernorm.weight']"), %arg21: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.10.mlp.experts.w13_weight']"), %arg22: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.10.mlp.experts.w2_weight']"), %arg23: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.10.mlp.gate.weight']"), %arg24: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.10.post_attention_layernorm.weight']"), %arg25: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.10.self_attn.k_norm.weight']"), %arg26: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.10.self_attn.o_proj.weight']"), %arg27: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.10.self_attn.q_norm.weight']"), %arg28: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.10.self_attn.qkv_proj.weight']"), %arg29: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.11.input_layernorm.weight']"), %arg30: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.11.mlp.experts.w13_weight']"), %arg31: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.11.mlp.experts.w2_weight']"), %arg32: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.11.mlp.gate.weight']"), %arg33: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.11.post_attention_layernorm.weight']"), %arg34: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.11.self_attn.k_norm.weight']"), %arg35: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.11.self_attn.o_proj.weight']"), %arg36: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.11.self_attn.q_norm.weight']"), %arg37: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.11.self_attn.qkv_proj.weight']"), %arg38: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.12.input_layernorm.weight']"), %arg39: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.12.mlp.experts.w13_weight']"), %arg40: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.12.mlp.experts.w2_weight']"), %arg41: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.12.mlp.gate.weight']"), %arg42: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.12.post_attention_layernorm.weight']"), %arg43: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.12.self_attn.k_norm.weight']"), %arg44: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.12.self_attn.o_proj.weight']"), %arg45: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.12.self_attn.q_norm.weight']"), %arg46: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.12.self_attn.qkv_proj.weight']"), %arg47: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.13.input_layernorm.weight']"), %arg48: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.13.mlp.experts.w13_weight']"), %arg49: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.13.mlp.experts.w2_weight']"), %arg50: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.13.mlp.gate.weight']"), %arg51: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.13.post_attention_layernorm.weight']"), %arg52: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.13.self_attn.k_norm.weight']"), %arg53: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.13.self_attn.o_proj.weight']"), %arg54: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.13.self_attn.q_norm.weight']"), %arg55: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.13.self_attn.qkv_proj.weight']"), %arg56: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.14.input_layernorm.weight']"), %arg57: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.14.mlp.experts.w13_weight']"), %arg58: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.14.mlp.experts.w2_weight']"), %arg59: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.14.mlp.gate.weight']"), %arg60: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.14.post_attention_layernorm.weight']"), %arg61: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.14.self_attn.k_norm.weight']"), %arg62: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.14.self_attn.o_proj.weight']"), %arg63: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.14.self_attn.q_norm.weight']"), %arg64: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.14.self_attn.qkv_proj.weight']"), %arg65: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.15.input_layernorm.weight']"), %arg66: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.15.mlp.experts.w13_weight']"), %arg67: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.15.mlp.experts.w2_weight']"), %arg68: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.15.mlp.gate.weight']"), %arg69: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.15.post_attention_layernorm.weight']"), %arg70: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.15.self_attn.k_norm.weight']"), %arg71: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.15.self_attn.o_proj.weight']"), %arg72: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.15.self_attn.q_norm.weight']"), %arg73: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.15.self_attn.qkv_proj.weight']"), %arg74: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.16.input_layernorm.weight']"), %arg75: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.16.mlp.experts.w13_weight']"), %arg76: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.16.mlp.experts.w2_weight']"), %arg77: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.16.mlp.gate.weight']"), %arg78: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.16.post_attention_layernorm.weight']"), %arg79: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.16.self_attn.k_norm.weight']"), %arg80: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.16.self_attn.o_proj.weight']"), %arg81: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.16.self_attn.q_norm.weight']"), %arg82: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.16.self_attn.qkv_proj.weight']"), %arg83: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.17.input_layernorm.weight']"), %arg84: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.17.mlp.experts.w13_weight']"), %arg85: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.17.mlp.experts.w2_weight']"), %arg86: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.17.mlp.gate.weight']"), %arg87: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.17.post_attention_layernorm.weight']"), %arg88: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.17.self_attn.k_norm.weight']"), %arg89: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.17.self_attn.o_proj.weight']"), %arg90: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.17.self_attn.q_norm.weight']"), %arg91: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.17.self_attn.qkv_proj.weight']"), %arg92: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.18.input_layernorm.weight']"), %arg93: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.18.mlp.experts.w13_weight']"), %arg94: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.18.mlp.experts.w2_weight']"), %arg95: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.18.mlp.gate.weight']"), %arg96: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.18.post_attention_layernorm.weight']"), %arg97: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.18.self_attn.k_norm.weight']"), %arg98: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.18.self_attn.o_proj.weight']"), %arg99: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.18.self_attn.q_norm.weight']"), %arg100: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.18.self_attn.qkv_proj.weight']"), %arg101: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.19.input_layernorm.weight']"), %arg102: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.19.mlp.experts.w13_weight']"), %arg103: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.19.mlp.experts.w2_weight']"), %arg104: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.19.mlp.gate.weight']"), %arg105: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.19.post_attention_layernorm.weight']"), %arg106: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.19.self_attn.k_norm.weight']"), %arg107: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.19.self_attn.o_proj.weight']"), %arg108: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.19.self_attn.q_norm.weight']"), %arg109: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.19.self_attn.qkv_proj.weight']"), %arg110: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.2.input_layernorm.weight']"), %arg111: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.2.mlp.experts.w13_weight']"), %arg112: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.2.mlp.experts.w2_weight']"), %arg113: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.2.mlp.gate.weight']"), %arg114: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.2.post_attention_layernorm.weight']"), %arg115: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.2.self_attn.k_norm.weight']"), %arg116: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.2.self_attn.o_proj.weight']"), %arg117: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.2.self_attn.q_norm.weight']"), %arg118: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.2.self_attn.qkv_proj.weight']"), %arg119: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.20.input_layernorm.weight']"), %arg120: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.20.mlp.experts.w13_weight']"), %arg121: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.20.mlp.experts.w2_weight']"), %arg122: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.20.mlp.gate.weight']"), %arg123: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.20.post_attention_layernorm.weight']"), %arg124: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.20.self_attn.k_norm.weight']"), %arg125: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.20.self_attn.o_proj.weight']"), %arg126: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.20.self_attn.q_norm.weight']"), %arg127: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.20.self_attn.qkv_proj.weight']"), %arg128: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.21.input_layernorm.weight']"), %arg129: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.21.mlp.experts.w13_weight']"), %arg130: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.21.mlp.experts.w2_weight']"), %arg131: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.21.mlp.gate.weight']"), %arg132: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.21.post_attention_layernorm.weight']"), %arg133: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.21.self_attn.k_norm.weight']"), %arg134: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.21.self_attn.o_proj.weight']"), %arg135: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.21.self_attn.q_norm.weight']"), %arg136: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.21.self_attn.qkv_proj.weight']"), %arg137: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.22.input_layernorm.weight']"), %arg138: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.22.mlp.experts.w13_weight']"), %arg139: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.22.mlp.experts.w2_weight']"), %arg140: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.22.mlp.gate.weight']"), %arg141: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.22.post_attention_layernorm.weight']"), %arg142: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.22.self_attn.k_norm.weight']"), %arg143: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.22.self_attn.o_proj.weight']"), %arg144: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.22.self_attn.q_norm.weight']"), %arg145: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.22.self_attn.qkv_proj.weight']"), %arg146: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.23.input_layernorm.weight']"), %arg147: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.23.mlp.experts.w13_weight']"), %arg148: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.23.mlp.experts.w2_weight']"), %arg149: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.23.mlp.gate.weight']"), %arg150: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.23.post_attention_layernorm.weight']"), %arg151: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.23.self_attn.k_norm.weight']"), %arg152: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.23.self_attn.o_proj.weight']"), %arg153: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.23.self_attn.q_norm.weight']"), %arg154: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.23.self_attn.qkv_proj.weight']"), %arg155: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.24.input_layernorm.weight']"), %arg156: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.24.mlp.experts.w13_weight']"), %arg157: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.24.mlp.experts.w2_weight']"), %arg158: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.24.mlp.gate.weight']"), %arg159: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.24.post_attention_layernorm.weight']"), %arg160: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.24.self_attn.k_norm.weight']"), %arg161: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.24.self_attn.o_proj.weight']"), %arg162: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.24.self_attn.q_norm.weight']"), %arg163: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.24.self_attn.qkv_proj.weight']"), %arg164: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.25.input_layernorm.weight']"), %arg165: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.25.mlp.experts.w13_weight']"), %arg166: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.25.mlp.experts.w2_weight']"), %arg167: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.25.mlp.gate.weight']"), %arg168: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.25.post_attention_layernorm.weight']"), %arg169: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.25.self_attn.k_norm.weight']"), %arg170: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.25.self_attn.o_proj.weight']"), %arg171: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.25.self_attn.q_norm.weight']"), %arg172: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.25.self_attn.qkv_proj.weight']"), %arg173: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.26.input_layernorm.weight']"), %arg174: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.26.mlp.experts.w13_weight']"), %arg175: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.26.mlp.experts.w2_weight']"), %arg176: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.26.mlp.gate.weight']"), %arg177: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.26.post_attention_layernorm.weight']"), %arg178: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.26.self_attn.k_norm.weight']"), %arg179: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.26.self_attn.o_proj.weight']"), %arg180: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.26.self_attn.q_norm.weight']"), %arg181: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.26.self_attn.qkv_proj.weight']"), %arg182: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.27.input_layernorm.weight']"), %arg183: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.27.mlp.experts.w13_weight']"), %arg184: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.27.mlp.experts.w2_weight']"), %arg185: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.27.mlp.gate.weight']"), %arg186: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.27.post_attention_layernorm.weight']"), %arg187: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.27.self_attn.k_norm.weight']"), %arg188: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.27.self_attn.o_proj.weight']"), %arg189: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.27.self_attn.q_norm.weight']"), %arg190: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.27.self_attn.qkv_proj.weight']"), %arg191: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.28.input_layernorm.weight']"), %arg192: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.28.mlp.experts.w13_weight']"), %arg193: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.28.mlp.experts.w2_weight']"), %arg194: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.28.mlp.gate.weight']"), %arg195: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.28.post_attention_layernorm.weight']"), %arg196: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.28.self_attn.k_norm.weight']"), %arg197: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.28.self_attn.o_proj.weight']"), %arg198: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.28.self_attn.q_norm.weight']"), %arg199: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.28.self_attn.qkv_proj.weight']"), %arg200: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.29.input_layernorm.weight']"), %arg201: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.29.mlp.experts.w13_weight']"), %arg202: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.29.mlp.experts.w2_weight']"), %arg203: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.29.mlp.gate.weight']"), %arg204: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.29.post_attention_layernorm.weight']"), %arg205: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.29.self_attn.k_norm.weight']"), %arg206: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.29.self_attn.o_proj.weight']"), %arg207: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.29.self_attn.q_norm.weight']"), %arg208: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.29.self_attn.qkv_proj.weight']"), %arg209: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.3.input_layernorm.weight']"), %arg210: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.3.mlp.experts.w13_weight']"), %arg211: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.3.mlp.experts.w2_weight']"), %arg212: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.3.mlp.gate.weight']"), %arg213: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.3.post_attention_layernorm.weight']"), %arg214: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.3.self_attn.k_norm.weight']"), %arg215: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.3.self_attn.o_proj.weight']"), %arg216: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.3.self_attn.q_norm.weight']"), %arg217: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.3.self_attn.qkv_proj.weight']"), %arg218: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.30.input_layernorm.weight']"), %arg219: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.30.mlp.experts.w13_weight']"), %arg220: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.30.mlp.experts.w2_weight']"), %arg221: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.30.mlp.gate.weight']"), %arg222: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.30.post_attention_layernorm.weight']"), %arg223: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.30.self_attn.k_norm.weight']"), %arg224: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.30.self_attn.o_proj.weight']"), %arg225: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.30.self_attn.q_norm.weight']"), %arg226: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.30.self_attn.qkv_proj.weight']"), %arg227: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.31.input_layernorm.weight']"), %arg228: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.31.mlp.experts.w13_weight']"), %arg229: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.31.mlp.experts.w2_weight']"), %arg230: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.31.mlp.gate.weight']"), %arg231: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.31.post_attention_layernorm.weight']"), %arg232: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.31.self_attn.k_norm.weight']"), %arg233: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.31.self_attn.o_proj.weight']"), %arg234: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.31.self_attn.q_norm.weight']"), %arg235: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.31.self_attn.qkv_proj.weight']"), %arg236: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.32.input_layernorm.weight']"), %arg237: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.32.mlp.experts.w13_weight']"), %arg238: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.32.mlp.experts.w2_weight']"), %arg239: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.32.mlp.gate.weight']"), %arg240: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.32.post_attention_layernorm.weight']"), %arg241: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.32.self_attn.k_norm.weight']"), %arg242: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.32.self_attn.o_proj.weight']"), %arg243: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.32.self_attn.q_norm.weight']"), %arg244: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.32.self_attn.qkv_proj.weight']"), %arg245: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.33.input_layernorm.weight']"), %arg246: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.33.mlp.experts.w13_weight']"), %arg247: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.33.mlp.experts.w2_weight']"), %arg248: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.33.mlp.gate.weight']"), %arg249: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.33.post_attention_layernorm.weight']"), %arg250: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.33.self_attn.k_norm.weight']"), %arg251: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.33.self_attn.o_proj.weight']"), %arg252: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.33.self_attn.q_norm.weight']"), %arg253: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.33.self_attn.qkv_proj.weight']"), %arg254: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.34.input_layernorm.weight']"), %arg255: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.34.mlp.experts.w13_weight']"), %arg256: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.34.mlp.experts.w2_weight']"), %arg257: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.34.mlp.gate.weight']"), %arg258: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.34.post_attention_layernorm.weight']"), %arg259: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.34.self_attn.k_norm.weight']"), %arg260: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.34.self_attn.o_proj.weight']"), %arg261: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.34.self_attn.q_norm.weight']"), %arg262: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.34.self_attn.qkv_proj.weight']"), %arg263: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.35.input_layernorm.weight']"), %arg264: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.35.mlp.experts.w13_weight']"), %arg265: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.35.mlp.experts.w2_weight']"), %arg266: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.35.mlp.gate.weight']"), %arg267: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.35.post_attention_layernorm.weight']"), %arg268: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.35.self_attn.k_norm.weight']"), %arg269: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.35.self_attn.o_proj.weight']"), %arg270: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.35.self_attn.q_norm.weight']"), %arg271: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.35.self_attn.qkv_proj.weight']"), %arg272: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.36.input_layernorm.weight']"), %arg273: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.36.mlp.experts.w13_weight']"), %arg274: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.36.mlp.experts.w2_weight']"), %arg275: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.36.mlp.gate.weight']"), %arg276: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.36.post_attention_layernorm.weight']"), %arg277: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.36.self_attn.k_norm.weight']"), %arg278: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.36.self_attn.o_proj.weight']"), %arg279: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.36.self_attn.q_norm.weight']"), %arg280: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.36.self_attn.qkv_proj.weight']"), %arg281: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.37.input_layernorm.weight']"), %arg282: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.37.mlp.experts.w13_weight']"), %arg283: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.37.mlp.experts.w2_weight']"), %arg284: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.37.mlp.gate.weight']"), %arg285: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.37.post_attention_layernorm.weight']"), %arg286: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.37.self_attn.k_norm.weight']"), %arg287: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.37.self_attn.o_proj.weight']"), %arg288: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.37.self_attn.q_norm.weight']"), %arg289: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.37.self_attn.qkv_proj.weight']"), %arg290: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.38.input_layernorm.weight']"), %arg291: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.38.mlp.experts.w13_weight']"), %arg292: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.38.mlp.experts.w2_weight']"), %arg293: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.38.mlp.gate.weight']"), %arg294: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.38.post_attention_layernorm.weight']"), %arg295: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.38.self_attn.k_norm.weight']"), %arg296: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.38.self_attn.o_proj.weight']"), %arg297: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.38.self_attn.q_norm.weight']"), %arg298: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.38.self_attn.qkv_proj.weight']"), %arg299: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.39.input_layernorm.weight']"), %arg300: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.39.mlp.experts.w13_weight']"), %arg301: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.39.mlp.experts.w2_weight']"), %arg302: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.39.mlp.gate.weight']"), %arg303: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.39.post_attention_layernorm.weight']"), %arg304: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.39.self_attn.k_norm.weight']"), %arg305: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.39.self_attn.o_proj.weight']"), %arg306: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.39.self_attn.q_norm.weight']"), %arg307: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.39.self_attn.qkv_proj.weight']"), %arg308: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.4.input_layernorm.weight']"), %arg309: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.4.mlp.experts.w13_weight']"), %arg310: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.4.mlp.experts.w2_weight']"), %arg311: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.4.mlp.gate.weight']"), %arg312: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.4.post_attention_layernorm.weight']"), %arg313: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.4.self_attn.k_norm.weight']"), %arg314: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.4.self_attn.o_proj.weight']"), %arg315: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.4.self_attn.q_norm.weight']"), %arg316: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.4.self_attn.qkv_proj.weight']"), %arg317: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.40.input_layernorm.weight']"), %arg318: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.40.mlp.experts.w13_weight']"), %arg319: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.40.mlp.experts.w2_weight']"), %arg320: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.40.mlp.gate.weight']"), %arg321: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.40.post_attention_layernorm.weight']"), %arg322: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.40.self_attn.k_norm.weight']"), %arg323: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.40.self_attn.o_proj.weight']"), %arg324: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.40.self_attn.q_norm.weight']"), %arg325: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.40.self_attn.qkv_proj.weight']"), %arg326: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.41.input_layernorm.weight']"), %arg327: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.41.mlp.experts.w13_weight']"), %arg328: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.41.mlp.experts.w2_weight']"), %arg329: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.41.mlp.gate.weight']"), %arg330: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.41.post_attention_layernorm.weight']"), %arg331: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.41.self_attn.k_norm.weight']"), %arg332: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.41.self_attn.o_proj.weight']"), %arg333: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.41.self_attn.q_norm.weight']"), %arg334: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.41.self_attn.qkv_proj.weight']"), %arg335: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.42.input_layernorm.weight']"), %arg336: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.42.mlp.experts.w13_weight']"), %arg337: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.42.mlp.experts.w2_weight']"), %arg338: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.42.mlp.gate.weight']"), %arg339: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.42.post_attention_layernorm.weight']"), %arg340: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.42.self_attn.k_norm.weight']"), %arg341: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.42.self_attn.o_proj.weight']"), %arg342: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.42.self_attn.q_norm.weight']"), %arg343: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.42.self_attn.qkv_proj.weight']"), %arg344: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.43.input_layernorm.weight']"), %arg345: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.43.mlp.experts.w13_weight']"), %arg346: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.43.mlp.experts.w2_weight']"), %arg347: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.43.mlp.gate.weight']"), %arg348: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.43.post_attention_layernorm.weight']"), %arg349: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.43.self_attn.k_norm.weight']"), %arg350: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.43.self_attn.o_proj.weight']"), %arg351: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.43.self_attn.q_norm.weight']"), %arg352: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.43.self_attn.qkv_proj.weight']"), %arg353: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.44.input_layernorm.weight']"), %arg354: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.44.mlp.experts.w13_weight']"), %arg355: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.44.mlp.experts.w2_weight']"), %arg356: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.44.mlp.gate.weight']"), %arg357: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.44.post_attention_layernorm.weight']"), %arg358: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.44.self_attn.k_norm.weight']"), %arg359: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.44.self_attn.o_proj.weight']"), %arg360: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.44.self_attn.q_norm.weight']"), %arg361: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.44.self_attn.qkv_proj.weight']"), %arg362: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.45.input_layernorm.weight']"), %arg363: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.45.mlp.experts.w13_weight']"), %arg364: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.45.mlp.experts.w2_weight']"), %arg365: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.45.mlp.gate.weight']"), %arg366: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.45.post_attention_layernorm.weight']"), %arg367: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.45.self_attn.k_norm.weight']"), %arg368: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.45.self_attn.o_proj.weight']"), %arg369: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.45.self_attn.q_norm.weight']"), %arg370: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.45.self_attn.qkv_proj.weight']"), %arg371: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.46.input_layernorm.weight']"), %arg372: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.46.mlp.experts.w13_weight']"), %arg373: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.46.mlp.experts.w2_weight']"), %arg374: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.46.mlp.gate.weight']"), %arg375: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.46.post_attention_layernorm.weight']"), %arg376: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.46.self_attn.k_norm.weight']"), %arg377: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.46.self_attn.o_proj.weight']"), %arg378: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.46.self_attn.q_norm.weight']"), %arg379: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.46.self_attn.qkv_proj.weight']"), %arg380: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.47.input_layernorm.weight']"), %arg381: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.47.mlp.experts.w13_weight']"), %arg382: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.47.mlp.experts.w2_weight']"), %arg383: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.47.mlp.gate.weight']"), %arg384: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.47.post_attention_layernorm.weight']"), %arg385: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.47.self_attn.k_norm.weight']"), %arg386: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.47.self_attn.o_proj.weight']"), %arg387: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.47.self_attn.q_norm.weight']"), %arg388: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.47.self_attn.qkv_proj.weight']"), %arg389: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.5.input_layernorm.weight']"), %arg390: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.5.mlp.experts.w13_weight']"), %arg391: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.5.mlp.experts.w2_weight']"), %arg392: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.5.mlp.gate.weight']"), %arg393: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.5.post_attention_layernorm.weight']"), %arg394: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.5.self_attn.k_norm.weight']"), %arg395: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.5.self_attn.o_proj.weight']"), %arg396: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.5.self_attn.q_norm.weight']"), %arg397: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.5.self_attn.qkv_proj.weight']"), %arg398: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.6.input_layernorm.weight']"), %arg399: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.6.mlp.experts.w13_weight']"), %arg400: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.6.mlp.experts.w2_weight']"), %arg401: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.6.mlp.gate.weight']"), %arg402: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.6.post_attention_layernorm.weight']"), %arg403: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.6.self_attn.k_norm.weight']"), %arg404: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.6.self_attn.o_proj.weight']"), %arg405: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.6.self_attn.q_norm.weight']"), %arg406: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.6.self_attn.qkv_proj.weight']"), %arg407: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.7.input_layernorm.weight']"), %arg408: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.7.mlp.experts.w13_weight']"), %arg409: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.7.mlp.experts.w2_weight']"), %arg410: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.7.mlp.gate.weight']"), %arg411: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.7.post_attention_layernorm.weight']"), %arg412: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.7.self_attn.k_norm.weight']"), %arg413: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.7.self_attn.o_proj.weight']"), %arg414: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.7.self_attn.q_norm.weight']"), %arg415: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.7.self_attn.qkv_proj.weight']"), %arg416: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.8.input_layernorm.weight']"), %arg417: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.8.mlp.experts.w13_weight']"), %arg418: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.8.mlp.experts.w2_weight']"), %arg419: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.8.mlp.gate.weight']"), %arg420: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.8.post_attention_layernorm.weight']"), %arg421: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.8.self_attn.k_norm.weight']"), %arg422: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.8.self_attn.o_proj.weight']"), %arg423: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.8.self_attn.q_norm.weight']"), %arg424: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.8.self_attn.qkv_proj.weight']"), %arg425: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.9.input_layernorm.weight']"), %arg426: tensor<128x1536x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.9.mlp.experts.w13_weight']"), %arg427: tensor<128x2048x768xbf16> {mhlo.layout_mode = "{2,1,0:T(8,128)(2,1)}", sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.9.mlp.experts.w2_weight']"), %arg428: tensor<128x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.9.mlp.gate.weight']"), %arg429: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.9.post_attention_layernorm.weight']"), %arg430: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.9.self_attn.k_norm.weight']"), %arg431: tensor<2048x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"model"}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.9.self_attn.o_proj.weight']"), %arg432: tensor<128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.9.self_attn.q_norm.weight']"), %arg433: tensor<5120x2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"model"}, {}]>} loc("params_and_buffers['vllm_model.language_model.model.layers.9.self_attn.qkv_proj.weight']"), %arg434: tensor<2048xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("params_and_buffers['vllm_model.language_model.model.norm.weight']"), %arg435: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[0]"), %arg436: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[1]"), %arg437: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[2]"), %arg438: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[3]"), %arg439: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[4]"), %arg440: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[5]"), %arg441: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[6]"), %arg442: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[7]"), %arg443: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[8]"), %arg444: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[9]"), %arg445: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[10]"), %arg446: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[11]"), %arg447: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[12]"), %arg448: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[13]"), %arg449: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[14]"), %arg450: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[15]"), %arg451: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[16]"), %arg452: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[17]"), %arg453: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[18]"), %arg454: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[19]"), %arg455: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[20]"), %arg456: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[21]"), %arg457: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[22]"), %arg458: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[23]"), %arg459: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[24]"), %arg460: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[25]"), %arg461: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[26]"), %arg462: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[27]"), %arg463: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[28]"), %arg464: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[29]"), %arg465: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[30]"), %arg466: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[31]"), %arg467: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[32]"), %arg468: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[33]"), %arg469: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[34]"), %arg470: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[35]"), %arg471: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[36]"), %arg472: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[37]"), %arg473: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[38]"), %arg474: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[39]"), %arg475: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[40]"), %arg476: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[41]"), %arg477: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[42]"), %arg478: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[43]"), %arg479: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[44]"), %arg480: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[45]"), %arg481: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[46]"), %arg482: tensor<14813x32x4x2x128xbf16> {jax.buffer_donor = true, sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"model"}, {}, {}]>} loc("kv_caches[47]"), %arg483: tensor<16xi32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("input_ids"), %arg484: tensor<16xi32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("attn_metadata.input_positions"), %arg485: tensor<131072xi32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("attn_metadata.block_tables"), %arg486: tensor<256xi32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("attn_metadata.seq_lens"), %arg487: tensor<257xi32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("attn_metadata.query_start_loc"), %arg488: tensor<3xi32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>} loc("attn_metadata.request_distribution")) -> (tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][0]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][1]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][2]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][3]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][4]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][5]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][6]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][7]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][8]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][9]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][10]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][11]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][12]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][13]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][14]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][15]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][16]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][17]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][18]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][19]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][20]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][21]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][22]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][23]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][24]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][25]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][26]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][27]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][28]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][29]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][30]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][31]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][32]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][33]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][34]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][35]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][36]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][37]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][38]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][39]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][40]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][41]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][42]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][43]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][44]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][45]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][46]"}, tensor<14813x32x4x2x128xbf16> {jax.result_info = "result[0][47]"}, tensor<16x2048xbf16> {jax.result_info = "result[1]"}) {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32> loc(#loc)
    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<bf16> loc(#loc)
    %c = stablehlo.constant dense<262144> : tensor<i32> loc(#loc)
    %c_1 = stablehlo.constant dense<0> : tensor<i32> loc(#loc)
    %cst_2 = stablehlo.constant dense<1.280000e+02> : tensor<f32> loc(#loc)
    %cst_3 = stablehlo.constant dense<9.99999997E-7> : tensor<f32> loc(#loc)
    %cst_4 = stablehlo.constant dense<2.048000e+03> : tensor<f32> loc(#loc)
    %cst_5 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %cst_6 = stablehlo.constant dense<2.000000e+00> : tensor<f32> loc(#loc)
    %0 = call @_take(%arg0, %arg483) : (tensor<152064x2048xbf16>, tensor<16xi32>) -> tensor<16x2048xbf16> loc(#loc1671)
    %1 = stablehlo.convert %0 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %2 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1673)
    %3 = stablehlo.power %1, %2 : tensor<16x2048xf32> loc(#loc1673)
    %4 = stablehlo.reduce(%3 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1674)
    %5 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1675)
    %6 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1676)
    %7 = stablehlo.divide %5, %6 : tensor<16x1xf32> loc(#loc1676)
    %8 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1677)
    %9 = stablehlo.add %7, %8 : tensor<16x1xf32> loc(#loc1677)
    %10 = stablehlo.rsqrt %9 : tensor<16x1xf32> loc(#loc1678)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1679)
    %12 = stablehlo.multiply %1, %11 : tensor<16x2048xf32> loc(#loc1679)
    %13 = stablehlo.convert %12 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %14 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1681)
    %15 = stablehlo.broadcast_in_dim %14, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1682)
    %16 = stablehlo.multiply %13, %15 : tensor<16x2048xbf16> loc(#loc1682)
    %17 = stablehlo.dot_general %16, %arg9, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %18 = stablehlo.reshape %17 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %19 = stablehlo.slice %18 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %20 = stablehlo.reshape %19 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %21 = stablehlo.slice %18 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %22 = stablehlo.reshape %21 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %23 = stablehlo.slice %18 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %24 = stablehlo.reshape %23 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %25 = stablehlo.concatenate %20, %22, %24, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %26 = stablehlo.slice %25 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %27 = stablehlo.slice %25 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %28 = stablehlo.slice %25 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %29 = stablehlo.reshape %26 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %30 = stablehlo.convert %29 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %31 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %32 = stablehlo.power %30, %31 : tensor<16x32x128xf32> loc(#loc1690)
    %33 = stablehlo.reduce(%32 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %34 = stablehlo.broadcast_in_dim %33, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %35 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %36 = stablehlo.divide %34, %35 : tensor<16x32x1xf32> loc(#loc1693)
    %37 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %38 = stablehlo.add %36, %37 : tensor<16x32x1xf32> loc(#loc1694)
    %39 = stablehlo.rsqrt %38 : tensor<16x32x1xf32> loc(#loc1678)
    %40 = stablehlo.broadcast_in_dim %39, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %41 = stablehlo.multiply %30, %40 : tensor<16x32x128xf32> loc(#loc1695)
    %42 = stablehlo.convert %41 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %43 = stablehlo.broadcast_in_dim %arg8, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %44 = stablehlo.broadcast_in_dim %43, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %45 = stablehlo.multiply %42, %44 : tensor<16x32x128xbf16> loc(#loc1697)
    %46 = stablehlo.reshape %45 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %47 = stablehlo.reshape %27 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %48 = stablehlo.convert %47 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %49 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %50 = stablehlo.power %48, %49 : tensor<16x4x128xf32> loc(#loc1700)
    %51 = stablehlo.reduce(%50 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %53 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %54 = stablehlo.divide %52, %53 : tensor<16x4x1xf32> loc(#loc1703)
    %55 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %56 = stablehlo.add %54, %55 : tensor<16x4x1xf32> loc(#loc1704)
    %57 = stablehlo.rsqrt %56 : tensor<16x4x1xf32> loc(#loc1678)
    %58 = stablehlo.broadcast_in_dim %57, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %59 = stablehlo.multiply %48, %58 : tensor<16x4x128xf32> loc(#loc1705)
    %60 = stablehlo.convert %59 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %61 = stablehlo.broadcast_in_dim %arg6, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %62 = stablehlo.broadcast_in_dim %61, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %63 = stablehlo.multiply %60, %62 : tensor<16x4x128xbf16> loc(#loc1707)
    %64 = stablehlo.reshape %63 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %65 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %66 = stablehlo.compare  LT, %arg484, %65,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %67 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %68 = stablehlo.add %arg484, %67 : tensor<16xi32> loc(#loc1710)
    %69 = stablehlo.select %66, %68, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %70 = stablehlo.broadcast_in_dim %69, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %71 = "stablehlo.gather"(%arg10, %70) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %72 = stablehlo.slice %71 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %73 = stablehlo.slice %71 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %74 = stablehlo.reshape %46 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %75 = stablehlo.broadcast_in_dim %72, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %76 = stablehlo.broadcast_in_dim %73, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %77 = stablehlo.slice %74 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %78 = stablehlo.slice %74 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %79 = stablehlo.broadcast_in_dim %75, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %80 = stablehlo.multiply %77, %79 : tensor<16x32x64xbf16> loc(#loc1719)
    %81 = stablehlo.broadcast_in_dim %76, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %82 = stablehlo.multiply %78, %81 : tensor<16x32x64xbf16> loc(#loc1720)
    %83 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %84 = stablehlo.multiply %82, %83 : tensor<16x32x64xbf16> loc(#loc1721)
    %85 = stablehlo.subtract %80, %84 : tensor<16x32x64xbf16> loc(#loc1722)
    %86 = stablehlo.broadcast_in_dim %75, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %87 = stablehlo.multiply %78, %86 : tensor<16x32x64xbf16> loc(#loc1723)
    %88 = stablehlo.broadcast_in_dim %76, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %89 = stablehlo.multiply %77, %88 : tensor<16x32x64xbf16> loc(#loc1724)
    %90 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %91 = stablehlo.multiply %89, %90 : tensor<16x32x64xbf16> loc(#loc1725)
    %92 = stablehlo.add %87, %91 : tensor<16x32x64xbf16> loc(#loc1726)
    %93 = stablehlo.concatenate %85, %92, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %94 = stablehlo.slice %74 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %95 = stablehlo.concatenate %93, %94, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %96 = stablehlo.reshape %95 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %97 = stablehlo.reshape %64 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %98 = stablehlo.broadcast_in_dim %72, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %99 = stablehlo.broadcast_in_dim %73, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %100 = stablehlo.slice %97 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %101 = stablehlo.slice %97 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %102 = stablehlo.broadcast_in_dim %98, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %103 = stablehlo.multiply %100, %102 : tensor<16x4x64xbf16> loc(#loc1735)
    %104 = stablehlo.broadcast_in_dim %99, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %105 = stablehlo.multiply %101, %104 : tensor<16x4x64xbf16> loc(#loc1736)
    %106 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %107 = stablehlo.multiply %105, %106 : tensor<16x4x64xbf16> loc(#loc1737)
    %108 = stablehlo.subtract %103, %107 : tensor<16x4x64xbf16> loc(#loc1738)
    %109 = stablehlo.broadcast_in_dim %98, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %110 = stablehlo.multiply %101, %109 : tensor<16x4x64xbf16> loc(#loc1739)
    %111 = stablehlo.broadcast_in_dim %99, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %112 = stablehlo.multiply %100, %111 : tensor<16x4x64xbf16> loc(#loc1740)
    %113 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %114 = stablehlo.multiply %112, %113 : tensor<16x4x64xbf16> loc(#loc1741)
    %115 = stablehlo.add %110, %114 : tensor<16x4x64xbf16> loc(#loc1742)
    %116 = stablehlo.concatenate %108, %115, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %117 = stablehlo.slice %97 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %118 = stablehlo.concatenate %116, %117, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %119 = stablehlo.reshape %118 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %120:2 = call @_jax_attn_func(%arg435, %96, %119, %28, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %121 = stablehlo.dot_general %120#1, %arg7, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %122 = stablehlo.reshape %121 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %123 = stablehlo.reshape %122 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %124 = stablehlo.convert %123 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %125 = stablehlo.convert %0 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %126 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %127 = stablehlo.multiply %125, %126 : tensor<16x2048xf32> loc(#loc1751)
    %128 = stablehlo.add %124, %127 : tensor<16x2048xf32> loc(#loc1752)
    %129 = stablehlo.convert %128 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %130 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %131 = stablehlo.power %128, %130 : tensor<16x2048xf32> loc(#loc1754)
    %132 = stablehlo.reduce(%131 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %133 = stablehlo.broadcast_in_dim %132, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %134 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %135 = stablehlo.divide %133, %134 : tensor<16x1xf32> loc(#loc1757)
    %136 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %137 = stablehlo.add %135, %136 : tensor<16x1xf32> loc(#loc1758)
    %138 = stablehlo.rsqrt %137 : tensor<16x1xf32> loc(#loc1678)
    %139 = stablehlo.broadcast_in_dim %138, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %140 = stablehlo.multiply %128, %139 : tensor<16x2048xf32> loc(#loc1759)
    %141 = stablehlo.convert %140 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %142 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %143 = stablehlo.broadcast_in_dim %142, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %144 = stablehlo.multiply %141, %143 : tensor<16x2048xbf16> loc(#loc1761)
    %145 = stablehlo.dot_general %144, %arg4, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %146 = stablehlo.reshape %145 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %147 = stablehlo.reshape %146 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %148 = call @jax_fused_moe_func_padded(%144, %arg2, %arg3, %147) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %149 = stablehlo.convert %148 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %150 = stablehlo.convert %129 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %151 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %152 = stablehlo.multiply %150, %151 : tensor<16x2048xf32> loc(#loc1766)
    %153 = stablehlo.add %149, %152 : tensor<16x2048xf32> loc(#loc1767)
    %154 = stablehlo.convert %153 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %155 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %156 = stablehlo.power %153, %155 : tensor<16x2048xf32> loc(#loc1768)
    %157 = stablehlo.reduce(%156 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %158 = stablehlo.broadcast_in_dim %157, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %159 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %160 = stablehlo.divide %158, %159 : tensor<16x1xf32> loc(#loc1771)
    %161 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %162 = stablehlo.add %160, %161 : tensor<16x1xf32> loc(#loc1772)
    %163 = stablehlo.rsqrt %162 : tensor<16x1xf32> loc(#loc1678)
    %164 = stablehlo.broadcast_in_dim %163, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %165 = stablehlo.multiply %153, %164 : tensor<16x2048xf32> loc(#loc1773)
    %166 = stablehlo.convert %165 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %167 = stablehlo.broadcast_in_dim %arg11, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %168 = stablehlo.broadcast_in_dim %167, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %169 = stablehlo.multiply %166, %168 : tensor<16x2048xbf16> loc(#loc1775)
    %170 = stablehlo.dot_general %169, %arg19, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %171 = stablehlo.reshape %170 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %172 = stablehlo.slice %171 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %173 = stablehlo.reshape %172 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %174 = stablehlo.slice %171 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %175 = stablehlo.reshape %174 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %176 = stablehlo.slice %171 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %177 = stablehlo.reshape %176 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %178 = stablehlo.concatenate %173, %175, %177, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %179 = stablehlo.slice %178 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %180 = stablehlo.slice %178 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %181 = stablehlo.slice %178 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %182 = stablehlo.reshape %179 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %183 = stablehlo.convert %182 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %184 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %185 = stablehlo.power %183, %184 : tensor<16x32x128xf32> loc(#loc1690)
    %186 = stablehlo.reduce(%185 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %187 = stablehlo.broadcast_in_dim %186, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %188 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %189 = stablehlo.divide %187, %188 : tensor<16x32x1xf32> loc(#loc1693)
    %190 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %191 = stablehlo.add %189, %190 : tensor<16x32x1xf32> loc(#loc1694)
    %192 = stablehlo.rsqrt %191 : tensor<16x32x1xf32> loc(#loc1678)
    %193 = stablehlo.broadcast_in_dim %192, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %194 = stablehlo.multiply %183, %193 : tensor<16x32x128xf32> loc(#loc1695)
    %195 = stablehlo.convert %194 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %196 = stablehlo.broadcast_in_dim %arg18, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %197 = stablehlo.broadcast_in_dim %196, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %198 = stablehlo.multiply %195, %197 : tensor<16x32x128xbf16> loc(#loc1697)
    %199 = stablehlo.reshape %198 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %200 = stablehlo.reshape %180 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %201 = stablehlo.convert %200 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %202 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %203 = stablehlo.power %201, %202 : tensor<16x4x128xf32> loc(#loc1700)
    %204 = stablehlo.reduce(%203 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %205 = stablehlo.broadcast_in_dim %204, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %206 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %207 = stablehlo.divide %205, %206 : tensor<16x4x1xf32> loc(#loc1703)
    %208 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %209 = stablehlo.add %207, %208 : tensor<16x4x1xf32> loc(#loc1704)
    %210 = stablehlo.rsqrt %209 : tensor<16x4x1xf32> loc(#loc1678)
    %211 = stablehlo.broadcast_in_dim %210, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %212 = stablehlo.multiply %201, %211 : tensor<16x4x128xf32> loc(#loc1705)
    %213 = stablehlo.convert %212 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %214 = stablehlo.broadcast_in_dim %arg16, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %215 = stablehlo.broadcast_in_dim %214, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %216 = stablehlo.multiply %213, %215 : tensor<16x4x128xbf16> loc(#loc1707)
    %217 = stablehlo.reshape %216 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %218 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %219 = stablehlo.compare  LT, %arg484, %218,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %220 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %221 = stablehlo.add %arg484, %220 : tensor<16xi32> loc(#loc1710)
    %222 = stablehlo.select %219, %221, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %223 = stablehlo.broadcast_in_dim %222, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %224 = "stablehlo.gather"(%arg10, %223) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %225 = stablehlo.slice %224 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %226 = stablehlo.slice %224 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %227 = stablehlo.reshape %199 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %228 = stablehlo.broadcast_in_dim %225, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %229 = stablehlo.broadcast_in_dim %226, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %230 = stablehlo.slice %227 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %231 = stablehlo.slice %227 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %232 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %233 = stablehlo.multiply %230, %232 : tensor<16x32x64xbf16> loc(#loc1719)
    %234 = stablehlo.broadcast_in_dim %229, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %235 = stablehlo.multiply %231, %234 : tensor<16x32x64xbf16> loc(#loc1720)
    %236 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %237 = stablehlo.multiply %235, %236 : tensor<16x32x64xbf16> loc(#loc1721)
    %238 = stablehlo.subtract %233, %237 : tensor<16x32x64xbf16> loc(#loc1722)
    %239 = stablehlo.broadcast_in_dim %228, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %240 = stablehlo.multiply %231, %239 : tensor<16x32x64xbf16> loc(#loc1723)
    %241 = stablehlo.broadcast_in_dim %229, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %242 = stablehlo.multiply %230, %241 : tensor<16x32x64xbf16> loc(#loc1724)
    %243 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %244 = stablehlo.multiply %242, %243 : tensor<16x32x64xbf16> loc(#loc1725)
    %245 = stablehlo.add %240, %244 : tensor<16x32x64xbf16> loc(#loc1726)
    %246 = stablehlo.concatenate %238, %245, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %247 = stablehlo.slice %227 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %248 = stablehlo.concatenate %246, %247, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %249 = stablehlo.reshape %248 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %250 = stablehlo.reshape %217 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %251 = stablehlo.broadcast_in_dim %225, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %252 = stablehlo.broadcast_in_dim %226, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %253 = stablehlo.slice %250 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %254 = stablehlo.slice %250 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %255 = stablehlo.broadcast_in_dim %251, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %256 = stablehlo.multiply %253, %255 : tensor<16x4x64xbf16> loc(#loc1735)
    %257 = stablehlo.broadcast_in_dim %252, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %258 = stablehlo.multiply %254, %257 : tensor<16x4x64xbf16> loc(#loc1736)
    %259 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %260 = stablehlo.multiply %258, %259 : tensor<16x4x64xbf16> loc(#loc1737)
    %261 = stablehlo.subtract %256, %260 : tensor<16x4x64xbf16> loc(#loc1738)
    %262 = stablehlo.broadcast_in_dim %251, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %263 = stablehlo.multiply %254, %262 : tensor<16x4x64xbf16> loc(#loc1739)
    %264 = stablehlo.broadcast_in_dim %252, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %265 = stablehlo.multiply %253, %264 : tensor<16x4x64xbf16> loc(#loc1740)
    %266 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %267 = stablehlo.multiply %265, %266 : tensor<16x4x64xbf16> loc(#loc1741)
    %268 = stablehlo.add %263, %267 : tensor<16x4x64xbf16> loc(#loc1742)
    %269 = stablehlo.concatenate %261, %268, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %270 = stablehlo.slice %250 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %271 = stablehlo.concatenate %269, %270, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %272 = stablehlo.reshape %271 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %273:2 = call @_jax_attn_func(%arg436, %249, %272, %181, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %274 = stablehlo.dot_general %273#1, %arg17, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %275 = stablehlo.reshape %274 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %276 = stablehlo.reshape %275 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %277 = stablehlo.convert %276 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %278 = stablehlo.convert %154 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %279 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %280 = stablehlo.multiply %278, %279 : tensor<16x2048xf32> loc(#loc1751)
    %281 = stablehlo.add %277, %280 : tensor<16x2048xf32> loc(#loc1752)
    %282 = stablehlo.convert %281 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %283 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %284 = stablehlo.power %281, %283 : tensor<16x2048xf32> loc(#loc1754)
    %285 = stablehlo.reduce(%284 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %286 = stablehlo.broadcast_in_dim %285, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %287 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %288 = stablehlo.divide %286, %287 : tensor<16x1xf32> loc(#loc1757)
    %289 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %290 = stablehlo.add %288, %289 : tensor<16x1xf32> loc(#loc1758)
    %291 = stablehlo.rsqrt %290 : tensor<16x1xf32> loc(#loc1678)
    %292 = stablehlo.broadcast_in_dim %291, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %293 = stablehlo.multiply %281, %292 : tensor<16x2048xf32> loc(#loc1759)
    %294 = stablehlo.convert %293 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %295 = stablehlo.broadcast_in_dim %arg15, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %296 = stablehlo.broadcast_in_dim %295, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %297 = stablehlo.multiply %294, %296 : tensor<16x2048xbf16> loc(#loc1761)
    %298 = stablehlo.dot_general %297, %arg14, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %299 = stablehlo.reshape %298 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %300 = stablehlo.reshape %299 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %301 = call @jax_fused_moe_func_padded(%297, %arg12, %arg13, %300) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %302 = stablehlo.convert %301 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %303 = stablehlo.convert %282 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %304 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %305 = stablehlo.multiply %303, %304 : tensor<16x2048xf32> loc(#loc1766)
    %306 = stablehlo.add %302, %305 : tensor<16x2048xf32> loc(#loc1767)
    %307 = stablehlo.convert %306 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %308 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %309 = stablehlo.power %306, %308 : tensor<16x2048xf32> loc(#loc1768)
    %310 = stablehlo.reduce(%309 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %311 = stablehlo.broadcast_in_dim %310, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %312 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %313 = stablehlo.divide %311, %312 : tensor<16x1xf32> loc(#loc1771)
    %314 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %315 = stablehlo.add %313, %314 : tensor<16x1xf32> loc(#loc1772)
    %316 = stablehlo.rsqrt %315 : tensor<16x1xf32> loc(#loc1678)
    %317 = stablehlo.broadcast_in_dim %316, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %318 = stablehlo.multiply %306, %317 : tensor<16x2048xf32> loc(#loc1773)
    %319 = stablehlo.convert %318 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %320 = stablehlo.broadcast_in_dim %arg110, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %321 = stablehlo.broadcast_in_dim %320, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %322 = stablehlo.multiply %319, %321 : tensor<16x2048xbf16> loc(#loc1775)
    %323 = stablehlo.dot_general %322, %arg118, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %324 = stablehlo.reshape %323 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %325 = stablehlo.slice %324 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %326 = stablehlo.reshape %325 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %327 = stablehlo.slice %324 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %328 = stablehlo.reshape %327 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %329 = stablehlo.slice %324 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %330 = stablehlo.reshape %329 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %331 = stablehlo.concatenate %326, %328, %330, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %332 = stablehlo.slice %331 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %333 = stablehlo.slice %331 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %334 = stablehlo.slice %331 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %335 = stablehlo.reshape %332 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %336 = stablehlo.convert %335 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %337 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %338 = stablehlo.power %336, %337 : tensor<16x32x128xf32> loc(#loc1690)
    %339 = stablehlo.reduce(%338 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %340 = stablehlo.broadcast_in_dim %339, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %341 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %342 = stablehlo.divide %340, %341 : tensor<16x32x1xf32> loc(#loc1693)
    %343 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %344 = stablehlo.add %342, %343 : tensor<16x32x1xf32> loc(#loc1694)
    %345 = stablehlo.rsqrt %344 : tensor<16x32x1xf32> loc(#loc1678)
    %346 = stablehlo.broadcast_in_dim %345, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %347 = stablehlo.multiply %336, %346 : tensor<16x32x128xf32> loc(#loc1695)
    %348 = stablehlo.convert %347 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %349 = stablehlo.broadcast_in_dim %arg117, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %350 = stablehlo.broadcast_in_dim %349, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %351 = stablehlo.multiply %348, %350 : tensor<16x32x128xbf16> loc(#loc1697)
    %352 = stablehlo.reshape %351 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %353 = stablehlo.reshape %333 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %354 = stablehlo.convert %353 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %355 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %356 = stablehlo.power %354, %355 : tensor<16x4x128xf32> loc(#loc1700)
    %357 = stablehlo.reduce(%356 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %358 = stablehlo.broadcast_in_dim %357, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %359 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %360 = stablehlo.divide %358, %359 : tensor<16x4x1xf32> loc(#loc1703)
    %361 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %362 = stablehlo.add %360, %361 : tensor<16x4x1xf32> loc(#loc1704)
    %363 = stablehlo.rsqrt %362 : tensor<16x4x1xf32> loc(#loc1678)
    %364 = stablehlo.broadcast_in_dim %363, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %365 = stablehlo.multiply %354, %364 : tensor<16x4x128xf32> loc(#loc1705)
    %366 = stablehlo.convert %365 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %367 = stablehlo.broadcast_in_dim %arg115, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %368 = stablehlo.broadcast_in_dim %367, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %369 = stablehlo.multiply %366, %368 : tensor<16x4x128xbf16> loc(#loc1707)
    %370 = stablehlo.reshape %369 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %371 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %372 = stablehlo.compare  LT, %arg484, %371,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %373 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %374 = stablehlo.add %arg484, %373 : tensor<16xi32> loc(#loc1710)
    %375 = stablehlo.select %372, %374, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %376 = stablehlo.broadcast_in_dim %375, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %377 = "stablehlo.gather"(%arg10, %376) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %378 = stablehlo.slice %377 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %379 = stablehlo.slice %377 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %380 = stablehlo.reshape %352 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %381 = stablehlo.broadcast_in_dim %378, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %382 = stablehlo.broadcast_in_dim %379, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %383 = stablehlo.slice %380 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %384 = stablehlo.slice %380 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %385 = stablehlo.broadcast_in_dim %381, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %386 = stablehlo.multiply %383, %385 : tensor<16x32x64xbf16> loc(#loc1719)
    %387 = stablehlo.broadcast_in_dim %382, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %388 = stablehlo.multiply %384, %387 : tensor<16x32x64xbf16> loc(#loc1720)
    %389 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %390 = stablehlo.multiply %388, %389 : tensor<16x32x64xbf16> loc(#loc1721)
    %391 = stablehlo.subtract %386, %390 : tensor<16x32x64xbf16> loc(#loc1722)
    %392 = stablehlo.broadcast_in_dim %381, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %393 = stablehlo.multiply %384, %392 : tensor<16x32x64xbf16> loc(#loc1723)
    %394 = stablehlo.broadcast_in_dim %382, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %395 = stablehlo.multiply %383, %394 : tensor<16x32x64xbf16> loc(#loc1724)
    %396 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %397 = stablehlo.multiply %395, %396 : tensor<16x32x64xbf16> loc(#loc1725)
    %398 = stablehlo.add %393, %397 : tensor<16x32x64xbf16> loc(#loc1726)
    %399 = stablehlo.concatenate %391, %398, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %400 = stablehlo.slice %380 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %401 = stablehlo.concatenate %399, %400, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %402 = stablehlo.reshape %401 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %403 = stablehlo.reshape %370 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %404 = stablehlo.broadcast_in_dim %378, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %405 = stablehlo.broadcast_in_dim %379, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %406 = stablehlo.slice %403 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %407 = stablehlo.slice %403 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %408 = stablehlo.broadcast_in_dim %404, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %409 = stablehlo.multiply %406, %408 : tensor<16x4x64xbf16> loc(#loc1735)
    %410 = stablehlo.broadcast_in_dim %405, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %411 = stablehlo.multiply %407, %410 : tensor<16x4x64xbf16> loc(#loc1736)
    %412 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %413 = stablehlo.multiply %411, %412 : tensor<16x4x64xbf16> loc(#loc1737)
    %414 = stablehlo.subtract %409, %413 : tensor<16x4x64xbf16> loc(#loc1738)
    %415 = stablehlo.broadcast_in_dim %404, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %416 = stablehlo.multiply %407, %415 : tensor<16x4x64xbf16> loc(#loc1739)
    %417 = stablehlo.broadcast_in_dim %405, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %418 = stablehlo.multiply %406, %417 : tensor<16x4x64xbf16> loc(#loc1740)
    %419 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %420 = stablehlo.multiply %418, %419 : tensor<16x4x64xbf16> loc(#loc1741)
    %421 = stablehlo.add %416, %420 : tensor<16x4x64xbf16> loc(#loc1742)
    %422 = stablehlo.concatenate %414, %421, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %423 = stablehlo.slice %403 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %424 = stablehlo.concatenate %422, %423, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %425 = stablehlo.reshape %424 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %426:2 = call @_jax_attn_func(%arg437, %402, %425, %334, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %427 = stablehlo.dot_general %426#1, %arg116, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %428 = stablehlo.reshape %427 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %429 = stablehlo.reshape %428 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %430 = stablehlo.convert %429 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %431 = stablehlo.convert %307 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %432 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %433 = stablehlo.multiply %431, %432 : tensor<16x2048xf32> loc(#loc1751)
    %434 = stablehlo.add %430, %433 : tensor<16x2048xf32> loc(#loc1752)
    %435 = stablehlo.convert %434 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %436 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %437 = stablehlo.power %434, %436 : tensor<16x2048xf32> loc(#loc1754)
    %438 = stablehlo.reduce(%437 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %439 = stablehlo.broadcast_in_dim %438, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %440 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %441 = stablehlo.divide %439, %440 : tensor<16x1xf32> loc(#loc1757)
    %442 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %443 = stablehlo.add %441, %442 : tensor<16x1xf32> loc(#loc1758)
    %444 = stablehlo.rsqrt %443 : tensor<16x1xf32> loc(#loc1678)
    %445 = stablehlo.broadcast_in_dim %444, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %446 = stablehlo.multiply %434, %445 : tensor<16x2048xf32> loc(#loc1759)
    %447 = stablehlo.convert %446 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %448 = stablehlo.broadcast_in_dim %arg114, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %449 = stablehlo.broadcast_in_dim %448, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %450 = stablehlo.multiply %447, %449 : tensor<16x2048xbf16> loc(#loc1761)
    %451 = stablehlo.dot_general %450, %arg113, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %452 = stablehlo.reshape %451 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %453 = stablehlo.reshape %452 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %454 = call @jax_fused_moe_func_padded(%450, %arg111, %arg112, %453) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %455 = stablehlo.convert %454 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %456 = stablehlo.convert %435 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %457 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %458 = stablehlo.multiply %456, %457 : tensor<16x2048xf32> loc(#loc1766)
    %459 = stablehlo.add %455, %458 : tensor<16x2048xf32> loc(#loc1767)
    %460 = stablehlo.convert %459 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %461 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %462 = stablehlo.power %459, %461 : tensor<16x2048xf32> loc(#loc1768)
    %463 = stablehlo.reduce(%462 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %464 = stablehlo.broadcast_in_dim %463, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %465 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %466 = stablehlo.divide %464, %465 : tensor<16x1xf32> loc(#loc1771)
    %467 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %468 = stablehlo.add %466, %467 : tensor<16x1xf32> loc(#loc1772)
    %469 = stablehlo.rsqrt %468 : tensor<16x1xf32> loc(#loc1678)
    %470 = stablehlo.broadcast_in_dim %469, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %471 = stablehlo.multiply %459, %470 : tensor<16x2048xf32> loc(#loc1773)
    %472 = stablehlo.convert %471 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %473 = stablehlo.broadcast_in_dim %arg209, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %474 = stablehlo.broadcast_in_dim %473, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %475 = stablehlo.multiply %472, %474 : tensor<16x2048xbf16> loc(#loc1775)
    %476 = stablehlo.dot_general %475, %arg217, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %477 = stablehlo.reshape %476 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %478 = stablehlo.slice %477 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %479 = stablehlo.reshape %478 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %480 = stablehlo.slice %477 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %481 = stablehlo.reshape %480 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %482 = stablehlo.slice %477 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %483 = stablehlo.reshape %482 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %484 = stablehlo.concatenate %479, %481, %483, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %485 = stablehlo.slice %484 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %486 = stablehlo.slice %484 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %487 = stablehlo.slice %484 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %488 = stablehlo.reshape %485 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %489 = stablehlo.convert %488 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %490 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %491 = stablehlo.power %489, %490 : tensor<16x32x128xf32> loc(#loc1690)
    %492 = stablehlo.reduce(%491 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %493 = stablehlo.broadcast_in_dim %492, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %494 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %495 = stablehlo.divide %493, %494 : tensor<16x32x1xf32> loc(#loc1693)
    %496 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %497 = stablehlo.add %495, %496 : tensor<16x32x1xf32> loc(#loc1694)
    %498 = stablehlo.rsqrt %497 : tensor<16x32x1xf32> loc(#loc1678)
    %499 = stablehlo.broadcast_in_dim %498, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %500 = stablehlo.multiply %489, %499 : tensor<16x32x128xf32> loc(#loc1695)
    %501 = stablehlo.convert %500 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %502 = stablehlo.broadcast_in_dim %arg216, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %503 = stablehlo.broadcast_in_dim %502, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %504 = stablehlo.multiply %501, %503 : tensor<16x32x128xbf16> loc(#loc1697)
    %505 = stablehlo.reshape %504 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %506 = stablehlo.reshape %486 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %507 = stablehlo.convert %506 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %508 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %509 = stablehlo.power %507, %508 : tensor<16x4x128xf32> loc(#loc1700)
    %510 = stablehlo.reduce(%509 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %511 = stablehlo.broadcast_in_dim %510, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %512 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %513 = stablehlo.divide %511, %512 : tensor<16x4x1xf32> loc(#loc1703)
    %514 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %515 = stablehlo.add %513, %514 : tensor<16x4x1xf32> loc(#loc1704)
    %516 = stablehlo.rsqrt %515 : tensor<16x4x1xf32> loc(#loc1678)
    %517 = stablehlo.broadcast_in_dim %516, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %518 = stablehlo.multiply %507, %517 : tensor<16x4x128xf32> loc(#loc1705)
    %519 = stablehlo.convert %518 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %520 = stablehlo.broadcast_in_dim %arg214, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %521 = stablehlo.broadcast_in_dim %520, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %522 = stablehlo.multiply %519, %521 : tensor<16x4x128xbf16> loc(#loc1707)
    %523 = stablehlo.reshape %522 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %524 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %525 = stablehlo.compare  LT, %arg484, %524,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %526 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %527 = stablehlo.add %arg484, %526 : tensor<16xi32> loc(#loc1710)
    %528 = stablehlo.select %525, %527, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %529 = stablehlo.broadcast_in_dim %528, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %530 = "stablehlo.gather"(%arg10, %529) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %531 = stablehlo.slice %530 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %532 = stablehlo.slice %530 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %533 = stablehlo.reshape %505 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %534 = stablehlo.broadcast_in_dim %531, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %535 = stablehlo.broadcast_in_dim %532, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %536 = stablehlo.slice %533 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %537 = stablehlo.slice %533 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %538 = stablehlo.broadcast_in_dim %534, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %539 = stablehlo.multiply %536, %538 : tensor<16x32x64xbf16> loc(#loc1719)
    %540 = stablehlo.broadcast_in_dim %535, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %541 = stablehlo.multiply %537, %540 : tensor<16x32x64xbf16> loc(#loc1720)
    %542 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %543 = stablehlo.multiply %541, %542 : tensor<16x32x64xbf16> loc(#loc1721)
    %544 = stablehlo.subtract %539, %543 : tensor<16x32x64xbf16> loc(#loc1722)
    %545 = stablehlo.broadcast_in_dim %534, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %546 = stablehlo.multiply %537, %545 : tensor<16x32x64xbf16> loc(#loc1723)
    %547 = stablehlo.broadcast_in_dim %535, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %548 = stablehlo.multiply %536, %547 : tensor<16x32x64xbf16> loc(#loc1724)
    %549 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %550 = stablehlo.multiply %548, %549 : tensor<16x32x64xbf16> loc(#loc1725)
    %551 = stablehlo.add %546, %550 : tensor<16x32x64xbf16> loc(#loc1726)
    %552 = stablehlo.concatenate %544, %551, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %553 = stablehlo.slice %533 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %554 = stablehlo.concatenate %552, %553, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %555 = stablehlo.reshape %554 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %556 = stablehlo.reshape %523 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %557 = stablehlo.broadcast_in_dim %531, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %558 = stablehlo.broadcast_in_dim %532, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %559 = stablehlo.slice %556 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %560 = stablehlo.slice %556 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %561 = stablehlo.broadcast_in_dim %557, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %562 = stablehlo.multiply %559, %561 : tensor<16x4x64xbf16> loc(#loc1735)
    %563 = stablehlo.broadcast_in_dim %558, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %564 = stablehlo.multiply %560, %563 : tensor<16x4x64xbf16> loc(#loc1736)
    %565 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %566 = stablehlo.multiply %564, %565 : tensor<16x4x64xbf16> loc(#loc1737)
    %567 = stablehlo.subtract %562, %566 : tensor<16x4x64xbf16> loc(#loc1738)
    %568 = stablehlo.broadcast_in_dim %557, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %569 = stablehlo.multiply %560, %568 : tensor<16x4x64xbf16> loc(#loc1739)
    %570 = stablehlo.broadcast_in_dim %558, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %571 = stablehlo.multiply %559, %570 : tensor<16x4x64xbf16> loc(#loc1740)
    %572 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %573 = stablehlo.multiply %571, %572 : tensor<16x4x64xbf16> loc(#loc1741)
    %574 = stablehlo.add %569, %573 : tensor<16x4x64xbf16> loc(#loc1742)
    %575 = stablehlo.concatenate %567, %574, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %576 = stablehlo.slice %556 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %577 = stablehlo.concatenate %575, %576, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %578 = stablehlo.reshape %577 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %579:2 = call @_jax_attn_func(%arg438, %555, %578, %487, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %580 = stablehlo.dot_general %579#1, %arg215, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %581 = stablehlo.reshape %580 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %582 = stablehlo.reshape %581 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %583 = stablehlo.convert %582 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %584 = stablehlo.convert %460 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %585 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %586 = stablehlo.multiply %584, %585 : tensor<16x2048xf32> loc(#loc1751)
    %587 = stablehlo.add %583, %586 : tensor<16x2048xf32> loc(#loc1752)
    %588 = stablehlo.convert %587 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %589 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %590 = stablehlo.power %587, %589 : tensor<16x2048xf32> loc(#loc1754)
    %591 = stablehlo.reduce(%590 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %592 = stablehlo.broadcast_in_dim %591, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %593 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %594 = stablehlo.divide %592, %593 : tensor<16x1xf32> loc(#loc1757)
    %595 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %596 = stablehlo.add %594, %595 : tensor<16x1xf32> loc(#loc1758)
    %597 = stablehlo.rsqrt %596 : tensor<16x1xf32> loc(#loc1678)
    %598 = stablehlo.broadcast_in_dim %597, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %599 = stablehlo.multiply %587, %598 : tensor<16x2048xf32> loc(#loc1759)
    %600 = stablehlo.convert %599 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %601 = stablehlo.broadcast_in_dim %arg213, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %602 = stablehlo.broadcast_in_dim %601, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %603 = stablehlo.multiply %600, %602 : tensor<16x2048xbf16> loc(#loc1761)
    %604 = stablehlo.dot_general %603, %arg212, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %605 = stablehlo.reshape %604 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %606 = stablehlo.reshape %605 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %607 = call @jax_fused_moe_func_padded(%603, %arg210, %arg211, %606) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %608 = stablehlo.convert %607 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %609 = stablehlo.convert %588 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %610 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %611 = stablehlo.multiply %609, %610 : tensor<16x2048xf32> loc(#loc1766)
    %612 = stablehlo.add %608, %611 : tensor<16x2048xf32> loc(#loc1767)
    %613 = stablehlo.convert %612 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %614 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %615 = stablehlo.power %612, %614 : tensor<16x2048xf32> loc(#loc1768)
    %616 = stablehlo.reduce(%615 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %617 = stablehlo.broadcast_in_dim %616, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %618 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %619 = stablehlo.divide %617, %618 : tensor<16x1xf32> loc(#loc1771)
    %620 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %621 = stablehlo.add %619, %620 : tensor<16x1xf32> loc(#loc1772)
    %622 = stablehlo.rsqrt %621 : tensor<16x1xf32> loc(#loc1678)
    %623 = stablehlo.broadcast_in_dim %622, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %624 = stablehlo.multiply %612, %623 : tensor<16x2048xf32> loc(#loc1773)
    %625 = stablehlo.convert %624 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %626 = stablehlo.broadcast_in_dim %arg308, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %627 = stablehlo.broadcast_in_dim %626, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %628 = stablehlo.multiply %625, %627 : tensor<16x2048xbf16> loc(#loc1775)
    %629 = stablehlo.dot_general %628, %arg316, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %630 = stablehlo.reshape %629 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %631 = stablehlo.slice %630 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %632 = stablehlo.reshape %631 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %633 = stablehlo.slice %630 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %634 = stablehlo.reshape %633 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %635 = stablehlo.slice %630 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %636 = stablehlo.reshape %635 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %637 = stablehlo.concatenate %632, %634, %636, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %638 = stablehlo.slice %637 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %639 = stablehlo.slice %637 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %640 = stablehlo.slice %637 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %641 = stablehlo.reshape %638 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %642 = stablehlo.convert %641 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %643 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %644 = stablehlo.power %642, %643 : tensor<16x32x128xf32> loc(#loc1690)
    %645 = stablehlo.reduce(%644 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %646 = stablehlo.broadcast_in_dim %645, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %647 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %648 = stablehlo.divide %646, %647 : tensor<16x32x1xf32> loc(#loc1693)
    %649 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %650 = stablehlo.add %648, %649 : tensor<16x32x1xf32> loc(#loc1694)
    %651 = stablehlo.rsqrt %650 : tensor<16x32x1xf32> loc(#loc1678)
    %652 = stablehlo.broadcast_in_dim %651, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %653 = stablehlo.multiply %642, %652 : tensor<16x32x128xf32> loc(#loc1695)
    %654 = stablehlo.convert %653 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %655 = stablehlo.broadcast_in_dim %arg315, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %656 = stablehlo.broadcast_in_dim %655, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %657 = stablehlo.multiply %654, %656 : tensor<16x32x128xbf16> loc(#loc1697)
    %658 = stablehlo.reshape %657 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %659 = stablehlo.reshape %639 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %660 = stablehlo.convert %659 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %661 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %662 = stablehlo.power %660, %661 : tensor<16x4x128xf32> loc(#loc1700)
    %663 = stablehlo.reduce(%662 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %664 = stablehlo.broadcast_in_dim %663, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %665 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %666 = stablehlo.divide %664, %665 : tensor<16x4x1xf32> loc(#loc1703)
    %667 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %668 = stablehlo.add %666, %667 : tensor<16x4x1xf32> loc(#loc1704)
    %669 = stablehlo.rsqrt %668 : tensor<16x4x1xf32> loc(#loc1678)
    %670 = stablehlo.broadcast_in_dim %669, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %671 = stablehlo.multiply %660, %670 : tensor<16x4x128xf32> loc(#loc1705)
    %672 = stablehlo.convert %671 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %673 = stablehlo.broadcast_in_dim %arg313, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %674 = stablehlo.broadcast_in_dim %673, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %675 = stablehlo.multiply %672, %674 : tensor<16x4x128xbf16> loc(#loc1707)
    %676 = stablehlo.reshape %675 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %677 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %678 = stablehlo.compare  LT, %arg484, %677,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %679 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %680 = stablehlo.add %arg484, %679 : tensor<16xi32> loc(#loc1710)
    %681 = stablehlo.select %678, %680, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %682 = stablehlo.broadcast_in_dim %681, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %683 = "stablehlo.gather"(%arg10, %682) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %684 = stablehlo.slice %683 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %685 = stablehlo.slice %683 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %686 = stablehlo.reshape %658 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %687 = stablehlo.broadcast_in_dim %684, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %688 = stablehlo.broadcast_in_dim %685, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %689 = stablehlo.slice %686 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %690 = stablehlo.slice %686 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %691 = stablehlo.broadcast_in_dim %687, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %692 = stablehlo.multiply %689, %691 : tensor<16x32x64xbf16> loc(#loc1719)
    %693 = stablehlo.broadcast_in_dim %688, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %694 = stablehlo.multiply %690, %693 : tensor<16x32x64xbf16> loc(#loc1720)
    %695 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %696 = stablehlo.multiply %694, %695 : tensor<16x32x64xbf16> loc(#loc1721)
    %697 = stablehlo.subtract %692, %696 : tensor<16x32x64xbf16> loc(#loc1722)
    %698 = stablehlo.broadcast_in_dim %687, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %699 = stablehlo.multiply %690, %698 : tensor<16x32x64xbf16> loc(#loc1723)
    %700 = stablehlo.broadcast_in_dim %688, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %701 = stablehlo.multiply %689, %700 : tensor<16x32x64xbf16> loc(#loc1724)
    %702 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %703 = stablehlo.multiply %701, %702 : tensor<16x32x64xbf16> loc(#loc1725)
    %704 = stablehlo.add %699, %703 : tensor<16x32x64xbf16> loc(#loc1726)
    %705 = stablehlo.concatenate %697, %704, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %706 = stablehlo.slice %686 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %707 = stablehlo.concatenate %705, %706, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %708 = stablehlo.reshape %707 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %709 = stablehlo.reshape %676 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %710 = stablehlo.broadcast_in_dim %684, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %711 = stablehlo.broadcast_in_dim %685, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %712 = stablehlo.slice %709 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %713 = stablehlo.slice %709 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %714 = stablehlo.broadcast_in_dim %710, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %715 = stablehlo.multiply %712, %714 : tensor<16x4x64xbf16> loc(#loc1735)
    %716 = stablehlo.broadcast_in_dim %711, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %717 = stablehlo.multiply %713, %716 : tensor<16x4x64xbf16> loc(#loc1736)
    %718 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %719 = stablehlo.multiply %717, %718 : tensor<16x4x64xbf16> loc(#loc1737)
    %720 = stablehlo.subtract %715, %719 : tensor<16x4x64xbf16> loc(#loc1738)
    %721 = stablehlo.broadcast_in_dim %710, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %722 = stablehlo.multiply %713, %721 : tensor<16x4x64xbf16> loc(#loc1739)
    %723 = stablehlo.broadcast_in_dim %711, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %724 = stablehlo.multiply %712, %723 : tensor<16x4x64xbf16> loc(#loc1740)
    %725 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %726 = stablehlo.multiply %724, %725 : tensor<16x4x64xbf16> loc(#loc1741)
    %727 = stablehlo.add %722, %726 : tensor<16x4x64xbf16> loc(#loc1742)
    %728 = stablehlo.concatenate %720, %727, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %729 = stablehlo.slice %709 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %730 = stablehlo.concatenate %728, %729, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %731 = stablehlo.reshape %730 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %732:2 = call @_jax_attn_func(%arg439, %708, %731, %640, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %733 = stablehlo.dot_general %732#1, %arg314, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %734 = stablehlo.reshape %733 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %735 = stablehlo.reshape %734 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %736 = stablehlo.convert %735 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %737 = stablehlo.convert %613 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %738 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %739 = stablehlo.multiply %737, %738 : tensor<16x2048xf32> loc(#loc1751)
    %740 = stablehlo.add %736, %739 : tensor<16x2048xf32> loc(#loc1752)
    %741 = stablehlo.convert %740 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %742 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %743 = stablehlo.power %740, %742 : tensor<16x2048xf32> loc(#loc1754)
    %744 = stablehlo.reduce(%743 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %745 = stablehlo.broadcast_in_dim %744, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %746 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %747 = stablehlo.divide %745, %746 : tensor<16x1xf32> loc(#loc1757)
    %748 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %749 = stablehlo.add %747, %748 : tensor<16x1xf32> loc(#loc1758)
    %750 = stablehlo.rsqrt %749 : tensor<16x1xf32> loc(#loc1678)
    %751 = stablehlo.broadcast_in_dim %750, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %752 = stablehlo.multiply %740, %751 : tensor<16x2048xf32> loc(#loc1759)
    %753 = stablehlo.convert %752 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %754 = stablehlo.broadcast_in_dim %arg312, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %755 = stablehlo.broadcast_in_dim %754, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %756 = stablehlo.multiply %753, %755 : tensor<16x2048xbf16> loc(#loc1761)
    %757 = stablehlo.dot_general %756, %arg311, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %758 = stablehlo.reshape %757 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %759 = stablehlo.reshape %758 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %760 = call @jax_fused_moe_func_padded(%756, %arg309, %arg310, %759) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %761 = stablehlo.convert %760 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %762 = stablehlo.convert %741 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %763 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %764 = stablehlo.multiply %762, %763 : tensor<16x2048xf32> loc(#loc1766)
    %765 = stablehlo.add %761, %764 : tensor<16x2048xf32> loc(#loc1767)
    %766 = stablehlo.convert %765 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %767 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %768 = stablehlo.power %765, %767 : tensor<16x2048xf32> loc(#loc1768)
    %769 = stablehlo.reduce(%768 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %770 = stablehlo.broadcast_in_dim %769, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %771 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %772 = stablehlo.divide %770, %771 : tensor<16x1xf32> loc(#loc1771)
    %773 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %774 = stablehlo.add %772, %773 : tensor<16x1xf32> loc(#loc1772)
    %775 = stablehlo.rsqrt %774 : tensor<16x1xf32> loc(#loc1678)
    %776 = stablehlo.broadcast_in_dim %775, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %777 = stablehlo.multiply %765, %776 : tensor<16x2048xf32> loc(#loc1773)
    %778 = stablehlo.convert %777 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %779 = stablehlo.broadcast_in_dim %arg389, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %780 = stablehlo.broadcast_in_dim %779, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %781 = stablehlo.multiply %778, %780 : tensor<16x2048xbf16> loc(#loc1775)
    %782 = stablehlo.dot_general %781, %arg397, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %783 = stablehlo.reshape %782 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %784 = stablehlo.slice %783 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %785 = stablehlo.reshape %784 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %786 = stablehlo.slice %783 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %787 = stablehlo.reshape %786 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %788 = stablehlo.slice %783 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %789 = stablehlo.reshape %788 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %790 = stablehlo.concatenate %785, %787, %789, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %791 = stablehlo.slice %790 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %792 = stablehlo.slice %790 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %793 = stablehlo.slice %790 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %794 = stablehlo.reshape %791 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %795 = stablehlo.convert %794 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %796 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %797 = stablehlo.power %795, %796 : tensor<16x32x128xf32> loc(#loc1690)
    %798 = stablehlo.reduce(%797 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %799 = stablehlo.broadcast_in_dim %798, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %800 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %801 = stablehlo.divide %799, %800 : tensor<16x32x1xf32> loc(#loc1693)
    %802 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %803 = stablehlo.add %801, %802 : tensor<16x32x1xf32> loc(#loc1694)
    %804 = stablehlo.rsqrt %803 : tensor<16x32x1xf32> loc(#loc1678)
    %805 = stablehlo.broadcast_in_dim %804, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %806 = stablehlo.multiply %795, %805 : tensor<16x32x128xf32> loc(#loc1695)
    %807 = stablehlo.convert %806 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %808 = stablehlo.broadcast_in_dim %arg396, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %809 = stablehlo.broadcast_in_dim %808, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %810 = stablehlo.multiply %807, %809 : tensor<16x32x128xbf16> loc(#loc1697)
    %811 = stablehlo.reshape %810 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %812 = stablehlo.reshape %792 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %813 = stablehlo.convert %812 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %814 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %815 = stablehlo.power %813, %814 : tensor<16x4x128xf32> loc(#loc1700)
    %816 = stablehlo.reduce(%815 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %817 = stablehlo.broadcast_in_dim %816, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %818 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %819 = stablehlo.divide %817, %818 : tensor<16x4x1xf32> loc(#loc1703)
    %820 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %821 = stablehlo.add %819, %820 : tensor<16x4x1xf32> loc(#loc1704)
    %822 = stablehlo.rsqrt %821 : tensor<16x4x1xf32> loc(#loc1678)
    %823 = stablehlo.broadcast_in_dim %822, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %824 = stablehlo.multiply %813, %823 : tensor<16x4x128xf32> loc(#loc1705)
    %825 = stablehlo.convert %824 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %826 = stablehlo.broadcast_in_dim %arg394, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %827 = stablehlo.broadcast_in_dim %826, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %828 = stablehlo.multiply %825, %827 : tensor<16x4x128xbf16> loc(#loc1707)
    %829 = stablehlo.reshape %828 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %830 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %831 = stablehlo.compare  LT, %arg484, %830,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %832 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %833 = stablehlo.add %arg484, %832 : tensor<16xi32> loc(#loc1710)
    %834 = stablehlo.select %831, %833, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %835 = stablehlo.broadcast_in_dim %834, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %836 = "stablehlo.gather"(%arg10, %835) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %837 = stablehlo.slice %836 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %838 = stablehlo.slice %836 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %839 = stablehlo.reshape %811 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %840 = stablehlo.broadcast_in_dim %837, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %841 = stablehlo.broadcast_in_dim %838, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %842 = stablehlo.slice %839 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %843 = stablehlo.slice %839 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %844 = stablehlo.broadcast_in_dim %840, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %845 = stablehlo.multiply %842, %844 : tensor<16x32x64xbf16> loc(#loc1719)
    %846 = stablehlo.broadcast_in_dim %841, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %847 = stablehlo.multiply %843, %846 : tensor<16x32x64xbf16> loc(#loc1720)
    %848 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %849 = stablehlo.multiply %847, %848 : tensor<16x32x64xbf16> loc(#loc1721)
    %850 = stablehlo.subtract %845, %849 : tensor<16x32x64xbf16> loc(#loc1722)
    %851 = stablehlo.broadcast_in_dim %840, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %852 = stablehlo.multiply %843, %851 : tensor<16x32x64xbf16> loc(#loc1723)
    %853 = stablehlo.broadcast_in_dim %841, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %854 = stablehlo.multiply %842, %853 : tensor<16x32x64xbf16> loc(#loc1724)
    %855 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %856 = stablehlo.multiply %854, %855 : tensor<16x32x64xbf16> loc(#loc1725)
    %857 = stablehlo.add %852, %856 : tensor<16x32x64xbf16> loc(#loc1726)
    %858 = stablehlo.concatenate %850, %857, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %859 = stablehlo.slice %839 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %860 = stablehlo.concatenate %858, %859, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %861 = stablehlo.reshape %860 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %862 = stablehlo.reshape %829 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %863 = stablehlo.broadcast_in_dim %837, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %864 = stablehlo.broadcast_in_dim %838, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %865 = stablehlo.slice %862 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %866 = stablehlo.slice %862 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %867 = stablehlo.broadcast_in_dim %863, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %868 = stablehlo.multiply %865, %867 : tensor<16x4x64xbf16> loc(#loc1735)
    %869 = stablehlo.broadcast_in_dim %864, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %870 = stablehlo.multiply %866, %869 : tensor<16x4x64xbf16> loc(#loc1736)
    %871 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %872 = stablehlo.multiply %870, %871 : tensor<16x4x64xbf16> loc(#loc1737)
    %873 = stablehlo.subtract %868, %872 : tensor<16x4x64xbf16> loc(#loc1738)
    %874 = stablehlo.broadcast_in_dim %863, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %875 = stablehlo.multiply %866, %874 : tensor<16x4x64xbf16> loc(#loc1739)
    %876 = stablehlo.broadcast_in_dim %864, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %877 = stablehlo.multiply %865, %876 : tensor<16x4x64xbf16> loc(#loc1740)
    %878 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %879 = stablehlo.multiply %877, %878 : tensor<16x4x64xbf16> loc(#loc1741)
    %880 = stablehlo.add %875, %879 : tensor<16x4x64xbf16> loc(#loc1742)
    %881 = stablehlo.concatenate %873, %880, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %882 = stablehlo.slice %862 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %883 = stablehlo.concatenate %881, %882, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %884 = stablehlo.reshape %883 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %885:2 = call @_jax_attn_func(%arg440, %861, %884, %793, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %886 = stablehlo.dot_general %885#1, %arg395, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %887 = stablehlo.reshape %886 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %888 = stablehlo.reshape %887 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %889 = stablehlo.convert %888 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %890 = stablehlo.convert %766 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %891 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %892 = stablehlo.multiply %890, %891 : tensor<16x2048xf32> loc(#loc1751)
    %893 = stablehlo.add %889, %892 : tensor<16x2048xf32> loc(#loc1752)
    %894 = stablehlo.convert %893 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %895 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %896 = stablehlo.power %893, %895 : tensor<16x2048xf32> loc(#loc1754)
    %897 = stablehlo.reduce(%896 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %898 = stablehlo.broadcast_in_dim %897, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %899 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %900 = stablehlo.divide %898, %899 : tensor<16x1xf32> loc(#loc1757)
    %901 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %902 = stablehlo.add %900, %901 : tensor<16x1xf32> loc(#loc1758)
    %903 = stablehlo.rsqrt %902 : tensor<16x1xf32> loc(#loc1678)
    %904 = stablehlo.broadcast_in_dim %903, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %905 = stablehlo.multiply %893, %904 : tensor<16x2048xf32> loc(#loc1759)
    %906 = stablehlo.convert %905 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %907 = stablehlo.broadcast_in_dim %arg393, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %908 = stablehlo.broadcast_in_dim %907, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %909 = stablehlo.multiply %906, %908 : tensor<16x2048xbf16> loc(#loc1761)
    %910 = stablehlo.dot_general %909, %arg392, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %911 = stablehlo.reshape %910 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %912 = stablehlo.reshape %911 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %913 = call @jax_fused_moe_func_padded(%909, %arg390, %arg391, %912) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %914 = stablehlo.convert %913 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %915 = stablehlo.convert %894 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %916 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %917 = stablehlo.multiply %915, %916 : tensor<16x2048xf32> loc(#loc1766)
    %918 = stablehlo.add %914, %917 : tensor<16x2048xf32> loc(#loc1767)
    %919 = stablehlo.convert %918 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %920 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %921 = stablehlo.power %918, %920 : tensor<16x2048xf32> loc(#loc1768)
    %922 = stablehlo.reduce(%921 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %923 = stablehlo.broadcast_in_dim %922, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %924 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %925 = stablehlo.divide %923, %924 : tensor<16x1xf32> loc(#loc1771)
    %926 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %927 = stablehlo.add %925, %926 : tensor<16x1xf32> loc(#loc1772)
    %928 = stablehlo.rsqrt %927 : tensor<16x1xf32> loc(#loc1678)
    %929 = stablehlo.broadcast_in_dim %928, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %930 = stablehlo.multiply %918, %929 : tensor<16x2048xf32> loc(#loc1773)
    %931 = stablehlo.convert %930 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %932 = stablehlo.broadcast_in_dim %arg398, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %933 = stablehlo.broadcast_in_dim %932, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %934 = stablehlo.multiply %931, %933 : tensor<16x2048xbf16> loc(#loc1775)
    %935 = stablehlo.dot_general %934, %arg406, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %936 = stablehlo.reshape %935 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %937 = stablehlo.slice %936 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %938 = stablehlo.reshape %937 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %939 = stablehlo.slice %936 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %940 = stablehlo.reshape %939 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %941 = stablehlo.slice %936 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %942 = stablehlo.reshape %941 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %943 = stablehlo.concatenate %938, %940, %942, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %944 = stablehlo.slice %943 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %945 = stablehlo.slice %943 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %946 = stablehlo.slice %943 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %947 = stablehlo.reshape %944 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %948 = stablehlo.convert %947 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %949 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %950 = stablehlo.power %948, %949 : tensor<16x32x128xf32> loc(#loc1690)
    %951 = stablehlo.reduce(%950 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %952 = stablehlo.broadcast_in_dim %951, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %953 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %954 = stablehlo.divide %952, %953 : tensor<16x32x1xf32> loc(#loc1693)
    %955 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %956 = stablehlo.add %954, %955 : tensor<16x32x1xf32> loc(#loc1694)
    %957 = stablehlo.rsqrt %956 : tensor<16x32x1xf32> loc(#loc1678)
    %958 = stablehlo.broadcast_in_dim %957, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %959 = stablehlo.multiply %948, %958 : tensor<16x32x128xf32> loc(#loc1695)
    %960 = stablehlo.convert %959 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %961 = stablehlo.broadcast_in_dim %arg405, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %962 = stablehlo.broadcast_in_dim %961, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %963 = stablehlo.multiply %960, %962 : tensor<16x32x128xbf16> loc(#loc1697)
    %964 = stablehlo.reshape %963 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %965 = stablehlo.reshape %945 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %966 = stablehlo.convert %965 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %967 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %968 = stablehlo.power %966, %967 : tensor<16x4x128xf32> loc(#loc1700)
    %969 = stablehlo.reduce(%968 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %970 = stablehlo.broadcast_in_dim %969, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %971 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %972 = stablehlo.divide %970, %971 : tensor<16x4x1xf32> loc(#loc1703)
    %973 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %974 = stablehlo.add %972, %973 : tensor<16x4x1xf32> loc(#loc1704)
    %975 = stablehlo.rsqrt %974 : tensor<16x4x1xf32> loc(#loc1678)
    %976 = stablehlo.broadcast_in_dim %975, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %977 = stablehlo.multiply %966, %976 : tensor<16x4x128xf32> loc(#loc1705)
    %978 = stablehlo.convert %977 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %979 = stablehlo.broadcast_in_dim %arg403, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %980 = stablehlo.broadcast_in_dim %979, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %981 = stablehlo.multiply %978, %980 : tensor<16x4x128xbf16> loc(#loc1707)
    %982 = stablehlo.reshape %981 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %983 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %984 = stablehlo.compare  LT, %arg484, %983,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %985 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %986 = stablehlo.add %arg484, %985 : tensor<16xi32> loc(#loc1710)
    %987 = stablehlo.select %984, %986, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %988 = stablehlo.broadcast_in_dim %987, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %989 = "stablehlo.gather"(%arg10, %988) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %990 = stablehlo.slice %989 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %991 = stablehlo.slice %989 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %992 = stablehlo.reshape %964 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %993 = stablehlo.broadcast_in_dim %990, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %994 = stablehlo.broadcast_in_dim %991, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %995 = stablehlo.slice %992 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %996 = stablehlo.slice %992 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %997 = stablehlo.broadcast_in_dim %993, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %998 = stablehlo.multiply %995, %997 : tensor<16x32x64xbf16> loc(#loc1719)
    %999 = stablehlo.broadcast_in_dim %994, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %1000 = stablehlo.multiply %996, %999 : tensor<16x32x64xbf16> loc(#loc1720)
    %1001 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %1002 = stablehlo.multiply %1000, %1001 : tensor<16x32x64xbf16> loc(#loc1721)
    %1003 = stablehlo.subtract %998, %1002 : tensor<16x32x64xbf16> loc(#loc1722)
    %1004 = stablehlo.broadcast_in_dim %993, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %1005 = stablehlo.multiply %996, %1004 : tensor<16x32x64xbf16> loc(#loc1723)
    %1006 = stablehlo.broadcast_in_dim %994, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %1007 = stablehlo.multiply %995, %1006 : tensor<16x32x64xbf16> loc(#loc1724)
    %1008 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %1009 = stablehlo.multiply %1007, %1008 : tensor<16x32x64xbf16> loc(#loc1725)
    %1010 = stablehlo.add %1005, %1009 : tensor<16x32x64xbf16> loc(#loc1726)
    %1011 = stablehlo.concatenate %1003, %1010, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %1012 = stablehlo.slice %992 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %1013 = stablehlo.concatenate %1011, %1012, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %1014 = stablehlo.reshape %1013 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %1015 = stablehlo.reshape %982 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %1016 = stablehlo.broadcast_in_dim %990, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %1017 = stablehlo.broadcast_in_dim %991, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %1018 = stablehlo.slice %1015 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %1019 = stablehlo.slice %1015 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %1020 = stablehlo.broadcast_in_dim %1016, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %1021 = stablehlo.multiply %1018, %1020 : tensor<16x4x64xbf16> loc(#loc1735)
    %1022 = stablehlo.broadcast_in_dim %1017, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %1023 = stablehlo.multiply %1019, %1022 : tensor<16x4x64xbf16> loc(#loc1736)
    %1024 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %1025 = stablehlo.multiply %1023, %1024 : tensor<16x4x64xbf16> loc(#loc1737)
    %1026 = stablehlo.subtract %1021, %1025 : tensor<16x4x64xbf16> loc(#loc1738)
    %1027 = stablehlo.broadcast_in_dim %1016, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %1028 = stablehlo.multiply %1019, %1027 : tensor<16x4x64xbf16> loc(#loc1739)
    %1029 = stablehlo.broadcast_in_dim %1017, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %1030 = stablehlo.multiply %1018, %1029 : tensor<16x4x64xbf16> loc(#loc1740)
    %1031 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %1032 = stablehlo.multiply %1030, %1031 : tensor<16x4x64xbf16> loc(#loc1741)
    %1033 = stablehlo.add %1028, %1032 : tensor<16x4x64xbf16> loc(#loc1742)
    %1034 = stablehlo.concatenate %1026, %1033, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %1035 = stablehlo.slice %1015 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %1036 = stablehlo.concatenate %1034, %1035, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %1037 = stablehlo.reshape %1036 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %1038:2 = call @_jax_attn_func(%arg441, %1014, %1037, %946, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %1039 = stablehlo.dot_general %1038#1, %arg404, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %1040 = stablehlo.reshape %1039 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %1041 = stablehlo.reshape %1040 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %1042 = stablehlo.convert %1041 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %1043 = stablehlo.convert %919 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %1044 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %1045 = stablehlo.multiply %1043, %1044 : tensor<16x2048xf32> loc(#loc1751)
    %1046 = stablehlo.add %1042, %1045 : tensor<16x2048xf32> loc(#loc1752)
    %1047 = stablehlo.convert %1046 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %1048 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %1049 = stablehlo.power %1046, %1048 : tensor<16x2048xf32> loc(#loc1754)
    %1050 = stablehlo.reduce(%1049 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %1051 = stablehlo.broadcast_in_dim %1050, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %1052 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %1053 = stablehlo.divide %1051, %1052 : tensor<16x1xf32> loc(#loc1757)
    %1054 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %1055 = stablehlo.add %1053, %1054 : tensor<16x1xf32> loc(#loc1758)
    %1056 = stablehlo.rsqrt %1055 : tensor<16x1xf32> loc(#loc1678)
    %1057 = stablehlo.broadcast_in_dim %1056, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %1058 = stablehlo.multiply %1046, %1057 : tensor<16x2048xf32> loc(#loc1759)
    %1059 = stablehlo.convert %1058 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %1060 = stablehlo.broadcast_in_dim %arg402, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %1061 = stablehlo.broadcast_in_dim %1060, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %1062 = stablehlo.multiply %1059, %1061 : tensor<16x2048xbf16> loc(#loc1761)
    %1063 = stablehlo.dot_general %1062, %arg401, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %1064 = stablehlo.reshape %1063 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %1065 = stablehlo.reshape %1064 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %1066 = call @jax_fused_moe_func_padded(%1062, %arg399, %arg400, %1065) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %1067 = stablehlo.convert %1066 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %1068 = stablehlo.convert %1047 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %1069 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %1070 = stablehlo.multiply %1068, %1069 : tensor<16x2048xf32> loc(#loc1766)
    %1071 = stablehlo.add %1067, %1070 : tensor<16x2048xf32> loc(#loc1767)
    %1072 = stablehlo.convert %1071 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %1073 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %1074 = stablehlo.power %1071, %1073 : tensor<16x2048xf32> loc(#loc1768)
    %1075 = stablehlo.reduce(%1074 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %1076 = stablehlo.broadcast_in_dim %1075, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %1077 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %1078 = stablehlo.divide %1076, %1077 : tensor<16x1xf32> loc(#loc1771)
    %1079 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %1080 = stablehlo.add %1078, %1079 : tensor<16x1xf32> loc(#loc1772)
    %1081 = stablehlo.rsqrt %1080 : tensor<16x1xf32> loc(#loc1678)
    %1082 = stablehlo.broadcast_in_dim %1081, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %1083 = stablehlo.multiply %1071, %1082 : tensor<16x2048xf32> loc(#loc1773)
    %1084 = stablehlo.convert %1083 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %1085 = stablehlo.broadcast_in_dim %arg407, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %1086 = stablehlo.broadcast_in_dim %1085, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %1087 = stablehlo.multiply %1084, %1086 : tensor<16x2048xbf16> loc(#loc1775)
    %1088 = stablehlo.dot_general %1087, %arg415, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %1089 = stablehlo.reshape %1088 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %1090 = stablehlo.slice %1089 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %1091 = stablehlo.reshape %1090 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %1092 = stablehlo.slice %1089 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %1093 = stablehlo.reshape %1092 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %1094 = stablehlo.slice %1089 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %1095 = stablehlo.reshape %1094 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %1096 = stablehlo.concatenate %1091, %1093, %1095, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %1097 = stablehlo.slice %1096 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %1098 = stablehlo.slice %1096 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %1099 = stablehlo.slice %1096 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %1100 = stablehlo.reshape %1097 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %1101 = stablehlo.convert %1100 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %1102 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %1103 = stablehlo.power %1101, %1102 : tensor<16x32x128xf32> loc(#loc1690)
    %1104 = stablehlo.reduce(%1103 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %1105 = stablehlo.broadcast_in_dim %1104, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %1106 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %1107 = stablehlo.divide %1105, %1106 : tensor<16x32x1xf32> loc(#loc1693)
    %1108 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %1109 = stablehlo.add %1107, %1108 : tensor<16x32x1xf32> loc(#loc1694)
    %1110 = stablehlo.rsqrt %1109 : tensor<16x32x1xf32> loc(#loc1678)
    %1111 = stablehlo.broadcast_in_dim %1110, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %1112 = stablehlo.multiply %1101, %1111 : tensor<16x32x128xf32> loc(#loc1695)
    %1113 = stablehlo.convert %1112 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %1114 = stablehlo.broadcast_in_dim %arg414, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %1115 = stablehlo.broadcast_in_dim %1114, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %1116 = stablehlo.multiply %1113, %1115 : tensor<16x32x128xbf16> loc(#loc1697)
    %1117 = stablehlo.reshape %1116 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %1118 = stablehlo.reshape %1098 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %1119 = stablehlo.convert %1118 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %1120 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %1121 = stablehlo.power %1119, %1120 : tensor<16x4x128xf32> loc(#loc1700)
    %1122 = stablehlo.reduce(%1121 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %1123 = stablehlo.broadcast_in_dim %1122, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %1124 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %1125 = stablehlo.divide %1123, %1124 : tensor<16x4x1xf32> loc(#loc1703)
    %1126 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %1127 = stablehlo.add %1125, %1126 : tensor<16x4x1xf32> loc(#loc1704)
    %1128 = stablehlo.rsqrt %1127 : tensor<16x4x1xf32> loc(#loc1678)
    %1129 = stablehlo.broadcast_in_dim %1128, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %1130 = stablehlo.multiply %1119, %1129 : tensor<16x4x128xf32> loc(#loc1705)
    %1131 = stablehlo.convert %1130 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %1132 = stablehlo.broadcast_in_dim %arg412, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %1133 = stablehlo.broadcast_in_dim %1132, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %1134 = stablehlo.multiply %1131, %1133 : tensor<16x4x128xbf16> loc(#loc1707)
    %1135 = stablehlo.reshape %1134 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %1136 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %1137 = stablehlo.compare  LT, %arg484, %1136,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %1138 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %1139 = stablehlo.add %arg484, %1138 : tensor<16xi32> loc(#loc1710)
    %1140 = stablehlo.select %1137, %1139, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %1141 = stablehlo.broadcast_in_dim %1140, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %1142 = "stablehlo.gather"(%arg10, %1141) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %1143 = stablehlo.slice %1142 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %1144 = stablehlo.slice %1142 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %1145 = stablehlo.reshape %1117 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %1146 = stablehlo.broadcast_in_dim %1143, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %1147 = stablehlo.broadcast_in_dim %1144, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %1148 = stablehlo.slice %1145 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %1149 = stablehlo.slice %1145 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %1150 = stablehlo.broadcast_in_dim %1146, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %1151 = stablehlo.multiply %1148, %1150 : tensor<16x32x64xbf16> loc(#loc1719)
    %1152 = stablehlo.broadcast_in_dim %1147, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %1153 = stablehlo.multiply %1149, %1152 : tensor<16x32x64xbf16> loc(#loc1720)
    %1154 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %1155 = stablehlo.multiply %1153, %1154 : tensor<16x32x64xbf16> loc(#loc1721)
    %1156 = stablehlo.subtract %1151, %1155 : tensor<16x32x64xbf16> loc(#loc1722)
    %1157 = stablehlo.broadcast_in_dim %1146, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %1158 = stablehlo.multiply %1149, %1157 : tensor<16x32x64xbf16> loc(#loc1723)
    %1159 = stablehlo.broadcast_in_dim %1147, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %1160 = stablehlo.multiply %1148, %1159 : tensor<16x32x64xbf16> loc(#loc1724)
    %1161 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %1162 = stablehlo.multiply %1160, %1161 : tensor<16x32x64xbf16> loc(#loc1725)
    %1163 = stablehlo.add %1158, %1162 : tensor<16x32x64xbf16> loc(#loc1726)
    %1164 = stablehlo.concatenate %1156, %1163, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %1165 = stablehlo.slice %1145 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %1166 = stablehlo.concatenate %1164, %1165, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %1167 = stablehlo.reshape %1166 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %1168 = stablehlo.reshape %1135 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %1169 = stablehlo.broadcast_in_dim %1143, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %1170 = stablehlo.broadcast_in_dim %1144, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %1171 = stablehlo.slice %1168 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %1172 = stablehlo.slice %1168 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %1173 = stablehlo.broadcast_in_dim %1169, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %1174 = stablehlo.multiply %1171, %1173 : tensor<16x4x64xbf16> loc(#loc1735)
    %1175 = stablehlo.broadcast_in_dim %1170, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %1176 = stablehlo.multiply %1172, %1175 : tensor<16x4x64xbf16> loc(#loc1736)
    %1177 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %1178 = stablehlo.multiply %1176, %1177 : tensor<16x4x64xbf16> loc(#loc1737)
    %1179 = stablehlo.subtract %1174, %1178 : tensor<16x4x64xbf16> loc(#loc1738)
    %1180 = stablehlo.broadcast_in_dim %1169, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %1181 = stablehlo.multiply %1172, %1180 : tensor<16x4x64xbf16> loc(#loc1739)
    %1182 = stablehlo.broadcast_in_dim %1170, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %1183 = stablehlo.multiply %1171, %1182 : tensor<16x4x64xbf16> loc(#loc1740)
    %1184 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %1185 = stablehlo.multiply %1183, %1184 : tensor<16x4x64xbf16> loc(#loc1741)
    %1186 = stablehlo.add %1181, %1185 : tensor<16x4x64xbf16> loc(#loc1742)
    %1187 = stablehlo.concatenate %1179, %1186, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %1188 = stablehlo.slice %1168 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %1189 = stablehlo.concatenate %1187, %1188, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %1190 = stablehlo.reshape %1189 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %1191:2 = call @_jax_attn_func(%arg442, %1167, %1190, %1099, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %1192 = stablehlo.dot_general %1191#1, %arg413, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %1193 = stablehlo.reshape %1192 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %1194 = stablehlo.reshape %1193 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %1195 = stablehlo.convert %1194 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %1196 = stablehlo.convert %1072 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %1197 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %1198 = stablehlo.multiply %1196, %1197 : tensor<16x2048xf32> loc(#loc1751)
    %1199 = stablehlo.add %1195, %1198 : tensor<16x2048xf32> loc(#loc1752)
    %1200 = stablehlo.convert %1199 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %1201 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %1202 = stablehlo.power %1199, %1201 : tensor<16x2048xf32> loc(#loc1754)
    %1203 = stablehlo.reduce(%1202 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %1204 = stablehlo.broadcast_in_dim %1203, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %1205 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %1206 = stablehlo.divide %1204, %1205 : tensor<16x1xf32> loc(#loc1757)
    %1207 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %1208 = stablehlo.add %1206, %1207 : tensor<16x1xf32> loc(#loc1758)
    %1209 = stablehlo.rsqrt %1208 : tensor<16x1xf32> loc(#loc1678)
    %1210 = stablehlo.broadcast_in_dim %1209, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %1211 = stablehlo.multiply %1199, %1210 : tensor<16x2048xf32> loc(#loc1759)
    %1212 = stablehlo.convert %1211 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %1213 = stablehlo.broadcast_in_dim %arg411, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %1214 = stablehlo.broadcast_in_dim %1213, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %1215 = stablehlo.multiply %1212, %1214 : tensor<16x2048xbf16> loc(#loc1761)
    %1216 = stablehlo.dot_general %1215, %arg410, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %1217 = stablehlo.reshape %1216 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %1218 = stablehlo.reshape %1217 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %1219 = call @jax_fused_moe_func_padded(%1215, %arg408, %arg409, %1218) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %1220 = stablehlo.convert %1219 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %1221 = stablehlo.convert %1200 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %1222 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %1223 = stablehlo.multiply %1221, %1222 : tensor<16x2048xf32> loc(#loc1766)
    %1224 = stablehlo.add %1220, %1223 : tensor<16x2048xf32> loc(#loc1767)
    %1225 = stablehlo.convert %1224 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %1226 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %1227 = stablehlo.power %1224, %1226 : tensor<16x2048xf32> loc(#loc1768)
    %1228 = stablehlo.reduce(%1227 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %1229 = stablehlo.broadcast_in_dim %1228, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %1230 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %1231 = stablehlo.divide %1229, %1230 : tensor<16x1xf32> loc(#loc1771)
    %1232 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %1233 = stablehlo.add %1231, %1232 : tensor<16x1xf32> loc(#loc1772)
    %1234 = stablehlo.rsqrt %1233 : tensor<16x1xf32> loc(#loc1678)
    %1235 = stablehlo.broadcast_in_dim %1234, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %1236 = stablehlo.multiply %1224, %1235 : tensor<16x2048xf32> loc(#loc1773)
    %1237 = stablehlo.convert %1236 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %1238 = stablehlo.broadcast_in_dim %arg416, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %1239 = stablehlo.broadcast_in_dim %1238, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %1240 = stablehlo.multiply %1237, %1239 : tensor<16x2048xbf16> loc(#loc1775)
    %1241 = stablehlo.dot_general %1240, %arg424, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %1242 = stablehlo.reshape %1241 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %1243 = stablehlo.slice %1242 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %1244 = stablehlo.reshape %1243 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %1245 = stablehlo.slice %1242 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %1246 = stablehlo.reshape %1245 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %1247 = stablehlo.slice %1242 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %1248 = stablehlo.reshape %1247 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %1249 = stablehlo.concatenate %1244, %1246, %1248, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %1250 = stablehlo.slice %1249 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %1251 = stablehlo.slice %1249 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %1252 = stablehlo.slice %1249 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %1253 = stablehlo.reshape %1250 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %1254 = stablehlo.convert %1253 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %1255 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %1256 = stablehlo.power %1254, %1255 : tensor<16x32x128xf32> loc(#loc1690)
    %1257 = stablehlo.reduce(%1256 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %1258 = stablehlo.broadcast_in_dim %1257, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %1259 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %1260 = stablehlo.divide %1258, %1259 : tensor<16x32x1xf32> loc(#loc1693)
    %1261 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %1262 = stablehlo.add %1260, %1261 : tensor<16x32x1xf32> loc(#loc1694)
    %1263 = stablehlo.rsqrt %1262 : tensor<16x32x1xf32> loc(#loc1678)
    %1264 = stablehlo.broadcast_in_dim %1263, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %1265 = stablehlo.multiply %1254, %1264 : tensor<16x32x128xf32> loc(#loc1695)
    %1266 = stablehlo.convert %1265 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %1267 = stablehlo.broadcast_in_dim %arg423, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %1268 = stablehlo.broadcast_in_dim %1267, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %1269 = stablehlo.multiply %1266, %1268 : tensor<16x32x128xbf16> loc(#loc1697)
    %1270 = stablehlo.reshape %1269 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %1271 = stablehlo.reshape %1251 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %1272 = stablehlo.convert %1271 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %1273 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %1274 = stablehlo.power %1272, %1273 : tensor<16x4x128xf32> loc(#loc1700)
    %1275 = stablehlo.reduce(%1274 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %1276 = stablehlo.broadcast_in_dim %1275, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %1277 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %1278 = stablehlo.divide %1276, %1277 : tensor<16x4x1xf32> loc(#loc1703)
    %1279 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %1280 = stablehlo.add %1278, %1279 : tensor<16x4x1xf32> loc(#loc1704)
    %1281 = stablehlo.rsqrt %1280 : tensor<16x4x1xf32> loc(#loc1678)
    %1282 = stablehlo.broadcast_in_dim %1281, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %1283 = stablehlo.multiply %1272, %1282 : tensor<16x4x128xf32> loc(#loc1705)
    %1284 = stablehlo.convert %1283 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %1285 = stablehlo.broadcast_in_dim %arg421, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %1286 = stablehlo.broadcast_in_dim %1285, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %1287 = stablehlo.multiply %1284, %1286 : tensor<16x4x128xbf16> loc(#loc1707)
    %1288 = stablehlo.reshape %1287 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %1289 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %1290 = stablehlo.compare  LT, %arg484, %1289,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %1291 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %1292 = stablehlo.add %arg484, %1291 : tensor<16xi32> loc(#loc1710)
    %1293 = stablehlo.select %1290, %1292, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %1294 = stablehlo.broadcast_in_dim %1293, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %1295 = "stablehlo.gather"(%arg10, %1294) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %1296 = stablehlo.slice %1295 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %1297 = stablehlo.slice %1295 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %1298 = stablehlo.reshape %1270 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %1299 = stablehlo.broadcast_in_dim %1296, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %1300 = stablehlo.broadcast_in_dim %1297, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %1301 = stablehlo.slice %1298 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %1302 = stablehlo.slice %1298 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %1303 = stablehlo.broadcast_in_dim %1299, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %1304 = stablehlo.multiply %1301, %1303 : tensor<16x32x64xbf16> loc(#loc1719)
    %1305 = stablehlo.broadcast_in_dim %1300, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %1306 = stablehlo.multiply %1302, %1305 : tensor<16x32x64xbf16> loc(#loc1720)
    %1307 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %1308 = stablehlo.multiply %1306, %1307 : tensor<16x32x64xbf16> loc(#loc1721)
    %1309 = stablehlo.subtract %1304, %1308 : tensor<16x32x64xbf16> loc(#loc1722)
    %1310 = stablehlo.broadcast_in_dim %1299, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %1311 = stablehlo.multiply %1302, %1310 : tensor<16x32x64xbf16> loc(#loc1723)
    %1312 = stablehlo.broadcast_in_dim %1300, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %1313 = stablehlo.multiply %1301, %1312 : tensor<16x32x64xbf16> loc(#loc1724)
    %1314 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %1315 = stablehlo.multiply %1313, %1314 : tensor<16x32x64xbf16> loc(#loc1725)
    %1316 = stablehlo.add %1311, %1315 : tensor<16x32x64xbf16> loc(#loc1726)
    %1317 = stablehlo.concatenate %1309, %1316, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %1318 = stablehlo.slice %1298 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %1319 = stablehlo.concatenate %1317, %1318, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %1320 = stablehlo.reshape %1319 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %1321 = stablehlo.reshape %1288 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %1322 = stablehlo.broadcast_in_dim %1296, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %1323 = stablehlo.broadcast_in_dim %1297, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %1324 = stablehlo.slice %1321 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %1325 = stablehlo.slice %1321 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %1326 = stablehlo.broadcast_in_dim %1322, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %1327 = stablehlo.multiply %1324, %1326 : tensor<16x4x64xbf16> loc(#loc1735)
    %1328 = stablehlo.broadcast_in_dim %1323, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %1329 = stablehlo.multiply %1325, %1328 : tensor<16x4x64xbf16> loc(#loc1736)
    %1330 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %1331 = stablehlo.multiply %1329, %1330 : tensor<16x4x64xbf16> loc(#loc1737)
    %1332 = stablehlo.subtract %1327, %1331 : tensor<16x4x64xbf16> loc(#loc1738)
    %1333 = stablehlo.broadcast_in_dim %1322, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %1334 = stablehlo.multiply %1325, %1333 : tensor<16x4x64xbf16> loc(#loc1739)
    %1335 = stablehlo.broadcast_in_dim %1323, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %1336 = stablehlo.multiply %1324, %1335 : tensor<16x4x64xbf16> loc(#loc1740)
    %1337 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %1338 = stablehlo.multiply %1336, %1337 : tensor<16x4x64xbf16> loc(#loc1741)
    %1339 = stablehlo.add %1334, %1338 : tensor<16x4x64xbf16> loc(#loc1742)
    %1340 = stablehlo.concatenate %1332, %1339, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %1341 = stablehlo.slice %1321 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %1342 = stablehlo.concatenate %1340, %1341, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %1343 = stablehlo.reshape %1342 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %1344:2 = call @_jax_attn_func(%arg443, %1320, %1343, %1252, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %1345 = stablehlo.dot_general %1344#1, %arg422, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %1346 = stablehlo.reshape %1345 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %1347 = stablehlo.reshape %1346 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %1348 = stablehlo.convert %1347 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %1349 = stablehlo.convert %1225 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %1350 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %1351 = stablehlo.multiply %1349, %1350 : tensor<16x2048xf32> loc(#loc1751)
    %1352 = stablehlo.add %1348, %1351 : tensor<16x2048xf32> loc(#loc1752)
    %1353 = stablehlo.convert %1352 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %1354 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %1355 = stablehlo.power %1352, %1354 : tensor<16x2048xf32> loc(#loc1754)
    %1356 = stablehlo.reduce(%1355 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %1357 = stablehlo.broadcast_in_dim %1356, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %1358 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %1359 = stablehlo.divide %1357, %1358 : tensor<16x1xf32> loc(#loc1757)
    %1360 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %1361 = stablehlo.add %1359, %1360 : tensor<16x1xf32> loc(#loc1758)
    %1362 = stablehlo.rsqrt %1361 : tensor<16x1xf32> loc(#loc1678)
    %1363 = stablehlo.broadcast_in_dim %1362, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %1364 = stablehlo.multiply %1352, %1363 : tensor<16x2048xf32> loc(#loc1759)
    %1365 = stablehlo.convert %1364 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %1366 = stablehlo.broadcast_in_dim %arg420, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %1367 = stablehlo.broadcast_in_dim %1366, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %1368 = stablehlo.multiply %1365, %1367 : tensor<16x2048xbf16> loc(#loc1761)
    %1369 = stablehlo.dot_general %1368, %arg419, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %1370 = stablehlo.reshape %1369 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %1371 = stablehlo.reshape %1370 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %1372 = call @jax_fused_moe_func_padded(%1368, %arg417, %arg418, %1371) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %1373 = stablehlo.convert %1372 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %1374 = stablehlo.convert %1353 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %1375 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %1376 = stablehlo.multiply %1374, %1375 : tensor<16x2048xf32> loc(#loc1766)
    %1377 = stablehlo.add %1373, %1376 : tensor<16x2048xf32> loc(#loc1767)
    %1378 = stablehlo.convert %1377 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %1379 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %1380 = stablehlo.power %1377, %1379 : tensor<16x2048xf32> loc(#loc1768)
    %1381 = stablehlo.reduce(%1380 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %1382 = stablehlo.broadcast_in_dim %1381, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %1383 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %1384 = stablehlo.divide %1382, %1383 : tensor<16x1xf32> loc(#loc1771)
    %1385 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %1386 = stablehlo.add %1384, %1385 : tensor<16x1xf32> loc(#loc1772)
    %1387 = stablehlo.rsqrt %1386 : tensor<16x1xf32> loc(#loc1678)
    %1388 = stablehlo.broadcast_in_dim %1387, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %1389 = stablehlo.multiply %1377, %1388 : tensor<16x2048xf32> loc(#loc1773)
    %1390 = stablehlo.convert %1389 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %1391 = stablehlo.broadcast_in_dim %arg425, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %1392 = stablehlo.broadcast_in_dim %1391, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %1393 = stablehlo.multiply %1390, %1392 : tensor<16x2048xbf16> loc(#loc1775)
    %1394 = stablehlo.dot_general %1393, %arg433, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %1395 = stablehlo.reshape %1394 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %1396 = stablehlo.slice %1395 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %1397 = stablehlo.reshape %1396 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %1398 = stablehlo.slice %1395 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %1399 = stablehlo.reshape %1398 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %1400 = stablehlo.slice %1395 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %1401 = stablehlo.reshape %1400 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %1402 = stablehlo.concatenate %1397, %1399, %1401, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %1403 = stablehlo.slice %1402 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %1404 = stablehlo.slice %1402 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %1405 = stablehlo.slice %1402 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %1406 = stablehlo.reshape %1403 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %1407 = stablehlo.convert %1406 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %1408 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %1409 = stablehlo.power %1407, %1408 : tensor<16x32x128xf32> loc(#loc1690)
    %1410 = stablehlo.reduce(%1409 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %1411 = stablehlo.broadcast_in_dim %1410, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %1412 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %1413 = stablehlo.divide %1411, %1412 : tensor<16x32x1xf32> loc(#loc1693)
    %1414 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %1415 = stablehlo.add %1413, %1414 : tensor<16x32x1xf32> loc(#loc1694)
    %1416 = stablehlo.rsqrt %1415 : tensor<16x32x1xf32> loc(#loc1678)
    %1417 = stablehlo.broadcast_in_dim %1416, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %1418 = stablehlo.multiply %1407, %1417 : tensor<16x32x128xf32> loc(#loc1695)
    %1419 = stablehlo.convert %1418 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %1420 = stablehlo.broadcast_in_dim %arg432, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %1421 = stablehlo.broadcast_in_dim %1420, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %1422 = stablehlo.multiply %1419, %1421 : tensor<16x32x128xbf16> loc(#loc1697)
    %1423 = stablehlo.reshape %1422 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %1424 = stablehlo.reshape %1404 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %1425 = stablehlo.convert %1424 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %1426 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %1427 = stablehlo.power %1425, %1426 : tensor<16x4x128xf32> loc(#loc1700)
    %1428 = stablehlo.reduce(%1427 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %1429 = stablehlo.broadcast_in_dim %1428, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %1430 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %1431 = stablehlo.divide %1429, %1430 : tensor<16x4x1xf32> loc(#loc1703)
    %1432 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %1433 = stablehlo.add %1431, %1432 : tensor<16x4x1xf32> loc(#loc1704)
    %1434 = stablehlo.rsqrt %1433 : tensor<16x4x1xf32> loc(#loc1678)
    %1435 = stablehlo.broadcast_in_dim %1434, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %1436 = stablehlo.multiply %1425, %1435 : tensor<16x4x128xf32> loc(#loc1705)
    %1437 = stablehlo.convert %1436 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %1438 = stablehlo.broadcast_in_dim %arg430, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %1439 = stablehlo.broadcast_in_dim %1438, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %1440 = stablehlo.multiply %1437, %1439 : tensor<16x4x128xbf16> loc(#loc1707)
    %1441 = stablehlo.reshape %1440 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %1442 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %1443 = stablehlo.compare  LT, %arg484, %1442,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %1444 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %1445 = stablehlo.add %arg484, %1444 : tensor<16xi32> loc(#loc1710)
    %1446 = stablehlo.select %1443, %1445, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %1447 = stablehlo.broadcast_in_dim %1446, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %1448 = "stablehlo.gather"(%arg10, %1447) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %1449 = stablehlo.slice %1448 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %1450 = stablehlo.slice %1448 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %1451 = stablehlo.reshape %1423 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %1452 = stablehlo.broadcast_in_dim %1449, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %1453 = stablehlo.broadcast_in_dim %1450, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %1454 = stablehlo.slice %1451 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %1455 = stablehlo.slice %1451 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %1456 = stablehlo.broadcast_in_dim %1452, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %1457 = stablehlo.multiply %1454, %1456 : tensor<16x32x64xbf16> loc(#loc1719)
    %1458 = stablehlo.broadcast_in_dim %1453, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %1459 = stablehlo.multiply %1455, %1458 : tensor<16x32x64xbf16> loc(#loc1720)
    %1460 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %1461 = stablehlo.multiply %1459, %1460 : tensor<16x32x64xbf16> loc(#loc1721)
    %1462 = stablehlo.subtract %1457, %1461 : tensor<16x32x64xbf16> loc(#loc1722)
    %1463 = stablehlo.broadcast_in_dim %1452, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %1464 = stablehlo.multiply %1455, %1463 : tensor<16x32x64xbf16> loc(#loc1723)
    %1465 = stablehlo.broadcast_in_dim %1453, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %1466 = stablehlo.multiply %1454, %1465 : tensor<16x32x64xbf16> loc(#loc1724)
    %1467 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %1468 = stablehlo.multiply %1466, %1467 : tensor<16x32x64xbf16> loc(#loc1725)
    %1469 = stablehlo.add %1464, %1468 : tensor<16x32x64xbf16> loc(#loc1726)
    %1470 = stablehlo.concatenate %1462, %1469, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %1471 = stablehlo.slice %1451 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %1472 = stablehlo.concatenate %1470, %1471, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %1473 = stablehlo.reshape %1472 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %1474 = stablehlo.reshape %1441 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %1475 = stablehlo.broadcast_in_dim %1449, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %1476 = stablehlo.broadcast_in_dim %1450, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %1477 = stablehlo.slice %1474 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %1478 = stablehlo.slice %1474 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %1479 = stablehlo.broadcast_in_dim %1475, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %1480 = stablehlo.multiply %1477, %1479 : tensor<16x4x64xbf16> loc(#loc1735)
    %1481 = stablehlo.broadcast_in_dim %1476, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %1482 = stablehlo.multiply %1478, %1481 : tensor<16x4x64xbf16> loc(#loc1736)
    %1483 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %1484 = stablehlo.multiply %1482, %1483 : tensor<16x4x64xbf16> loc(#loc1737)
    %1485 = stablehlo.subtract %1480, %1484 : tensor<16x4x64xbf16> loc(#loc1738)
    %1486 = stablehlo.broadcast_in_dim %1475, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %1487 = stablehlo.multiply %1478, %1486 : tensor<16x4x64xbf16> loc(#loc1739)
    %1488 = stablehlo.broadcast_in_dim %1476, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %1489 = stablehlo.multiply %1477, %1488 : tensor<16x4x64xbf16> loc(#loc1740)
    %1490 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %1491 = stablehlo.multiply %1489, %1490 : tensor<16x4x64xbf16> loc(#loc1741)
    %1492 = stablehlo.add %1487, %1491 : tensor<16x4x64xbf16> loc(#loc1742)
    %1493 = stablehlo.concatenate %1485, %1492, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %1494 = stablehlo.slice %1474 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %1495 = stablehlo.concatenate %1493, %1494, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %1496 = stablehlo.reshape %1495 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %1497:2 = call @_jax_attn_func(%arg444, %1473, %1496, %1405, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %1498 = stablehlo.dot_general %1497#1, %arg431, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %1499 = stablehlo.reshape %1498 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %1500 = stablehlo.reshape %1499 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %1501 = stablehlo.convert %1500 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %1502 = stablehlo.convert %1378 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %1503 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %1504 = stablehlo.multiply %1502, %1503 : tensor<16x2048xf32> loc(#loc1751)
    %1505 = stablehlo.add %1501, %1504 : tensor<16x2048xf32> loc(#loc1752)
    %1506 = stablehlo.convert %1505 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %1507 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %1508 = stablehlo.power %1505, %1507 : tensor<16x2048xf32> loc(#loc1754)
    %1509 = stablehlo.reduce(%1508 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %1510 = stablehlo.broadcast_in_dim %1509, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %1511 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %1512 = stablehlo.divide %1510, %1511 : tensor<16x1xf32> loc(#loc1757)
    %1513 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %1514 = stablehlo.add %1512, %1513 : tensor<16x1xf32> loc(#loc1758)
    %1515 = stablehlo.rsqrt %1514 : tensor<16x1xf32> loc(#loc1678)
    %1516 = stablehlo.broadcast_in_dim %1515, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %1517 = stablehlo.multiply %1505, %1516 : tensor<16x2048xf32> loc(#loc1759)
    %1518 = stablehlo.convert %1517 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %1519 = stablehlo.broadcast_in_dim %arg429, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %1520 = stablehlo.broadcast_in_dim %1519, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %1521 = stablehlo.multiply %1518, %1520 : tensor<16x2048xbf16> loc(#loc1761)
    %1522 = stablehlo.dot_general %1521, %arg428, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %1523 = stablehlo.reshape %1522 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %1524 = stablehlo.reshape %1523 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %1525 = call @jax_fused_moe_func_padded(%1521, %arg426, %arg427, %1524) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %1526 = stablehlo.convert %1525 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %1527 = stablehlo.convert %1506 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %1528 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %1529 = stablehlo.multiply %1527, %1528 : tensor<16x2048xf32> loc(#loc1766)
    %1530 = stablehlo.add %1526, %1529 : tensor<16x2048xf32> loc(#loc1767)
    %1531 = stablehlo.convert %1530 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %1532 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %1533 = stablehlo.power %1530, %1532 : tensor<16x2048xf32> loc(#loc1768)
    %1534 = stablehlo.reduce(%1533 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %1535 = stablehlo.broadcast_in_dim %1534, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %1536 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %1537 = stablehlo.divide %1535, %1536 : tensor<16x1xf32> loc(#loc1771)
    %1538 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %1539 = stablehlo.add %1537, %1538 : tensor<16x1xf32> loc(#loc1772)
    %1540 = stablehlo.rsqrt %1539 : tensor<16x1xf32> loc(#loc1678)
    %1541 = stablehlo.broadcast_in_dim %1540, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %1542 = stablehlo.multiply %1530, %1541 : tensor<16x2048xf32> loc(#loc1773)
    %1543 = stablehlo.convert %1542 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %1544 = stablehlo.broadcast_in_dim %arg20, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %1545 = stablehlo.broadcast_in_dim %1544, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %1546 = stablehlo.multiply %1543, %1545 : tensor<16x2048xbf16> loc(#loc1775)
    %1547 = stablehlo.dot_general %1546, %arg28, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %1548 = stablehlo.reshape %1547 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %1549 = stablehlo.slice %1548 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %1550 = stablehlo.reshape %1549 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %1551 = stablehlo.slice %1548 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %1552 = stablehlo.reshape %1551 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %1553 = stablehlo.slice %1548 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %1554 = stablehlo.reshape %1553 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %1555 = stablehlo.concatenate %1550, %1552, %1554, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %1556 = stablehlo.slice %1555 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %1557 = stablehlo.slice %1555 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %1558 = stablehlo.slice %1555 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %1559 = stablehlo.reshape %1556 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %1560 = stablehlo.convert %1559 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %1561 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %1562 = stablehlo.power %1560, %1561 : tensor<16x32x128xf32> loc(#loc1690)
    %1563 = stablehlo.reduce(%1562 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %1564 = stablehlo.broadcast_in_dim %1563, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %1565 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %1566 = stablehlo.divide %1564, %1565 : tensor<16x32x1xf32> loc(#loc1693)
    %1567 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %1568 = stablehlo.add %1566, %1567 : tensor<16x32x1xf32> loc(#loc1694)
    %1569 = stablehlo.rsqrt %1568 : tensor<16x32x1xf32> loc(#loc1678)
    %1570 = stablehlo.broadcast_in_dim %1569, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %1571 = stablehlo.multiply %1560, %1570 : tensor<16x32x128xf32> loc(#loc1695)
    %1572 = stablehlo.convert %1571 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %1573 = stablehlo.broadcast_in_dim %arg27, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %1574 = stablehlo.broadcast_in_dim %1573, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %1575 = stablehlo.multiply %1572, %1574 : tensor<16x32x128xbf16> loc(#loc1697)
    %1576 = stablehlo.reshape %1575 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %1577 = stablehlo.reshape %1557 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %1578 = stablehlo.convert %1577 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %1579 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %1580 = stablehlo.power %1578, %1579 : tensor<16x4x128xf32> loc(#loc1700)
    %1581 = stablehlo.reduce(%1580 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %1582 = stablehlo.broadcast_in_dim %1581, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %1583 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %1584 = stablehlo.divide %1582, %1583 : tensor<16x4x1xf32> loc(#loc1703)
    %1585 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %1586 = stablehlo.add %1584, %1585 : tensor<16x4x1xf32> loc(#loc1704)
    %1587 = stablehlo.rsqrt %1586 : tensor<16x4x1xf32> loc(#loc1678)
    %1588 = stablehlo.broadcast_in_dim %1587, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %1589 = stablehlo.multiply %1578, %1588 : tensor<16x4x128xf32> loc(#loc1705)
    %1590 = stablehlo.convert %1589 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %1591 = stablehlo.broadcast_in_dim %arg25, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %1592 = stablehlo.broadcast_in_dim %1591, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %1593 = stablehlo.multiply %1590, %1592 : tensor<16x4x128xbf16> loc(#loc1707)
    %1594 = stablehlo.reshape %1593 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %1595 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %1596 = stablehlo.compare  LT, %arg484, %1595,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %1597 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %1598 = stablehlo.add %arg484, %1597 : tensor<16xi32> loc(#loc1710)
    %1599 = stablehlo.select %1596, %1598, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %1600 = stablehlo.broadcast_in_dim %1599, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %1601 = "stablehlo.gather"(%arg10, %1600) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %1602 = stablehlo.slice %1601 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %1603 = stablehlo.slice %1601 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %1604 = stablehlo.reshape %1576 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %1605 = stablehlo.broadcast_in_dim %1602, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %1606 = stablehlo.broadcast_in_dim %1603, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %1607 = stablehlo.slice %1604 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %1608 = stablehlo.slice %1604 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %1609 = stablehlo.broadcast_in_dim %1605, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %1610 = stablehlo.multiply %1607, %1609 : tensor<16x32x64xbf16> loc(#loc1719)
    %1611 = stablehlo.broadcast_in_dim %1606, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %1612 = stablehlo.multiply %1608, %1611 : tensor<16x32x64xbf16> loc(#loc1720)
    %1613 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %1614 = stablehlo.multiply %1612, %1613 : tensor<16x32x64xbf16> loc(#loc1721)
    %1615 = stablehlo.subtract %1610, %1614 : tensor<16x32x64xbf16> loc(#loc1722)
    %1616 = stablehlo.broadcast_in_dim %1605, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %1617 = stablehlo.multiply %1608, %1616 : tensor<16x32x64xbf16> loc(#loc1723)
    %1618 = stablehlo.broadcast_in_dim %1606, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %1619 = stablehlo.multiply %1607, %1618 : tensor<16x32x64xbf16> loc(#loc1724)
    %1620 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %1621 = stablehlo.multiply %1619, %1620 : tensor<16x32x64xbf16> loc(#loc1725)
    %1622 = stablehlo.add %1617, %1621 : tensor<16x32x64xbf16> loc(#loc1726)
    %1623 = stablehlo.concatenate %1615, %1622, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %1624 = stablehlo.slice %1604 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %1625 = stablehlo.concatenate %1623, %1624, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %1626 = stablehlo.reshape %1625 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %1627 = stablehlo.reshape %1594 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %1628 = stablehlo.broadcast_in_dim %1602, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %1629 = stablehlo.broadcast_in_dim %1603, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %1630 = stablehlo.slice %1627 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %1631 = stablehlo.slice %1627 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %1632 = stablehlo.broadcast_in_dim %1628, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %1633 = stablehlo.multiply %1630, %1632 : tensor<16x4x64xbf16> loc(#loc1735)
    %1634 = stablehlo.broadcast_in_dim %1629, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %1635 = stablehlo.multiply %1631, %1634 : tensor<16x4x64xbf16> loc(#loc1736)
    %1636 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %1637 = stablehlo.multiply %1635, %1636 : tensor<16x4x64xbf16> loc(#loc1737)
    %1638 = stablehlo.subtract %1633, %1637 : tensor<16x4x64xbf16> loc(#loc1738)
    %1639 = stablehlo.broadcast_in_dim %1628, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %1640 = stablehlo.multiply %1631, %1639 : tensor<16x4x64xbf16> loc(#loc1739)
    %1641 = stablehlo.broadcast_in_dim %1629, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %1642 = stablehlo.multiply %1630, %1641 : tensor<16x4x64xbf16> loc(#loc1740)
    %1643 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %1644 = stablehlo.multiply %1642, %1643 : tensor<16x4x64xbf16> loc(#loc1741)
    %1645 = stablehlo.add %1640, %1644 : tensor<16x4x64xbf16> loc(#loc1742)
    %1646 = stablehlo.concatenate %1638, %1645, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %1647 = stablehlo.slice %1627 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %1648 = stablehlo.concatenate %1646, %1647, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %1649 = stablehlo.reshape %1648 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %1650:2 = call @_jax_attn_func(%arg445, %1626, %1649, %1558, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %1651 = stablehlo.dot_general %1650#1, %arg26, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %1652 = stablehlo.reshape %1651 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %1653 = stablehlo.reshape %1652 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %1654 = stablehlo.convert %1653 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %1655 = stablehlo.convert %1531 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %1656 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %1657 = stablehlo.multiply %1655, %1656 : tensor<16x2048xf32> loc(#loc1751)
    %1658 = stablehlo.add %1654, %1657 : tensor<16x2048xf32> loc(#loc1752)
    %1659 = stablehlo.convert %1658 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %1660 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %1661 = stablehlo.power %1658, %1660 : tensor<16x2048xf32> loc(#loc1754)
    %1662 = stablehlo.reduce(%1661 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %1663 = stablehlo.broadcast_in_dim %1662, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %1664 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %1665 = stablehlo.divide %1663, %1664 : tensor<16x1xf32> loc(#loc1757)
    %1666 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %1667 = stablehlo.add %1665, %1666 : tensor<16x1xf32> loc(#loc1758)
    %1668 = stablehlo.rsqrt %1667 : tensor<16x1xf32> loc(#loc1678)
    %1669 = stablehlo.broadcast_in_dim %1668, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %1670 = stablehlo.multiply %1658, %1669 : tensor<16x2048xf32> loc(#loc1759)
    %1671 = stablehlo.convert %1670 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %1672 = stablehlo.broadcast_in_dim %arg24, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %1673 = stablehlo.broadcast_in_dim %1672, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %1674 = stablehlo.multiply %1671, %1673 : tensor<16x2048xbf16> loc(#loc1761)
    %1675 = stablehlo.dot_general %1674, %arg23, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %1676 = stablehlo.reshape %1675 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %1677 = stablehlo.reshape %1676 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %1678 = call @jax_fused_moe_func_padded(%1674, %arg21, %arg22, %1677) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %1679 = stablehlo.convert %1678 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %1680 = stablehlo.convert %1659 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %1681 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %1682 = stablehlo.multiply %1680, %1681 : tensor<16x2048xf32> loc(#loc1766)
    %1683 = stablehlo.add %1679, %1682 : tensor<16x2048xf32> loc(#loc1767)
    %1684 = stablehlo.convert %1683 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %1685 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %1686 = stablehlo.power %1683, %1685 : tensor<16x2048xf32> loc(#loc1768)
    %1687 = stablehlo.reduce(%1686 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %1688 = stablehlo.broadcast_in_dim %1687, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %1689 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %1690 = stablehlo.divide %1688, %1689 : tensor<16x1xf32> loc(#loc1771)
    %1691 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %1692 = stablehlo.add %1690, %1691 : tensor<16x1xf32> loc(#loc1772)
    %1693 = stablehlo.rsqrt %1692 : tensor<16x1xf32> loc(#loc1678)
    %1694 = stablehlo.broadcast_in_dim %1693, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %1695 = stablehlo.multiply %1683, %1694 : tensor<16x2048xf32> loc(#loc1773)
    %1696 = stablehlo.convert %1695 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %1697 = stablehlo.broadcast_in_dim %arg29, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %1698 = stablehlo.broadcast_in_dim %1697, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %1699 = stablehlo.multiply %1696, %1698 : tensor<16x2048xbf16> loc(#loc1775)
    %1700 = stablehlo.dot_general %1699, %arg37, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %1701 = stablehlo.reshape %1700 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %1702 = stablehlo.slice %1701 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %1703 = stablehlo.reshape %1702 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %1704 = stablehlo.slice %1701 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %1705 = stablehlo.reshape %1704 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %1706 = stablehlo.slice %1701 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %1707 = stablehlo.reshape %1706 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %1708 = stablehlo.concatenate %1703, %1705, %1707, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %1709 = stablehlo.slice %1708 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %1710 = stablehlo.slice %1708 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %1711 = stablehlo.slice %1708 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %1712 = stablehlo.reshape %1709 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %1713 = stablehlo.convert %1712 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %1714 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %1715 = stablehlo.power %1713, %1714 : tensor<16x32x128xf32> loc(#loc1690)
    %1716 = stablehlo.reduce(%1715 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %1717 = stablehlo.broadcast_in_dim %1716, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %1718 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %1719 = stablehlo.divide %1717, %1718 : tensor<16x32x1xf32> loc(#loc1693)
    %1720 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %1721 = stablehlo.add %1719, %1720 : tensor<16x32x1xf32> loc(#loc1694)
    %1722 = stablehlo.rsqrt %1721 : tensor<16x32x1xf32> loc(#loc1678)
    %1723 = stablehlo.broadcast_in_dim %1722, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %1724 = stablehlo.multiply %1713, %1723 : tensor<16x32x128xf32> loc(#loc1695)
    %1725 = stablehlo.convert %1724 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %1726 = stablehlo.broadcast_in_dim %arg36, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %1727 = stablehlo.broadcast_in_dim %1726, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %1728 = stablehlo.multiply %1725, %1727 : tensor<16x32x128xbf16> loc(#loc1697)
    %1729 = stablehlo.reshape %1728 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %1730 = stablehlo.reshape %1710 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %1731 = stablehlo.convert %1730 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %1732 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %1733 = stablehlo.power %1731, %1732 : tensor<16x4x128xf32> loc(#loc1700)
    %1734 = stablehlo.reduce(%1733 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %1735 = stablehlo.broadcast_in_dim %1734, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %1736 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %1737 = stablehlo.divide %1735, %1736 : tensor<16x4x1xf32> loc(#loc1703)
    %1738 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %1739 = stablehlo.add %1737, %1738 : tensor<16x4x1xf32> loc(#loc1704)
    %1740 = stablehlo.rsqrt %1739 : tensor<16x4x1xf32> loc(#loc1678)
    %1741 = stablehlo.broadcast_in_dim %1740, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %1742 = stablehlo.multiply %1731, %1741 : tensor<16x4x128xf32> loc(#loc1705)
    %1743 = stablehlo.convert %1742 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %1744 = stablehlo.broadcast_in_dim %arg34, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %1745 = stablehlo.broadcast_in_dim %1744, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %1746 = stablehlo.multiply %1743, %1745 : tensor<16x4x128xbf16> loc(#loc1707)
    %1747 = stablehlo.reshape %1746 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %1748 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %1749 = stablehlo.compare  LT, %arg484, %1748,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %1750 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %1751 = stablehlo.add %arg484, %1750 : tensor<16xi32> loc(#loc1710)
    %1752 = stablehlo.select %1749, %1751, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %1753 = stablehlo.broadcast_in_dim %1752, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %1754 = "stablehlo.gather"(%arg10, %1753) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %1755 = stablehlo.slice %1754 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %1756 = stablehlo.slice %1754 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %1757 = stablehlo.reshape %1729 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %1758 = stablehlo.broadcast_in_dim %1755, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %1759 = stablehlo.broadcast_in_dim %1756, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %1760 = stablehlo.slice %1757 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %1761 = stablehlo.slice %1757 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %1762 = stablehlo.broadcast_in_dim %1758, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %1763 = stablehlo.multiply %1760, %1762 : tensor<16x32x64xbf16> loc(#loc1719)
    %1764 = stablehlo.broadcast_in_dim %1759, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %1765 = stablehlo.multiply %1761, %1764 : tensor<16x32x64xbf16> loc(#loc1720)
    %1766 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %1767 = stablehlo.multiply %1765, %1766 : tensor<16x32x64xbf16> loc(#loc1721)
    %1768 = stablehlo.subtract %1763, %1767 : tensor<16x32x64xbf16> loc(#loc1722)
    %1769 = stablehlo.broadcast_in_dim %1758, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %1770 = stablehlo.multiply %1761, %1769 : tensor<16x32x64xbf16> loc(#loc1723)
    %1771 = stablehlo.broadcast_in_dim %1759, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %1772 = stablehlo.multiply %1760, %1771 : tensor<16x32x64xbf16> loc(#loc1724)
    %1773 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %1774 = stablehlo.multiply %1772, %1773 : tensor<16x32x64xbf16> loc(#loc1725)
    %1775 = stablehlo.add %1770, %1774 : tensor<16x32x64xbf16> loc(#loc1726)
    %1776 = stablehlo.concatenate %1768, %1775, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %1777 = stablehlo.slice %1757 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %1778 = stablehlo.concatenate %1776, %1777, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %1779 = stablehlo.reshape %1778 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %1780 = stablehlo.reshape %1747 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %1781 = stablehlo.broadcast_in_dim %1755, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %1782 = stablehlo.broadcast_in_dim %1756, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %1783 = stablehlo.slice %1780 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %1784 = stablehlo.slice %1780 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %1785 = stablehlo.broadcast_in_dim %1781, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %1786 = stablehlo.multiply %1783, %1785 : tensor<16x4x64xbf16> loc(#loc1735)
    %1787 = stablehlo.broadcast_in_dim %1782, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %1788 = stablehlo.multiply %1784, %1787 : tensor<16x4x64xbf16> loc(#loc1736)
    %1789 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %1790 = stablehlo.multiply %1788, %1789 : tensor<16x4x64xbf16> loc(#loc1737)
    %1791 = stablehlo.subtract %1786, %1790 : tensor<16x4x64xbf16> loc(#loc1738)
    %1792 = stablehlo.broadcast_in_dim %1781, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %1793 = stablehlo.multiply %1784, %1792 : tensor<16x4x64xbf16> loc(#loc1739)
    %1794 = stablehlo.broadcast_in_dim %1782, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %1795 = stablehlo.multiply %1783, %1794 : tensor<16x4x64xbf16> loc(#loc1740)
    %1796 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %1797 = stablehlo.multiply %1795, %1796 : tensor<16x4x64xbf16> loc(#loc1741)
    %1798 = stablehlo.add %1793, %1797 : tensor<16x4x64xbf16> loc(#loc1742)
    %1799 = stablehlo.concatenate %1791, %1798, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %1800 = stablehlo.slice %1780 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %1801 = stablehlo.concatenate %1799, %1800, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %1802 = stablehlo.reshape %1801 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %1803:2 = call @_jax_attn_func(%arg446, %1779, %1802, %1711, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %1804 = stablehlo.dot_general %1803#1, %arg35, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %1805 = stablehlo.reshape %1804 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %1806 = stablehlo.reshape %1805 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %1807 = stablehlo.convert %1806 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %1808 = stablehlo.convert %1684 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %1809 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %1810 = stablehlo.multiply %1808, %1809 : tensor<16x2048xf32> loc(#loc1751)
    %1811 = stablehlo.add %1807, %1810 : tensor<16x2048xf32> loc(#loc1752)
    %1812 = stablehlo.convert %1811 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %1813 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %1814 = stablehlo.power %1811, %1813 : tensor<16x2048xf32> loc(#loc1754)
    %1815 = stablehlo.reduce(%1814 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %1816 = stablehlo.broadcast_in_dim %1815, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %1817 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %1818 = stablehlo.divide %1816, %1817 : tensor<16x1xf32> loc(#loc1757)
    %1819 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %1820 = stablehlo.add %1818, %1819 : tensor<16x1xf32> loc(#loc1758)
    %1821 = stablehlo.rsqrt %1820 : tensor<16x1xf32> loc(#loc1678)
    %1822 = stablehlo.broadcast_in_dim %1821, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %1823 = stablehlo.multiply %1811, %1822 : tensor<16x2048xf32> loc(#loc1759)
    %1824 = stablehlo.convert %1823 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %1825 = stablehlo.broadcast_in_dim %arg33, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %1826 = stablehlo.broadcast_in_dim %1825, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %1827 = stablehlo.multiply %1824, %1826 : tensor<16x2048xbf16> loc(#loc1761)
    %1828 = stablehlo.dot_general %1827, %arg32, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %1829 = stablehlo.reshape %1828 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %1830 = stablehlo.reshape %1829 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %1831 = call @jax_fused_moe_func_padded(%1827, %arg30, %arg31, %1830) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %1832 = stablehlo.convert %1831 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %1833 = stablehlo.convert %1812 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %1834 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %1835 = stablehlo.multiply %1833, %1834 : tensor<16x2048xf32> loc(#loc1766)
    %1836 = stablehlo.add %1832, %1835 : tensor<16x2048xf32> loc(#loc1767)
    %1837 = stablehlo.convert %1836 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %1838 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %1839 = stablehlo.power %1836, %1838 : tensor<16x2048xf32> loc(#loc1768)
    %1840 = stablehlo.reduce(%1839 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %1841 = stablehlo.broadcast_in_dim %1840, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %1842 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %1843 = stablehlo.divide %1841, %1842 : tensor<16x1xf32> loc(#loc1771)
    %1844 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %1845 = stablehlo.add %1843, %1844 : tensor<16x1xf32> loc(#loc1772)
    %1846 = stablehlo.rsqrt %1845 : tensor<16x1xf32> loc(#loc1678)
    %1847 = stablehlo.broadcast_in_dim %1846, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %1848 = stablehlo.multiply %1836, %1847 : tensor<16x2048xf32> loc(#loc1773)
    %1849 = stablehlo.convert %1848 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %1850 = stablehlo.broadcast_in_dim %arg38, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %1851 = stablehlo.broadcast_in_dim %1850, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %1852 = stablehlo.multiply %1849, %1851 : tensor<16x2048xbf16> loc(#loc1775)
    %1853 = stablehlo.dot_general %1852, %arg46, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %1854 = stablehlo.reshape %1853 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %1855 = stablehlo.slice %1854 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %1856 = stablehlo.reshape %1855 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %1857 = stablehlo.slice %1854 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %1858 = stablehlo.reshape %1857 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %1859 = stablehlo.slice %1854 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %1860 = stablehlo.reshape %1859 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %1861 = stablehlo.concatenate %1856, %1858, %1860, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %1862 = stablehlo.slice %1861 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %1863 = stablehlo.slice %1861 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %1864 = stablehlo.slice %1861 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %1865 = stablehlo.reshape %1862 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %1866 = stablehlo.convert %1865 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %1867 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %1868 = stablehlo.power %1866, %1867 : tensor<16x32x128xf32> loc(#loc1690)
    %1869 = stablehlo.reduce(%1868 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %1870 = stablehlo.broadcast_in_dim %1869, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %1871 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %1872 = stablehlo.divide %1870, %1871 : tensor<16x32x1xf32> loc(#loc1693)
    %1873 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %1874 = stablehlo.add %1872, %1873 : tensor<16x32x1xf32> loc(#loc1694)
    %1875 = stablehlo.rsqrt %1874 : tensor<16x32x1xf32> loc(#loc1678)
    %1876 = stablehlo.broadcast_in_dim %1875, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %1877 = stablehlo.multiply %1866, %1876 : tensor<16x32x128xf32> loc(#loc1695)
    %1878 = stablehlo.convert %1877 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %1879 = stablehlo.broadcast_in_dim %arg45, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %1880 = stablehlo.broadcast_in_dim %1879, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %1881 = stablehlo.multiply %1878, %1880 : tensor<16x32x128xbf16> loc(#loc1697)
    %1882 = stablehlo.reshape %1881 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %1883 = stablehlo.reshape %1863 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %1884 = stablehlo.convert %1883 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %1885 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %1886 = stablehlo.power %1884, %1885 : tensor<16x4x128xf32> loc(#loc1700)
    %1887 = stablehlo.reduce(%1886 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %1888 = stablehlo.broadcast_in_dim %1887, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %1889 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %1890 = stablehlo.divide %1888, %1889 : tensor<16x4x1xf32> loc(#loc1703)
    %1891 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %1892 = stablehlo.add %1890, %1891 : tensor<16x4x1xf32> loc(#loc1704)
    %1893 = stablehlo.rsqrt %1892 : tensor<16x4x1xf32> loc(#loc1678)
    %1894 = stablehlo.broadcast_in_dim %1893, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %1895 = stablehlo.multiply %1884, %1894 : tensor<16x4x128xf32> loc(#loc1705)
    %1896 = stablehlo.convert %1895 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %1897 = stablehlo.broadcast_in_dim %arg43, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %1898 = stablehlo.broadcast_in_dim %1897, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %1899 = stablehlo.multiply %1896, %1898 : tensor<16x4x128xbf16> loc(#loc1707)
    %1900 = stablehlo.reshape %1899 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %1901 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %1902 = stablehlo.compare  LT, %arg484, %1901,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %1903 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %1904 = stablehlo.add %arg484, %1903 : tensor<16xi32> loc(#loc1710)
    %1905 = stablehlo.select %1902, %1904, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %1906 = stablehlo.broadcast_in_dim %1905, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %1907 = "stablehlo.gather"(%arg10, %1906) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %1908 = stablehlo.slice %1907 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %1909 = stablehlo.slice %1907 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %1910 = stablehlo.reshape %1882 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %1911 = stablehlo.broadcast_in_dim %1908, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %1912 = stablehlo.broadcast_in_dim %1909, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %1913 = stablehlo.slice %1910 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %1914 = stablehlo.slice %1910 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %1915 = stablehlo.broadcast_in_dim %1911, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %1916 = stablehlo.multiply %1913, %1915 : tensor<16x32x64xbf16> loc(#loc1719)
    %1917 = stablehlo.broadcast_in_dim %1912, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %1918 = stablehlo.multiply %1914, %1917 : tensor<16x32x64xbf16> loc(#loc1720)
    %1919 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %1920 = stablehlo.multiply %1918, %1919 : tensor<16x32x64xbf16> loc(#loc1721)
    %1921 = stablehlo.subtract %1916, %1920 : tensor<16x32x64xbf16> loc(#loc1722)
    %1922 = stablehlo.broadcast_in_dim %1911, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %1923 = stablehlo.multiply %1914, %1922 : tensor<16x32x64xbf16> loc(#loc1723)
    %1924 = stablehlo.broadcast_in_dim %1912, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %1925 = stablehlo.multiply %1913, %1924 : tensor<16x32x64xbf16> loc(#loc1724)
    %1926 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %1927 = stablehlo.multiply %1925, %1926 : tensor<16x32x64xbf16> loc(#loc1725)
    %1928 = stablehlo.add %1923, %1927 : tensor<16x32x64xbf16> loc(#loc1726)
    %1929 = stablehlo.concatenate %1921, %1928, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %1930 = stablehlo.slice %1910 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %1931 = stablehlo.concatenate %1929, %1930, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %1932 = stablehlo.reshape %1931 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %1933 = stablehlo.reshape %1900 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %1934 = stablehlo.broadcast_in_dim %1908, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %1935 = stablehlo.broadcast_in_dim %1909, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %1936 = stablehlo.slice %1933 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %1937 = stablehlo.slice %1933 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %1938 = stablehlo.broadcast_in_dim %1934, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %1939 = stablehlo.multiply %1936, %1938 : tensor<16x4x64xbf16> loc(#loc1735)
    %1940 = stablehlo.broadcast_in_dim %1935, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %1941 = stablehlo.multiply %1937, %1940 : tensor<16x4x64xbf16> loc(#loc1736)
    %1942 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %1943 = stablehlo.multiply %1941, %1942 : tensor<16x4x64xbf16> loc(#loc1737)
    %1944 = stablehlo.subtract %1939, %1943 : tensor<16x4x64xbf16> loc(#loc1738)
    %1945 = stablehlo.broadcast_in_dim %1934, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %1946 = stablehlo.multiply %1937, %1945 : tensor<16x4x64xbf16> loc(#loc1739)
    %1947 = stablehlo.broadcast_in_dim %1935, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %1948 = stablehlo.multiply %1936, %1947 : tensor<16x4x64xbf16> loc(#loc1740)
    %1949 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %1950 = stablehlo.multiply %1948, %1949 : tensor<16x4x64xbf16> loc(#loc1741)
    %1951 = stablehlo.add %1946, %1950 : tensor<16x4x64xbf16> loc(#loc1742)
    %1952 = stablehlo.concatenate %1944, %1951, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %1953 = stablehlo.slice %1933 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %1954 = stablehlo.concatenate %1952, %1953, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %1955 = stablehlo.reshape %1954 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %1956:2 = call @_jax_attn_func(%arg447, %1932, %1955, %1864, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %1957 = stablehlo.dot_general %1956#1, %arg44, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %1958 = stablehlo.reshape %1957 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %1959 = stablehlo.reshape %1958 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %1960 = stablehlo.convert %1959 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %1961 = stablehlo.convert %1837 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %1962 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %1963 = stablehlo.multiply %1961, %1962 : tensor<16x2048xf32> loc(#loc1751)
    %1964 = stablehlo.add %1960, %1963 : tensor<16x2048xf32> loc(#loc1752)
    %1965 = stablehlo.convert %1964 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %1966 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %1967 = stablehlo.power %1964, %1966 : tensor<16x2048xf32> loc(#loc1754)
    %1968 = stablehlo.reduce(%1967 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %1969 = stablehlo.broadcast_in_dim %1968, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %1970 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %1971 = stablehlo.divide %1969, %1970 : tensor<16x1xf32> loc(#loc1757)
    %1972 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %1973 = stablehlo.add %1971, %1972 : tensor<16x1xf32> loc(#loc1758)
    %1974 = stablehlo.rsqrt %1973 : tensor<16x1xf32> loc(#loc1678)
    %1975 = stablehlo.broadcast_in_dim %1974, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %1976 = stablehlo.multiply %1964, %1975 : tensor<16x2048xf32> loc(#loc1759)
    %1977 = stablehlo.convert %1976 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %1978 = stablehlo.broadcast_in_dim %arg42, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %1979 = stablehlo.broadcast_in_dim %1978, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %1980 = stablehlo.multiply %1977, %1979 : tensor<16x2048xbf16> loc(#loc1761)
    %1981 = stablehlo.dot_general %1980, %arg41, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %1982 = stablehlo.reshape %1981 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %1983 = stablehlo.reshape %1982 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %1984 = call @jax_fused_moe_func_padded(%1980, %arg39, %arg40, %1983) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %1985 = stablehlo.convert %1984 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %1986 = stablehlo.convert %1965 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %1987 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %1988 = stablehlo.multiply %1986, %1987 : tensor<16x2048xf32> loc(#loc1766)
    %1989 = stablehlo.add %1985, %1988 : tensor<16x2048xf32> loc(#loc1767)
    %1990 = stablehlo.convert %1989 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %1991 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %1992 = stablehlo.power %1989, %1991 : tensor<16x2048xf32> loc(#loc1768)
    %1993 = stablehlo.reduce(%1992 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %1994 = stablehlo.broadcast_in_dim %1993, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %1995 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %1996 = stablehlo.divide %1994, %1995 : tensor<16x1xf32> loc(#loc1771)
    %1997 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %1998 = stablehlo.add %1996, %1997 : tensor<16x1xf32> loc(#loc1772)
    %1999 = stablehlo.rsqrt %1998 : tensor<16x1xf32> loc(#loc1678)
    %2000 = stablehlo.broadcast_in_dim %1999, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %2001 = stablehlo.multiply %1989, %2000 : tensor<16x2048xf32> loc(#loc1773)
    %2002 = stablehlo.convert %2001 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %2003 = stablehlo.broadcast_in_dim %arg47, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %2004 = stablehlo.broadcast_in_dim %2003, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %2005 = stablehlo.multiply %2002, %2004 : tensor<16x2048xbf16> loc(#loc1775)
    %2006 = stablehlo.dot_general %2005, %arg55, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %2007 = stablehlo.reshape %2006 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %2008 = stablehlo.slice %2007 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %2009 = stablehlo.reshape %2008 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %2010 = stablehlo.slice %2007 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %2011 = stablehlo.reshape %2010 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %2012 = stablehlo.slice %2007 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %2013 = stablehlo.reshape %2012 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %2014 = stablehlo.concatenate %2009, %2011, %2013, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %2015 = stablehlo.slice %2014 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %2016 = stablehlo.slice %2014 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %2017 = stablehlo.slice %2014 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %2018 = stablehlo.reshape %2015 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %2019 = stablehlo.convert %2018 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %2020 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %2021 = stablehlo.power %2019, %2020 : tensor<16x32x128xf32> loc(#loc1690)
    %2022 = stablehlo.reduce(%2021 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %2023 = stablehlo.broadcast_in_dim %2022, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %2024 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %2025 = stablehlo.divide %2023, %2024 : tensor<16x32x1xf32> loc(#loc1693)
    %2026 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %2027 = stablehlo.add %2025, %2026 : tensor<16x32x1xf32> loc(#loc1694)
    %2028 = stablehlo.rsqrt %2027 : tensor<16x32x1xf32> loc(#loc1678)
    %2029 = stablehlo.broadcast_in_dim %2028, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %2030 = stablehlo.multiply %2019, %2029 : tensor<16x32x128xf32> loc(#loc1695)
    %2031 = stablehlo.convert %2030 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %2032 = stablehlo.broadcast_in_dim %arg54, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %2033 = stablehlo.broadcast_in_dim %2032, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %2034 = stablehlo.multiply %2031, %2033 : tensor<16x32x128xbf16> loc(#loc1697)
    %2035 = stablehlo.reshape %2034 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %2036 = stablehlo.reshape %2016 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %2037 = stablehlo.convert %2036 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %2038 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %2039 = stablehlo.power %2037, %2038 : tensor<16x4x128xf32> loc(#loc1700)
    %2040 = stablehlo.reduce(%2039 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %2041 = stablehlo.broadcast_in_dim %2040, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %2042 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %2043 = stablehlo.divide %2041, %2042 : tensor<16x4x1xf32> loc(#loc1703)
    %2044 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %2045 = stablehlo.add %2043, %2044 : tensor<16x4x1xf32> loc(#loc1704)
    %2046 = stablehlo.rsqrt %2045 : tensor<16x4x1xf32> loc(#loc1678)
    %2047 = stablehlo.broadcast_in_dim %2046, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %2048 = stablehlo.multiply %2037, %2047 : tensor<16x4x128xf32> loc(#loc1705)
    %2049 = stablehlo.convert %2048 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %2050 = stablehlo.broadcast_in_dim %arg52, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %2051 = stablehlo.broadcast_in_dim %2050, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %2052 = stablehlo.multiply %2049, %2051 : tensor<16x4x128xbf16> loc(#loc1707)
    %2053 = stablehlo.reshape %2052 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %2054 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %2055 = stablehlo.compare  LT, %arg484, %2054,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %2056 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %2057 = stablehlo.add %arg484, %2056 : tensor<16xi32> loc(#loc1710)
    %2058 = stablehlo.select %2055, %2057, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %2059 = stablehlo.broadcast_in_dim %2058, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %2060 = "stablehlo.gather"(%arg10, %2059) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %2061 = stablehlo.slice %2060 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %2062 = stablehlo.slice %2060 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %2063 = stablehlo.reshape %2035 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %2064 = stablehlo.broadcast_in_dim %2061, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %2065 = stablehlo.broadcast_in_dim %2062, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %2066 = stablehlo.slice %2063 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %2067 = stablehlo.slice %2063 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %2068 = stablehlo.broadcast_in_dim %2064, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %2069 = stablehlo.multiply %2066, %2068 : tensor<16x32x64xbf16> loc(#loc1719)
    %2070 = stablehlo.broadcast_in_dim %2065, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %2071 = stablehlo.multiply %2067, %2070 : tensor<16x32x64xbf16> loc(#loc1720)
    %2072 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %2073 = stablehlo.multiply %2071, %2072 : tensor<16x32x64xbf16> loc(#loc1721)
    %2074 = stablehlo.subtract %2069, %2073 : tensor<16x32x64xbf16> loc(#loc1722)
    %2075 = stablehlo.broadcast_in_dim %2064, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %2076 = stablehlo.multiply %2067, %2075 : tensor<16x32x64xbf16> loc(#loc1723)
    %2077 = stablehlo.broadcast_in_dim %2065, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %2078 = stablehlo.multiply %2066, %2077 : tensor<16x32x64xbf16> loc(#loc1724)
    %2079 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %2080 = stablehlo.multiply %2078, %2079 : tensor<16x32x64xbf16> loc(#loc1725)
    %2081 = stablehlo.add %2076, %2080 : tensor<16x32x64xbf16> loc(#loc1726)
    %2082 = stablehlo.concatenate %2074, %2081, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %2083 = stablehlo.slice %2063 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %2084 = stablehlo.concatenate %2082, %2083, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %2085 = stablehlo.reshape %2084 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %2086 = stablehlo.reshape %2053 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %2087 = stablehlo.broadcast_in_dim %2061, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %2088 = stablehlo.broadcast_in_dim %2062, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %2089 = stablehlo.slice %2086 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %2090 = stablehlo.slice %2086 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %2091 = stablehlo.broadcast_in_dim %2087, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %2092 = stablehlo.multiply %2089, %2091 : tensor<16x4x64xbf16> loc(#loc1735)
    %2093 = stablehlo.broadcast_in_dim %2088, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %2094 = stablehlo.multiply %2090, %2093 : tensor<16x4x64xbf16> loc(#loc1736)
    %2095 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %2096 = stablehlo.multiply %2094, %2095 : tensor<16x4x64xbf16> loc(#loc1737)
    %2097 = stablehlo.subtract %2092, %2096 : tensor<16x4x64xbf16> loc(#loc1738)
    %2098 = stablehlo.broadcast_in_dim %2087, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %2099 = stablehlo.multiply %2090, %2098 : tensor<16x4x64xbf16> loc(#loc1739)
    %2100 = stablehlo.broadcast_in_dim %2088, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %2101 = stablehlo.multiply %2089, %2100 : tensor<16x4x64xbf16> loc(#loc1740)
    %2102 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %2103 = stablehlo.multiply %2101, %2102 : tensor<16x4x64xbf16> loc(#loc1741)
    %2104 = stablehlo.add %2099, %2103 : tensor<16x4x64xbf16> loc(#loc1742)
    %2105 = stablehlo.concatenate %2097, %2104, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %2106 = stablehlo.slice %2086 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %2107 = stablehlo.concatenate %2105, %2106, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %2108 = stablehlo.reshape %2107 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %2109:2 = call @_jax_attn_func(%arg448, %2085, %2108, %2017, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %2110 = stablehlo.dot_general %2109#1, %arg53, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %2111 = stablehlo.reshape %2110 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %2112 = stablehlo.reshape %2111 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %2113 = stablehlo.convert %2112 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %2114 = stablehlo.convert %1990 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %2115 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %2116 = stablehlo.multiply %2114, %2115 : tensor<16x2048xf32> loc(#loc1751)
    %2117 = stablehlo.add %2113, %2116 : tensor<16x2048xf32> loc(#loc1752)
    %2118 = stablehlo.convert %2117 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %2119 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %2120 = stablehlo.power %2117, %2119 : tensor<16x2048xf32> loc(#loc1754)
    %2121 = stablehlo.reduce(%2120 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %2122 = stablehlo.broadcast_in_dim %2121, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %2123 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %2124 = stablehlo.divide %2122, %2123 : tensor<16x1xf32> loc(#loc1757)
    %2125 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %2126 = stablehlo.add %2124, %2125 : tensor<16x1xf32> loc(#loc1758)
    %2127 = stablehlo.rsqrt %2126 : tensor<16x1xf32> loc(#loc1678)
    %2128 = stablehlo.broadcast_in_dim %2127, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %2129 = stablehlo.multiply %2117, %2128 : tensor<16x2048xf32> loc(#loc1759)
    %2130 = stablehlo.convert %2129 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %2131 = stablehlo.broadcast_in_dim %arg51, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %2132 = stablehlo.broadcast_in_dim %2131, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %2133 = stablehlo.multiply %2130, %2132 : tensor<16x2048xbf16> loc(#loc1761)
    %2134 = stablehlo.dot_general %2133, %arg50, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %2135 = stablehlo.reshape %2134 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %2136 = stablehlo.reshape %2135 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %2137 = call @jax_fused_moe_func_padded(%2133, %arg48, %arg49, %2136) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %2138 = stablehlo.convert %2137 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %2139 = stablehlo.convert %2118 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %2140 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %2141 = stablehlo.multiply %2139, %2140 : tensor<16x2048xf32> loc(#loc1766)
    %2142 = stablehlo.add %2138, %2141 : tensor<16x2048xf32> loc(#loc1767)
    %2143 = stablehlo.convert %2142 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %2144 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %2145 = stablehlo.power %2142, %2144 : tensor<16x2048xf32> loc(#loc1768)
    %2146 = stablehlo.reduce(%2145 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %2147 = stablehlo.broadcast_in_dim %2146, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %2148 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %2149 = stablehlo.divide %2147, %2148 : tensor<16x1xf32> loc(#loc1771)
    %2150 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %2151 = stablehlo.add %2149, %2150 : tensor<16x1xf32> loc(#loc1772)
    %2152 = stablehlo.rsqrt %2151 : tensor<16x1xf32> loc(#loc1678)
    %2153 = stablehlo.broadcast_in_dim %2152, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %2154 = stablehlo.multiply %2142, %2153 : tensor<16x2048xf32> loc(#loc1773)
    %2155 = stablehlo.convert %2154 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %2156 = stablehlo.broadcast_in_dim %arg56, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %2157 = stablehlo.broadcast_in_dim %2156, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %2158 = stablehlo.multiply %2155, %2157 : tensor<16x2048xbf16> loc(#loc1775)
    %2159 = stablehlo.dot_general %2158, %arg64, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %2160 = stablehlo.reshape %2159 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %2161 = stablehlo.slice %2160 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %2162 = stablehlo.reshape %2161 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %2163 = stablehlo.slice %2160 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %2164 = stablehlo.reshape %2163 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %2165 = stablehlo.slice %2160 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %2166 = stablehlo.reshape %2165 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %2167 = stablehlo.concatenate %2162, %2164, %2166, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %2168 = stablehlo.slice %2167 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %2169 = stablehlo.slice %2167 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %2170 = stablehlo.slice %2167 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %2171 = stablehlo.reshape %2168 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %2172 = stablehlo.convert %2171 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %2173 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %2174 = stablehlo.power %2172, %2173 : tensor<16x32x128xf32> loc(#loc1690)
    %2175 = stablehlo.reduce(%2174 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %2176 = stablehlo.broadcast_in_dim %2175, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %2177 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %2178 = stablehlo.divide %2176, %2177 : tensor<16x32x1xf32> loc(#loc1693)
    %2179 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %2180 = stablehlo.add %2178, %2179 : tensor<16x32x1xf32> loc(#loc1694)
    %2181 = stablehlo.rsqrt %2180 : tensor<16x32x1xf32> loc(#loc1678)
    %2182 = stablehlo.broadcast_in_dim %2181, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %2183 = stablehlo.multiply %2172, %2182 : tensor<16x32x128xf32> loc(#loc1695)
    %2184 = stablehlo.convert %2183 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %2185 = stablehlo.broadcast_in_dim %arg63, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %2186 = stablehlo.broadcast_in_dim %2185, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %2187 = stablehlo.multiply %2184, %2186 : tensor<16x32x128xbf16> loc(#loc1697)
    %2188 = stablehlo.reshape %2187 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %2189 = stablehlo.reshape %2169 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %2190 = stablehlo.convert %2189 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %2191 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %2192 = stablehlo.power %2190, %2191 : tensor<16x4x128xf32> loc(#loc1700)
    %2193 = stablehlo.reduce(%2192 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %2194 = stablehlo.broadcast_in_dim %2193, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %2195 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %2196 = stablehlo.divide %2194, %2195 : tensor<16x4x1xf32> loc(#loc1703)
    %2197 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %2198 = stablehlo.add %2196, %2197 : tensor<16x4x1xf32> loc(#loc1704)
    %2199 = stablehlo.rsqrt %2198 : tensor<16x4x1xf32> loc(#loc1678)
    %2200 = stablehlo.broadcast_in_dim %2199, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %2201 = stablehlo.multiply %2190, %2200 : tensor<16x4x128xf32> loc(#loc1705)
    %2202 = stablehlo.convert %2201 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %2203 = stablehlo.broadcast_in_dim %arg61, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %2204 = stablehlo.broadcast_in_dim %2203, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %2205 = stablehlo.multiply %2202, %2204 : tensor<16x4x128xbf16> loc(#loc1707)
    %2206 = stablehlo.reshape %2205 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %2207 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %2208 = stablehlo.compare  LT, %arg484, %2207,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %2209 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %2210 = stablehlo.add %arg484, %2209 : tensor<16xi32> loc(#loc1710)
    %2211 = stablehlo.select %2208, %2210, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %2212 = stablehlo.broadcast_in_dim %2211, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %2213 = "stablehlo.gather"(%arg10, %2212) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %2214 = stablehlo.slice %2213 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %2215 = stablehlo.slice %2213 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %2216 = stablehlo.reshape %2188 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %2217 = stablehlo.broadcast_in_dim %2214, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %2218 = stablehlo.broadcast_in_dim %2215, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %2219 = stablehlo.slice %2216 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %2220 = stablehlo.slice %2216 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %2221 = stablehlo.broadcast_in_dim %2217, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %2222 = stablehlo.multiply %2219, %2221 : tensor<16x32x64xbf16> loc(#loc1719)
    %2223 = stablehlo.broadcast_in_dim %2218, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %2224 = stablehlo.multiply %2220, %2223 : tensor<16x32x64xbf16> loc(#loc1720)
    %2225 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %2226 = stablehlo.multiply %2224, %2225 : tensor<16x32x64xbf16> loc(#loc1721)
    %2227 = stablehlo.subtract %2222, %2226 : tensor<16x32x64xbf16> loc(#loc1722)
    %2228 = stablehlo.broadcast_in_dim %2217, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %2229 = stablehlo.multiply %2220, %2228 : tensor<16x32x64xbf16> loc(#loc1723)
    %2230 = stablehlo.broadcast_in_dim %2218, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %2231 = stablehlo.multiply %2219, %2230 : tensor<16x32x64xbf16> loc(#loc1724)
    %2232 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %2233 = stablehlo.multiply %2231, %2232 : tensor<16x32x64xbf16> loc(#loc1725)
    %2234 = stablehlo.add %2229, %2233 : tensor<16x32x64xbf16> loc(#loc1726)
    %2235 = stablehlo.concatenate %2227, %2234, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %2236 = stablehlo.slice %2216 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %2237 = stablehlo.concatenate %2235, %2236, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %2238 = stablehlo.reshape %2237 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %2239 = stablehlo.reshape %2206 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %2240 = stablehlo.broadcast_in_dim %2214, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %2241 = stablehlo.broadcast_in_dim %2215, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %2242 = stablehlo.slice %2239 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %2243 = stablehlo.slice %2239 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %2244 = stablehlo.broadcast_in_dim %2240, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %2245 = stablehlo.multiply %2242, %2244 : tensor<16x4x64xbf16> loc(#loc1735)
    %2246 = stablehlo.broadcast_in_dim %2241, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %2247 = stablehlo.multiply %2243, %2246 : tensor<16x4x64xbf16> loc(#loc1736)
    %2248 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %2249 = stablehlo.multiply %2247, %2248 : tensor<16x4x64xbf16> loc(#loc1737)
    %2250 = stablehlo.subtract %2245, %2249 : tensor<16x4x64xbf16> loc(#loc1738)
    %2251 = stablehlo.broadcast_in_dim %2240, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %2252 = stablehlo.multiply %2243, %2251 : tensor<16x4x64xbf16> loc(#loc1739)
    %2253 = stablehlo.broadcast_in_dim %2241, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %2254 = stablehlo.multiply %2242, %2253 : tensor<16x4x64xbf16> loc(#loc1740)
    %2255 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %2256 = stablehlo.multiply %2254, %2255 : tensor<16x4x64xbf16> loc(#loc1741)
    %2257 = stablehlo.add %2252, %2256 : tensor<16x4x64xbf16> loc(#loc1742)
    %2258 = stablehlo.concatenate %2250, %2257, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %2259 = stablehlo.slice %2239 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %2260 = stablehlo.concatenate %2258, %2259, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %2261 = stablehlo.reshape %2260 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %2262:2 = call @_jax_attn_func(%arg449, %2238, %2261, %2170, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %2263 = stablehlo.dot_general %2262#1, %arg62, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %2264 = stablehlo.reshape %2263 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %2265 = stablehlo.reshape %2264 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %2266 = stablehlo.convert %2265 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %2267 = stablehlo.convert %2143 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %2268 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %2269 = stablehlo.multiply %2267, %2268 : tensor<16x2048xf32> loc(#loc1751)
    %2270 = stablehlo.add %2266, %2269 : tensor<16x2048xf32> loc(#loc1752)
    %2271 = stablehlo.convert %2270 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %2272 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %2273 = stablehlo.power %2270, %2272 : tensor<16x2048xf32> loc(#loc1754)
    %2274 = stablehlo.reduce(%2273 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %2275 = stablehlo.broadcast_in_dim %2274, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %2276 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %2277 = stablehlo.divide %2275, %2276 : tensor<16x1xf32> loc(#loc1757)
    %2278 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %2279 = stablehlo.add %2277, %2278 : tensor<16x1xf32> loc(#loc1758)
    %2280 = stablehlo.rsqrt %2279 : tensor<16x1xf32> loc(#loc1678)
    %2281 = stablehlo.broadcast_in_dim %2280, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %2282 = stablehlo.multiply %2270, %2281 : tensor<16x2048xf32> loc(#loc1759)
    %2283 = stablehlo.convert %2282 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %2284 = stablehlo.broadcast_in_dim %arg60, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %2285 = stablehlo.broadcast_in_dim %2284, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %2286 = stablehlo.multiply %2283, %2285 : tensor<16x2048xbf16> loc(#loc1761)
    %2287 = stablehlo.dot_general %2286, %arg59, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %2288 = stablehlo.reshape %2287 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %2289 = stablehlo.reshape %2288 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %2290 = call @jax_fused_moe_func_padded(%2286, %arg57, %arg58, %2289) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %2291 = stablehlo.convert %2290 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %2292 = stablehlo.convert %2271 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %2293 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %2294 = stablehlo.multiply %2292, %2293 : tensor<16x2048xf32> loc(#loc1766)
    %2295 = stablehlo.add %2291, %2294 : tensor<16x2048xf32> loc(#loc1767)
    %2296 = stablehlo.convert %2295 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %2297 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %2298 = stablehlo.power %2295, %2297 : tensor<16x2048xf32> loc(#loc1768)
    %2299 = stablehlo.reduce(%2298 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %2300 = stablehlo.broadcast_in_dim %2299, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %2301 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %2302 = stablehlo.divide %2300, %2301 : tensor<16x1xf32> loc(#loc1771)
    %2303 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %2304 = stablehlo.add %2302, %2303 : tensor<16x1xf32> loc(#loc1772)
    %2305 = stablehlo.rsqrt %2304 : tensor<16x1xf32> loc(#loc1678)
    %2306 = stablehlo.broadcast_in_dim %2305, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %2307 = stablehlo.multiply %2295, %2306 : tensor<16x2048xf32> loc(#loc1773)
    %2308 = stablehlo.convert %2307 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %2309 = stablehlo.broadcast_in_dim %arg65, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %2310 = stablehlo.broadcast_in_dim %2309, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %2311 = stablehlo.multiply %2308, %2310 : tensor<16x2048xbf16> loc(#loc1775)
    %2312 = stablehlo.dot_general %2311, %arg73, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %2313 = stablehlo.reshape %2312 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %2314 = stablehlo.slice %2313 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %2315 = stablehlo.reshape %2314 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %2316 = stablehlo.slice %2313 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %2317 = stablehlo.reshape %2316 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %2318 = stablehlo.slice %2313 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %2319 = stablehlo.reshape %2318 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %2320 = stablehlo.concatenate %2315, %2317, %2319, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %2321 = stablehlo.slice %2320 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %2322 = stablehlo.slice %2320 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %2323 = stablehlo.slice %2320 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %2324 = stablehlo.reshape %2321 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %2325 = stablehlo.convert %2324 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %2326 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %2327 = stablehlo.power %2325, %2326 : tensor<16x32x128xf32> loc(#loc1690)
    %2328 = stablehlo.reduce(%2327 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %2329 = stablehlo.broadcast_in_dim %2328, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %2330 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %2331 = stablehlo.divide %2329, %2330 : tensor<16x32x1xf32> loc(#loc1693)
    %2332 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %2333 = stablehlo.add %2331, %2332 : tensor<16x32x1xf32> loc(#loc1694)
    %2334 = stablehlo.rsqrt %2333 : tensor<16x32x1xf32> loc(#loc1678)
    %2335 = stablehlo.broadcast_in_dim %2334, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %2336 = stablehlo.multiply %2325, %2335 : tensor<16x32x128xf32> loc(#loc1695)
    %2337 = stablehlo.convert %2336 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %2338 = stablehlo.broadcast_in_dim %arg72, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %2339 = stablehlo.broadcast_in_dim %2338, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %2340 = stablehlo.multiply %2337, %2339 : tensor<16x32x128xbf16> loc(#loc1697)
    %2341 = stablehlo.reshape %2340 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %2342 = stablehlo.reshape %2322 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %2343 = stablehlo.convert %2342 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %2344 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %2345 = stablehlo.power %2343, %2344 : tensor<16x4x128xf32> loc(#loc1700)
    %2346 = stablehlo.reduce(%2345 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %2347 = stablehlo.broadcast_in_dim %2346, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %2348 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %2349 = stablehlo.divide %2347, %2348 : tensor<16x4x1xf32> loc(#loc1703)
    %2350 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %2351 = stablehlo.add %2349, %2350 : tensor<16x4x1xf32> loc(#loc1704)
    %2352 = stablehlo.rsqrt %2351 : tensor<16x4x1xf32> loc(#loc1678)
    %2353 = stablehlo.broadcast_in_dim %2352, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %2354 = stablehlo.multiply %2343, %2353 : tensor<16x4x128xf32> loc(#loc1705)
    %2355 = stablehlo.convert %2354 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %2356 = stablehlo.broadcast_in_dim %arg70, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %2357 = stablehlo.broadcast_in_dim %2356, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %2358 = stablehlo.multiply %2355, %2357 : tensor<16x4x128xbf16> loc(#loc1707)
    %2359 = stablehlo.reshape %2358 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %2360 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %2361 = stablehlo.compare  LT, %arg484, %2360,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %2362 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %2363 = stablehlo.add %arg484, %2362 : tensor<16xi32> loc(#loc1710)
    %2364 = stablehlo.select %2361, %2363, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %2365 = stablehlo.broadcast_in_dim %2364, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %2366 = "stablehlo.gather"(%arg10, %2365) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %2367 = stablehlo.slice %2366 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %2368 = stablehlo.slice %2366 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %2369 = stablehlo.reshape %2341 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %2370 = stablehlo.broadcast_in_dim %2367, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %2371 = stablehlo.broadcast_in_dim %2368, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %2372 = stablehlo.slice %2369 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %2373 = stablehlo.slice %2369 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %2374 = stablehlo.broadcast_in_dim %2370, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %2375 = stablehlo.multiply %2372, %2374 : tensor<16x32x64xbf16> loc(#loc1719)
    %2376 = stablehlo.broadcast_in_dim %2371, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %2377 = stablehlo.multiply %2373, %2376 : tensor<16x32x64xbf16> loc(#loc1720)
    %2378 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %2379 = stablehlo.multiply %2377, %2378 : tensor<16x32x64xbf16> loc(#loc1721)
    %2380 = stablehlo.subtract %2375, %2379 : tensor<16x32x64xbf16> loc(#loc1722)
    %2381 = stablehlo.broadcast_in_dim %2370, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %2382 = stablehlo.multiply %2373, %2381 : tensor<16x32x64xbf16> loc(#loc1723)
    %2383 = stablehlo.broadcast_in_dim %2371, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %2384 = stablehlo.multiply %2372, %2383 : tensor<16x32x64xbf16> loc(#loc1724)
    %2385 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %2386 = stablehlo.multiply %2384, %2385 : tensor<16x32x64xbf16> loc(#loc1725)
    %2387 = stablehlo.add %2382, %2386 : tensor<16x32x64xbf16> loc(#loc1726)
    %2388 = stablehlo.concatenate %2380, %2387, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %2389 = stablehlo.slice %2369 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %2390 = stablehlo.concatenate %2388, %2389, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %2391 = stablehlo.reshape %2390 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %2392 = stablehlo.reshape %2359 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %2393 = stablehlo.broadcast_in_dim %2367, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %2394 = stablehlo.broadcast_in_dim %2368, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %2395 = stablehlo.slice %2392 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %2396 = stablehlo.slice %2392 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %2397 = stablehlo.broadcast_in_dim %2393, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %2398 = stablehlo.multiply %2395, %2397 : tensor<16x4x64xbf16> loc(#loc1735)
    %2399 = stablehlo.broadcast_in_dim %2394, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %2400 = stablehlo.multiply %2396, %2399 : tensor<16x4x64xbf16> loc(#loc1736)
    %2401 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %2402 = stablehlo.multiply %2400, %2401 : tensor<16x4x64xbf16> loc(#loc1737)
    %2403 = stablehlo.subtract %2398, %2402 : tensor<16x4x64xbf16> loc(#loc1738)
    %2404 = stablehlo.broadcast_in_dim %2393, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %2405 = stablehlo.multiply %2396, %2404 : tensor<16x4x64xbf16> loc(#loc1739)
    %2406 = stablehlo.broadcast_in_dim %2394, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %2407 = stablehlo.multiply %2395, %2406 : tensor<16x4x64xbf16> loc(#loc1740)
    %2408 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %2409 = stablehlo.multiply %2407, %2408 : tensor<16x4x64xbf16> loc(#loc1741)
    %2410 = stablehlo.add %2405, %2409 : tensor<16x4x64xbf16> loc(#loc1742)
    %2411 = stablehlo.concatenate %2403, %2410, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %2412 = stablehlo.slice %2392 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %2413 = stablehlo.concatenate %2411, %2412, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %2414 = stablehlo.reshape %2413 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %2415:2 = call @_jax_attn_func(%arg450, %2391, %2414, %2323, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %2416 = stablehlo.dot_general %2415#1, %arg71, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %2417 = stablehlo.reshape %2416 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %2418 = stablehlo.reshape %2417 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %2419 = stablehlo.convert %2418 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %2420 = stablehlo.convert %2296 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %2421 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %2422 = stablehlo.multiply %2420, %2421 : tensor<16x2048xf32> loc(#loc1751)
    %2423 = stablehlo.add %2419, %2422 : tensor<16x2048xf32> loc(#loc1752)
    %2424 = stablehlo.convert %2423 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %2425 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %2426 = stablehlo.power %2423, %2425 : tensor<16x2048xf32> loc(#loc1754)
    %2427 = stablehlo.reduce(%2426 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %2428 = stablehlo.broadcast_in_dim %2427, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %2429 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %2430 = stablehlo.divide %2428, %2429 : tensor<16x1xf32> loc(#loc1757)
    %2431 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %2432 = stablehlo.add %2430, %2431 : tensor<16x1xf32> loc(#loc1758)
    %2433 = stablehlo.rsqrt %2432 : tensor<16x1xf32> loc(#loc1678)
    %2434 = stablehlo.broadcast_in_dim %2433, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %2435 = stablehlo.multiply %2423, %2434 : tensor<16x2048xf32> loc(#loc1759)
    %2436 = stablehlo.convert %2435 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %2437 = stablehlo.broadcast_in_dim %arg69, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %2438 = stablehlo.broadcast_in_dim %2437, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %2439 = stablehlo.multiply %2436, %2438 : tensor<16x2048xbf16> loc(#loc1761)
    %2440 = stablehlo.dot_general %2439, %arg68, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %2441 = stablehlo.reshape %2440 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %2442 = stablehlo.reshape %2441 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %2443 = call @jax_fused_moe_func_padded(%2439, %arg66, %arg67, %2442) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %2444 = stablehlo.convert %2443 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %2445 = stablehlo.convert %2424 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %2446 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %2447 = stablehlo.multiply %2445, %2446 : tensor<16x2048xf32> loc(#loc1766)
    %2448 = stablehlo.add %2444, %2447 : tensor<16x2048xf32> loc(#loc1767)
    %2449 = stablehlo.convert %2448 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %2450 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %2451 = stablehlo.power %2448, %2450 : tensor<16x2048xf32> loc(#loc1768)
    %2452 = stablehlo.reduce(%2451 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %2453 = stablehlo.broadcast_in_dim %2452, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %2454 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %2455 = stablehlo.divide %2453, %2454 : tensor<16x1xf32> loc(#loc1771)
    %2456 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %2457 = stablehlo.add %2455, %2456 : tensor<16x1xf32> loc(#loc1772)
    %2458 = stablehlo.rsqrt %2457 : tensor<16x1xf32> loc(#loc1678)
    %2459 = stablehlo.broadcast_in_dim %2458, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %2460 = stablehlo.multiply %2448, %2459 : tensor<16x2048xf32> loc(#loc1773)
    %2461 = stablehlo.convert %2460 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %2462 = stablehlo.broadcast_in_dim %arg74, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %2463 = stablehlo.broadcast_in_dim %2462, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %2464 = stablehlo.multiply %2461, %2463 : tensor<16x2048xbf16> loc(#loc1775)
    %2465 = stablehlo.dot_general %2464, %arg82, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %2466 = stablehlo.reshape %2465 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %2467 = stablehlo.slice %2466 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %2468 = stablehlo.reshape %2467 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %2469 = stablehlo.slice %2466 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %2470 = stablehlo.reshape %2469 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %2471 = stablehlo.slice %2466 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %2472 = stablehlo.reshape %2471 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %2473 = stablehlo.concatenate %2468, %2470, %2472, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %2474 = stablehlo.slice %2473 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %2475 = stablehlo.slice %2473 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %2476 = stablehlo.slice %2473 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %2477 = stablehlo.reshape %2474 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %2478 = stablehlo.convert %2477 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %2479 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %2480 = stablehlo.power %2478, %2479 : tensor<16x32x128xf32> loc(#loc1690)
    %2481 = stablehlo.reduce(%2480 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %2482 = stablehlo.broadcast_in_dim %2481, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %2483 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %2484 = stablehlo.divide %2482, %2483 : tensor<16x32x1xf32> loc(#loc1693)
    %2485 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %2486 = stablehlo.add %2484, %2485 : tensor<16x32x1xf32> loc(#loc1694)
    %2487 = stablehlo.rsqrt %2486 : tensor<16x32x1xf32> loc(#loc1678)
    %2488 = stablehlo.broadcast_in_dim %2487, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %2489 = stablehlo.multiply %2478, %2488 : tensor<16x32x128xf32> loc(#loc1695)
    %2490 = stablehlo.convert %2489 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %2491 = stablehlo.broadcast_in_dim %arg81, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %2492 = stablehlo.broadcast_in_dim %2491, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %2493 = stablehlo.multiply %2490, %2492 : tensor<16x32x128xbf16> loc(#loc1697)
    %2494 = stablehlo.reshape %2493 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %2495 = stablehlo.reshape %2475 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %2496 = stablehlo.convert %2495 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %2497 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %2498 = stablehlo.power %2496, %2497 : tensor<16x4x128xf32> loc(#loc1700)
    %2499 = stablehlo.reduce(%2498 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %2500 = stablehlo.broadcast_in_dim %2499, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %2501 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %2502 = stablehlo.divide %2500, %2501 : tensor<16x4x1xf32> loc(#loc1703)
    %2503 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %2504 = stablehlo.add %2502, %2503 : tensor<16x4x1xf32> loc(#loc1704)
    %2505 = stablehlo.rsqrt %2504 : tensor<16x4x1xf32> loc(#loc1678)
    %2506 = stablehlo.broadcast_in_dim %2505, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %2507 = stablehlo.multiply %2496, %2506 : tensor<16x4x128xf32> loc(#loc1705)
    %2508 = stablehlo.convert %2507 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %2509 = stablehlo.broadcast_in_dim %arg79, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %2510 = stablehlo.broadcast_in_dim %2509, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %2511 = stablehlo.multiply %2508, %2510 : tensor<16x4x128xbf16> loc(#loc1707)
    %2512 = stablehlo.reshape %2511 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %2513 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %2514 = stablehlo.compare  LT, %arg484, %2513,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %2515 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %2516 = stablehlo.add %arg484, %2515 : tensor<16xi32> loc(#loc1710)
    %2517 = stablehlo.select %2514, %2516, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %2518 = stablehlo.broadcast_in_dim %2517, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %2519 = "stablehlo.gather"(%arg10, %2518) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %2520 = stablehlo.slice %2519 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %2521 = stablehlo.slice %2519 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %2522 = stablehlo.reshape %2494 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %2523 = stablehlo.broadcast_in_dim %2520, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %2524 = stablehlo.broadcast_in_dim %2521, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %2525 = stablehlo.slice %2522 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %2526 = stablehlo.slice %2522 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %2527 = stablehlo.broadcast_in_dim %2523, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %2528 = stablehlo.multiply %2525, %2527 : tensor<16x32x64xbf16> loc(#loc1719)
    %2529 = stablehlo.broadcast_in_dim %2524, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %2530 = stablehlo.multiply %2526, %2529 : tensor<16x32x64xbf16> loc(#loc1720)
    %2531 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %2532 = stablehlo.multiply %2530, %2531 : tensor<16x32x64xbf16> loc(#loc1721)
    %2533 = stablehlo.subtract %2528, %2532 : tensor<16x32x64xbf16> loc(#loc1722)
    %2534 = stablehlo.broadcast_in_dim %2523, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %2535 = stablehlo.multiply %2526, %2534 : tensor<16x32x64xbf16> loc(#loc1723)
    %2536 = stablehlo.broadcast_in_dim %2524, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %2537 = stablehlo.multiply %2525, %2536 : tensor<16x32x64xbf16> loc(#loc1724)
    %2538 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %2539 = stablehlo.multiply %2537, %2538 : tensor<16x32x64xbf16> loc(#loc1725)
    %2540 = stablehlo.add %2535, %2539 : tensor<16x32x64xbf16> loc(#loc1726)
    %2541 = stablehlo.concatenate %2533, %2540, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %2542 = stablehlo.slice %2522 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %2543 = stablehlo.concatenate %2541, %2542, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %2544 = stablehlo.reshape %2543 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %2545 = stablehlo.reshape %2512 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %2546 = stablehlo.broadcast_in_dim %2520, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %2547 = stablehlo.broadcast_in_dim %2521, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %2548 = stablehlo.slice %2545 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %2549 = stablehlo.slice %2545 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %2550 = stablehlo.broadcast_in_dim %2546, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %2551 = stablehlo.multiply %2548, %2550 : tensor<16x4x64xbf16> loc(#loc1735)
    %2552 = stablehlo.broadcast_in_dim %2547, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %2553 = stablehlo.multiply %2549, %2552 : tensor<16x4x64xbf16> loc(#loc1736)
    %2554 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %2555 = stablehlo.multiply %2553, %2554 : tensor<16x4x64xbf16> loc(#loc1737)
    %2556 = stablehlo.subtract %2551, %2555 : tensor<16x4x64xbf16> loc(#loc1738)
    %2557 = stablehlo.broadcast_in_dim %2546, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %2558 = stablehlo.multiply %2549, %2557 : tensor<16x4x64xbf16> loc(#loc1739)
    %2559 = stablehlo.broadcast_in_dim %2547, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %2560 = stablehlo.multiply %2548, %2559 : tensor<16x4x64xbf16> loc(#loc1740)
    %2561 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %2562 = stablehlo.multiply %2560, %2561 : tensor<16x4x64xbf16> loc(#loc1741)
    %2563 = stablehlo.add %2558, %2562 : tensor<16x4x64xbf16> loc(#loc1742)
    %2564 = stablehlo.concatenate %2556, %2563, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %2565 = stablehlo.slice %2545 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %2566 = stablehlo.concatenate %2564, %2565, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %2567 = stablehlo.reshape %2566 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %2568:2 = call @_jax_attn_func(%arg451, %2544, %2567, %2476, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %2569 = stablehlo.dot_general %2568#1, %arg80, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %2570 = stablehlo.reshape %2569 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %2571 = stablehlo.reshape %2570 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %2572 = stablehlo.convert %2571 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %2573 = stablehlo.convert %2449 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %2574 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %2575 = stablehlo.multiply %2573, %2574 : tensor<16x2048xf32> loc(#loc1751)
    %2576 = stablehlo.add %2572, %2575 : tensor<16x2048xf32> loc(#loc1752)
    %2577 = stablehlo.convert %2576 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %2578 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %2579 = stablehlo.power %2576, %2578 : tensor<16x2048xf32> loc(#loc1754)
    %2580 = stablehlo.reduce(%2579 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %2581 = stablehlo.broadcast_in_dim %2580, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %2582 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %2583 = stablehlo.divide %2581, %2582 : tensor<16x1xf32> loc(#loc1757)
    %2584 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %2585 = stablehlo.add %2583, %2584 : tensor<16x1xf32> loc(#loc1758)
    %2586 = stablehlo.rsqrt %2585 : tensor<16x1xf32> loc(#loc1678)
    %2587 = stablehlo.broadcast_in_dim %2586, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %2588 = stablehlo.multiply %2576, %2587 : tensor<16x2048xf32> loc(#loc1759)
    %2589 = stablehlo.convert %2588 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %2590 = stablehlo.broadcast_in_dim %arg78, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %2591 = stablehlo.broadcast_in_dim %2590, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %2592 = stablehlo.multiply %2589, %2591 : tensor<16x2048xbf16> loc(#loc1761)
    %2593 = stablehlo.dot_general %2592, %arg77, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %2594 = stablehlo.reshape %2593 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %2595 = stablehlo.reshape %2594 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %2596 = call @jax_fused_moe_func_padded(%2592, %arg75, %arg76, %2595) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %2597 = stablehlo.convert %2596 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %2598 = stablehlo.convert %2577 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %2599 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %2600 = stablehlo.multiply %2598, %2599 : tensor<16x2048xf32> loc(#loc1766)
    %2601 = stablehlo.add %2597, %2600 : tensor<16x2048xf32> loc(#loc1767)
    %2602 = stablehlo.convert %2601 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %2603 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %2604 = stablehlo.power %2601, %2603 : tensor<16x2048xf32> loc(#loc1768)
    %2605 = stablehlo.reduce(%2604 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %2606 = stablehlo.broadcast_in_dim %2605, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %2607 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %2608 = stablehlo.divide %2606, %2607 : tensor<16x1xf32> loc(#loc1771)
    %2609 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %2610 = stablehlo.add %2608, %2609 : tensor<16x1xf32> loc(#loc1772)
    %2611 = stablehlo.rsqrt %2610 : tensor<16x1xf32> loc(#loc1678)
    %2612 = stablehlo.broadcast_in_dim %2611, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %2613 = stablehlo.multiply %2601, %2612 : tensor<16x2048xf32> loc(#loc1773)
    %2614 = stablehlo.convert %2613 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %2615 = stablehlo.broadcast_in_dim %arg83, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %2616 = stablehlo.broadcast_in_dim %2615, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %2617 = stablehlo.multiply %2614, %2616 : tensor<16x2048xbf16> loc(#loc1775)
    %2618 = stablehlo.dot_general %2617, %arg91, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %2619 = stablehlo.reshape %2618 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %2620 = stablehlo.slice %2619 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %2621 = stablehlo.reshape %2620 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %2622 = stablehlo.slice %2619 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %2623 = stablehlo.reshape %2622 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %2624 = stablehlo.slice %2619 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %2625 = stablehlo.reshape %2624 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %2626 = stablehlo.concatenate %2621, %2623, %2625, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %2627 = stablehlo.slice %2626 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %2628 = stablehlo.slice %2626 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %2629 = stablehlo.slice %2626 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %2630 = stablehlo.reshape %2627 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %2631 = stablehlo.convert %2630 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %2632 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %2633 = stablehlo.power %2631, %2632 : tensor<16x32x128xf32> loc(#loc1690)
    %2634 = stablehlo.reduce(%2633 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %2635 = stablehlo.broadcast_in_dim %2634, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %2636 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %2637 = stablehlo.divide %2635, %2636 : tensor<16x32x1xf32> loc(#loc1693)
    %2638 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %2639 = stablehlo.add %2637, %2638 : tensor<16x32x1xf32> loc(#loc1694)
    %2640 = stablehlo.rsqrt %2639 : tensor<16x32x1xf32> loc(#loc1678)
    %2641 = stablehlo.broadcast_in_dim %2640, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %2642 = stablehlo.multiply %2631, %2641 : tensor<16x32x128xf32> loc(#loc1695)
    %2643 = stablehlo.convert %2642 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %2644 = stablehlo.broadcast_in_dim %arg90, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %2645 = stablehlo.broadcast_in_dim %2644, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %2646 = stablehlo.multiply %2643, %2645 : tensor<16x32x128xbf16> loc(#loc1697)
    %2647 = stablehlo.reshape %2646 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %2648 = stablehlo.reshape %2628 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %2649 = stablehlo.convert %2648 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %2650 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %2651 = stablehlo.power %2649, %2650 : tensor<16x4x128xf32> loc(#loc1700)
    %2652 = stablehlo.reduce(%2651 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %2653 = stablehlo.broadcast_in_dim %2652, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %2654 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %2655 = stablehlo.divide %2653, %2654 : tensor<16x4x1xf32> loc(#loc1703)
    %2656 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %2657 = stablehlo.add %2655, %2656 : tensor<16x4x1xf32> loc(#loc1704)
    %2658 = stablehlo.rsqrt %2657 : tensor<16x4x1xf32> loc(#loc1678)
    %2659 = stablehlo.broadcast_in_dim %2658, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %2660 = stablehlo.multiply %2649, %2659 : tensor<16x4x128xf32> loc(#loc1705)
    %2661 = stablehlo.convert %2660 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %2662 = stablehlo.broadcast_in_dim %arg88, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %2663 = stablehlo.broadcast_in_dim %2662, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %2664 = stablehlo.multiply %2661, %2663 : tensor<16x4x128xbf16> loc(#loc1707)
    %2665 = stablehlo.reshape %2664 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %2666 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %2667 = stablehlo.compare  LT, %arg484, %2666,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %2668 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %2669 = stablehlo.add %arg484, %2668 : tensor<16xi32> loc(#loc1710)
    %2670 = stablehlo.select %2667, %2669, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %2671 = stablehlo.broadcast_in_dim %2670, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %2672 = "stablehlo.gather"(%arg10, %2671) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %2673 = stablehlo.slice %2672 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %2674 = stablehlo.slice %2672 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %2675 = stablehlo.reshape %2647 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %2676 = stablehlo.broadcast_in_dim %2673, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %2677 = stablehlo.broadcast_in_dim %2674, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %2678 = stablehlo.slice %2675 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %2679 = stablehlo.slice %2675 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %2680 = stablehlo.broadcast_in_dim %2676, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %2681 = stablehlo.multiply %2678, %2680 : tensor<16x32x64xbf16> loc(#loc1719)
    %2682 = stablehlo.broadcast_in_dim %2677, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %2683 = stablehlo.multiply %2679, %2682 : tensor<16x32x64xbf16> loc(#loc1720)
    %2684 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %2685 = stablehlo.multiply %2683, %2684 : tensor<16x32x64xbf16> loc(#loc1721)
    %2686 = stablehlo.subtract %2681, %2685 : tensor<16x32x64xbf16> loc(#loc1722)
    %2687 = stablehlo.broadcast_in_dim %2676, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %2688 = stablehlo.multiply %2679, %2687 : tensor<16x32x64xbf16> loc(#loc1723)
    %2689 = stablehlo.broadcast_in_dim %2677, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %2690 = stablehlo.multiply %2678, %2689 : tensor<16x32x64xbf16> loc(#loc1724)
    %2691 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %2692 = stablehlo.multiply %2690, %2691 : tensor<16x32x64xbf16> loc(#loc1725)
    %2693 = stablehlo.add %2688, %2692 : tensor<16x32x64xbf16> loc(#loc1726)
    %2694 = stablehlo.concatenate %2686, %2693, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %2695 = stablehlo.slice %2675 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %2696 = stablehlo.concatenate %2694, %2695, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %2697 = stablehlo.reshape %2696 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %2698 = stablehlo.reshape %2665 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %2699 = stablehlo.broadcast_in_dim %2673, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %2700 = stablehlo.broadcast_in_dim %2674, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %2701 = stablehlo.slice %2698 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %2702 = stablehlo.slice %2698 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %2703 = stablehlo.broadcast_in_dim %2699, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %2704 = stablehlo.multiply %2701, %2703 : tensor<16x4x64xbf16> loc(#loc1735)
    %2705 = stablehlo.broadcast_in_dim %2700, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %2706 = stablehlo.multiply %2702, %2705 : tensor<16x4x64xbf16> loc(#loc1736)
    %2707 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %2708 = stablehlo.multiply %2706, %2707 : tensor<16x4x64xbf16> loc(#loc1737)
    %2709 = stablehlo.subtract %2704, %2708 : tensor<16x4x64xbf16> loc(#loc1738)
    %2710 = stablehlo.broadcast_in_dim %2699, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %2711 = stablehlo.multiply %2702, %2710 : tensor<16x4x64xbf16> loc(#loc1739)
    %2712 = stablehlo.broadcast_in_dim %2700, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %2713 = stablehlo.multiply %2701, %2712 : tensor<16x4x64xbf16> loc(#loc1740)
    %2714 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %2715 = stablehlo.multiply %2713, %2714 : tensor<16x4x64xbf16> loc(#loc1741)
    %2716 = stablehlo.add %2711, %2715 : tensor<16x4x64xbf16> loc(#loc1742)
    %2717 = stablehlo.concatenate %2709, %2716, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %2718 = stablehlo.slice %2698 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %2719 = stablehlo.concatenate %2717, %2718, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %2720 = stablehlo.reshape %2719 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %2721:2 = call @_jax_attn_func(%arg452, %2697, %2720, %2629, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %2722 = stablehlo.dot_general %2721#1, %arg89, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %2723 = stablehlo.reshape %2722 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %2724 = stablehlo.reshape %2723 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %2725 = stablehlo.convert %2724 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %2726 = stablehlo.convert %2602 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %2727 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %2728 = stablehlo.multiply %2726, %2727 : tensor<16x2048xf32> loc(#loc1751)
    %2729 = stablehlo.add %2725, %2728 : tensor<16x2048xf32> loc(#loc1752)
    %2730 = stablehlo.convert %2729 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %2731 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %2732 = stablehlo.power %2729, %2731 : tensor<16x2048xf32> loc(#loc1754)
    %2733 = stablehlo.reduce(%2732 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %2734 = stablehlo.broadcast_in_dim %2733, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %2735 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %2736 = stablehlo.divide %2734, %2735 : tensor<16x1xf32> loc(#loc1757)
    %2737 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %2738 = stablehlo.add %2736, %2737 : tensor<16x1xf32> loc(#loc1758)
    %2739 = stablehlo.rsqrt %2738 : tensor<16x1xf32> loc(#loc1678)
    %2740 = stablehlo.broadcast_in_dim %2739, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %2741 = stablehlo.multiply %2729, %2740 : tensor<16x2048xf32> loc(#loc1759)
    %2742 = stablehlo.convert %2741 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %2743 = stablehlo.broadcast_in_dim %arg87, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %2744 = stablehlo.broadcast_in_dim %2743, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %2745 = stablehlo.multiply %2742, %2744 : tensor<16x2048xbf16> loc(#loc1761)
    %2746 = stablehlo.dot_general %2745, %arg86, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %2747 = stablehlo.reshape %2746 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %2748 = stablehlo.reshape %2747 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %2749 = call @jax_fused_moe_func_padded(%2745, %arg84, %arg85, %2748) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %2750 = stablehlo.convert %2749 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %2751 = stablehlo.convert %2730 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %2752 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %2753 = stablehlo.multiply %2751, %2752 : tensor<16x2048xf32> loc(#loc1766)
    %2754 = stablehlo.add %2750, %2753 : tensor<16x2048xf32> loc(#loc1767)
    %2755 = stablehlo.convert %2754 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %2756 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %2757 = stablehlo.power %2754, %2756 : tensor<16x2048xf32> loc(#loc1768)
    %2758 = stablehlo.reduce(%2757 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %2759 = stablehlo.broadcast_in_dim %2758, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %2760 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %2761 = stablehlo.divide %2759, %2760 : tensor<16x1xf32> loc(#loc1771)
    %2762 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %2763 = stablehlo.add %2761, %2762 : tensor<16x1xf32> loc(#loc1772)
    %2764 = stablehlo.rsqrt %2763 : tensor<16x1xf32> loc(#loc1678)
    %2765 = stablehlo.broadcast_in_dim %2764, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %2766 = stablehlo.multiply %2754, %2765 : tensor<16x2048xf32> loc(#loc1773)
    %2767 = stablehlo.convert %2766 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %2768 = stablehlo.broadcast_in_dim %arg92, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %2769 = stablehlo.broadcast_in_dim %2768, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %2770 = stablehlo.multiply %2767, %2769 : tensor<16x2048xbf16> loc(#loc1775)
    %2771 = stablehlo.dot_general %2770, %arg100, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %2772 = stablehlo.reshape %2771 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %2773 = stablehlo.slice %2772 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %2774 = stablehlo.reshape %2773 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %2775 = stablehlo.slice %2772 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %2776 = stablehlo.reshape %2775 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %2777 = stablehlo.slice %2772 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %2778 = stablehlo.reshape %2777 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %2779 = stablehlo.concatenate %2774, %2776, %2778, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %2780 = stablehlo.slice %2779 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %2781 = stablehlo.slice %2779 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %2782 = stablehlo.slice %2779 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %2783 = stablehlo.reshape %2780 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %2784 = stablehlo.convert %2783 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %2785 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %2786 = stablehlo.power %2784, %2785 : tensor<16x32x128xf32> loc(#loc1690)
    %2787 = stablehlo.reduce(%2786 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %2788 = stablehlo.broadcast_in_dim %2787, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %2789 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %2790 = stablehlo.divide %2788, %2789 : tensor<16x32x1xf32> loc(#loc1693)
    %2791 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %2792 = stablehlo.add %2790, %2791 : tensor<16x32x1xf32> loc(#loc1694)
    %2793 = stablehlo.rsqrt %2792 : tensor<16x32x1xf32> loc(#loc1678)
    %2794 = stablehlo.broadcast_in_dim %2793, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %2795 = stablehlo.multiply %2784, %2794 : tensor<16x32x128xf32> loc(#loc1695)
    %2796 = stablehlo.convert %2795 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %2797 = stablehlo.broadcast_in_dim %arg99, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %2798 = stablehlo.broadcast_in_dim %2797, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %2799 = stablehlo.multiply %2796, %2798 : tensor<16x32x128xbf16> loc(#loc1697)
    %2800 = stablehlo.reshape %2799 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %2801 = stablehlo.reshape %2781 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %2802 = stablehlo.convert %2801 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %2803 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %2804 = stablehlo.power %2802, %2803 : tensor<16x4x128xf32> loc(#loc1700)
    %2805 = stablehlo.reduce(%2804 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %2806 = stablehlo.broadcast_in_dim %2805, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %2807 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %2808 = stablehlo.divide %2806, %2807 : tensor<16x4x1xf32> loc(#loc1703)
    %2809 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %2810 = stablehlo.add %2808, %2809 : tensor<16x4x1xf32> loc(#loc1704)
    %2811 = stablehlo.rsqrt %2810 : tensor<16x4x1xf32> loc(#loc1678)
    %2812 = stablehlo.broadcast_in_dim %2811, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %2813 = stablehlo.multiply %2802, %2812 : tensor<16x4x128xf32> loc(#loc1705)
    %2814 = stablehlo.convert %2813 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %2815 = stablehlo.broadcast_in_dim %arg97, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %2816 = stablehlo.broadcast_in_dim %2815, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %2817 = stablehlo.multiply %2814, %2816 : tensor<16x4x128xbf16> loc(#loc1707)
    %2818 = stablehlo.reshape %2817 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %2819 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %2820 = stablehlo.compare  LT, %arg484, %2819,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %2821 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %2822 = stablehlo.add %arg484, %2821 : tensor<16xi32> loc(#loc1710)
    %2823 = stablehlo.select %2820, %2822, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %2824 = stablehlo.broadcast_in_dim %2823, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %2825 = "stablehlo.gather"(%arg10, %2824) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %2826 = stablehlo.slice %2825 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %2827 = stablehlo.slice %2825 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %2828 = stablehlo.reshape %2800 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %2829 = stablehlo.broadcast_in_dim %2826, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %2830 = stablehlo.broadcast_in_dim %2827, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %2831 = stablehlo.slice %2828 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %2832 = stablehlo.slice %2828 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %2833 = stablehlo.broadcast_in_dim %2829, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %2834 = stablehlo.multiply %2831, %2833 : tensor<16x32x64xbf16> loc(#loc1719)
    %2835 = stablehlo.broadcast_in_dim %2830, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %2836 = stablehlo.multiply %2832, %2835 : tensor<16x32x64xbf16> loc(#loc1720)
    %2837 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %2838 = stablehlo.multiply %2836, %2837 : tensor<16x32x64xbf16> loc(#loc1721)
    %2839 = stablehlo.subtract %2834, %2838 : tensor<16x32x64xbf16> loc(#loc1722)
    %2840 = stablehlo.broadcast_in_dim %2829, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %2841 = stablehlo.multiply %2832, %2840 : tensor<16x32x64xbf16> loc(#loc1723)
    %2842 = stablehlo.broadcast_in_dim %2830, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %2843 = stablehlo.multiply %2831, %2842 : tensor<16x32x64xbf16> loc(#loc1724)
    %2844 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %2845 = stablehlo.multiply %2843, %2844 : tensor<16x32x64xbf16> loc(#loc1725)
    %2846 = stablehlo.add %2841, %2845 : tensor<16x32x64xbf16> loc(#loc1726)
    %2847 = stablehlo.concatenate %2839, %2846, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %2848 = stablehlo.slice %2828 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %2849 = stablehlo.concatenate %2847, %2848, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %2850 = stablehlo.reshape %2849 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %2851 = stablehlo.reshape %2818 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %2852 = stablehlo.broadcast_in_dim %2826, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %2853 = stablehlo.broadcast_in_dim %2827, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %2854 = stablehlo.slice %2851 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %2855 = stablehlo.slice %2851 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %2856 = stablehlo.broadcast_in_dim %2852, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %2857 = stablehlo.multiply %2854, %2856 : tensor<16x4x64xbf16> loc(#loc1735)
    %2858 = stablehlo.broadcast_in_dim %2853, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %2859 = stablehlo.multiply %2855, %2858 : tensor<16x4x64xbf16> loc(#loc1736)
    %2860 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %2861 = stablehlo.multiply %2859, %2860 : tensor<16x4x64xbf16> loc(#loc1737)
    %2862 = stablehlo.subtract %2857, %2861 : tensor<16x4x64xbf16> loc(#loc1738)
    %2863 = stablehlo.broadcast_in_dim %2852, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %2864 = stablehlo.multiply %2855, %2863 : tensor<16x4x64xbf16> loc(#loc1739)
    %2865 = stablehlo.broadcast_in_dim %2853, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %2866 = stablehlo.multiply %2854, %2865 : tensor<16x4x64xbf16> loc(#loc1740)
    %2867 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %2868 = stablehlo.multiply %2866, %2867 : tensor<16x4x64xbf16> loc(#loc1741)
    %2869 = stablehlo.add %2864, %2868 : tensor<16x4x64xbf16> loc(#loc1742)
    %2870 = stablehlo.concatenate %2862, %2869, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %2871 = stablehlo.slice %2851 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %2872 = stablehlo.concatenate %2870, %2871, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %2873 = stablehlo.reshape %2872 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %2874:2 = call @_jax_attn_func(%arg453, %2850, %2873, %2782, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %2875 = stablehlo.dot_general %2874#1, %arg98, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %2876 = stablehlo.reshape %2875 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %2877 = stablehlo.reshape %2876 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %2878 = stablehlo.convert %2877 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %2879 = stablehlo.convert %2755 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %2880 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %2881 = stablehlo.multiply %2879, %2880 : tensor<16x2048xf32> loc(#loc1751)
    %2882 = stablehlo.add %2878, %2881 : tensor<16x2048xf32> loc(#loc1752)
    %2883 = stablehlo.convert %2882 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %2884 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %2885 = stablehlo.power %2882, %2884 : tensor<16x2048xf32> loc(#loc1754)
    %2886 = stablehlo.reduce(%2885 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %2887 = stablehlo.broadcast_in_dim %2886, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %2888 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %2889 = stablehlo.divide %2887, %2888 : tensor<16x1xf32> loc(#loc1757)
    %2890 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %2891 = stablehlo.add %2889, %2890 : tensor<16x1xf32> loc(#loc1758)
    %2892 = stablehlo.rsqrt %2891 : tensor<16x1xf32> loc(#loc1678)
    %2893 = stablehlo.broadcast_in_dim %2892, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %2894 = stablehlo.multiply %2882, %2893 : tensor<16x2048xf32> loc(#loc1759)
    %2895 = stablehlo.convert %2894 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %2896 = stablehlo.broadcast_in_dim %arg96, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %2897 = stablehlo.broadcast_in_dim %2896, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %2898 = stablehlo.multiply %2895, %2897 : tensor<16x2048xbf16> loc(#loc1761)
    %2899 = stablehlo.dot_general %2898, %arg95, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %2900 = stablehlo.reshape %2899 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %2901 = stablehlo.reshape %2900 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %2902 = call @jax_fused_moe_func_padded(%2898, %arg93, %arg94, %2901) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %2903 = stablehlo.convert %2902 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %2904 = stablehlo.convert %2883 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %2905 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %2906 = stablehlo.multiply %2904, %2905 : tensor<16x2048xf32> loc(#loc1766)
    %2907 = stablehlo.add %2903, %2906 : tensor<16x2048xf32> loc(#loc1767)
    %2908 = stablehlo.convert %2907 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %2909 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %2910 = stablehlo.power %2907, %2909 : tensor<16x2048xf32> loc(#loc1768)
    %2911 = stablehlo.reduce(%2910 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %2912 = stablehlo.broadcast_in_dim %2911, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %2913 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %2914 = stablehlo.divide %2912, %2913 : tensor<16x1xf32> loc(#loc1771)
    %2915 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %2916 = stablehlo.add %2914, %2915 : tensor<16x1xf32> loc(#loc1772)
    %2917 = stablehlo.rsqrt %2916 : tensor<16x1xf32> loc(#loc1678)
    %2918 = stablehlo.broadcast_in_dim %2917, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %2919 = stablehlo.multiply %2907, %2918 : tensor<16x2048xf32> loc(#loc1773)
    %2920 = stablehlo.convert %2919 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %2921 = stablehlo.broadcast_in_dim %arg101, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %2922 = stablehlo.broadcast_in_dim %2921, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %2923 = stablehlo.multiply %2920, %2922 : tensor<16x2048xbf16> loc(#loc1775)
    %2924 = stablehlo.dot_general %2923, %arg109, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %2925 = stablehlo.reshape %2924 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %2926 = stablehlo.slice %2925 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %2927 = stablehlo.reshape %2926 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %2928 = stablehlo.slice %2925 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %2929 = stablehlo.reshape %2928 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %2930 = stablehlo.slice %2925 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %2931 = stablehlo.reshape %2930 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %2932 = stablehlo.concatenate %2927, %2929, %2931, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %2933 = stablehlo.slice %2932 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %2934 = stablehlo.slice %2932 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %2935 = stablehlo.slice %2932 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %2936 = stablehlo.reshape %2933 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %2937 = stablehlo.convert %2936 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %2938 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %2939 = stablehlo.power %2937, %2938 : tensor<16x32x128xf32> loc(#loc1690)
    %2940 = stablehlo.reduce(%2939 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %2941 = stablehlo.broadcast_in_dim %2940, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %2942 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %2943 = stablehlo.divide %2941, %2942 : tensor<16x32x1xf32> loc(#loc1693)
    %2944 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %2945 = stablehlo.add %2943, %2944 : tensor<16x32x1xf32> loc(#loc1694)
    %2946 = stablehlo.rsqrt %2945 : tensor<16x32x1xf32> loc(#loc1678)
    %2947 = stablehlo.broadcast_in_dim %2946, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %2948 = stablehlo.multiply %2937, %2947 : tensor<16x32x128xf32> loc(#loc1695)
    %2949 = stablehlo.convert %2948 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %2950 = stablehlo.broadcast_in_dim %arg108, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %2951 = stablehlo.broadcast_in_dim %2950, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %2952 = stablehlo.multiply %2949, %2951 : tensor<16x32x128xbf16> loc(#loc1697)
    %2953 = stablehlo.reshape %2952 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %2954 = stablehlo.reshape %2934 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %2955 = stablehlo.convert %2954 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %2956 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %2957 = stablehlo.power %2955, %2956 : tensor<16x4x128xf32> loc(#loc1700)
    %2958 = stablehlo.reduce(%2957 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %2959 = stablehlo.broadcast_in_dim %2958, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %2960 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %2961 = stablehlo.divide %2959, %2960 : tensor<16x4x1xf32> loc(#loc1703)
    %2962 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %2963 = stablehlo.add %2961, %2962 : tensor<16x4x1xf32> loc(#loc1704)
    %2964 = stablehlo.rsqrt %2963 : tensor<16x4x1xf32> loc(#loc1678)
    %2965 = stablehlo.broadcast_in_dim %2964, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %2966 = stablehlo.multiply %2955, %2965 : tensor<16x4x128xf32> loc(#loc1705)
    %2967 = stablehlo.convert %2966 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %2968 = stablehlo.broadcast_in_dim %arg106, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %2969 = stablehlo.broadcast_in_dim %2968, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %2970 = stablehlo.multiply %2967, %2969 : tensor<16x4x128xbf16> loc(#loc1707)
    %2971 = stablehlo.reshape %2970 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %2972 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %2973 = stablehlo.compare  LT, %arg484, %2972,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %2974 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %2975 = stablehlo.add %arg484, %2974 : tensor<16xi32> loc(#loc1710)
    %2976 = stablehlo.select %2973, %2975, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %2977 = stablehlo.broadcast_in_dim %2976, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %2978 = "stablehlo.gather"(%arg10, %2977) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %2979 = stablehlo.slice %2978 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %2980 = stablehlo.slice %2978 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %2981 = stablehlo.reshape %2953 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %2982 = stablehlo.broadcast_in_dim %2979, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %2983 = stablehlo.broadcast_in_dim %2980, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %2984 = stablehlo.slice %2981 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %2985 = stablehlo.slice %2981 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %2986 = stablehlo.broadcast_in_dim %2982, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %2987 = stablehlo.multiply %2984, %2986 : tensor<16x32x64xbf16> loc(#loc1719)
    %2988 = stablehlo.broadcast_in_dim %2983, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %2989 = stablehlo.multiply %2985, %2988 : tensor<16x32x64xbf16> loc(#loc1720)
    %2990 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %2991 = stablehlo.multiply %2989, %2990 : tensor<16x32x64xbf16> loc(#loc1721)
    %2992 = stablehlo.subtract %2987, %2991 : tensor<16x32x64xbf16> loc(#loc1722)
    %2993 = stablehlo.broadcast_in_dim %2982, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %2994 = stablehlo.multiply %2985, %2993 : tensor<16x32x64xbf16> loc(#loc1723)
    %2995 = stablehlo.broadcast_in_dim %2983, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %2996 = stablehlo.multiply %2984, %2995 : tensor<16x32x64xbf16> loc(#loc1724)
    %2997 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %2998 = stablehlo.multiply %2996, %2997 : tensor<16x32x64xbf16> loc(#loc1725)
    %2999 = stablehlo.add %2994, %2998 : tensor<16x32x64xbf16> loc(#loc1726)
    %3000 = stablehlo.concatenate %2992, %2999, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %3001 = stablehlo.slice %2981 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %3002 = stablehlo.concatenate %3000, %3001, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %3003 = stablehlo.reshape %3002 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %3004 = stablehlo.reshape %2971 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %3005 = stablehlo.broadcast_in_dim %2979, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %3006 = stablehlo.broadcast_in_dim %2980, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %3007 = stablehlo.slice %3004 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %3008 = stablehlo.slice %3004 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %3009 = stablehlo.broadcast_in_dim %3005, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %3010 = stablehlo.multiply %3007, %3009 : tensor<16x4x64xbf16> loc(#loc1735)
    %3011 = stablehlo.broadcast_in_dim %3006, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %3012 = stablehlo.multiply %3008, %3011 : tensor<16x4x64xbf16> loc(#loc1736)
    %3013 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %3014 = stablehlo.multiply %3012, %3013 : tensor<16x4x64xbf16> loc(#loc1737)
    %3015 = stablehlo.subtract %3010, %3014 : tensor<16x4x64xbf16> loc(#loc1738)
    %3016 = stablehlo.broadcast_in_dim %3005, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %3017 = stablehlo.multiply %3008, %3016 : tensor<16x4x64xbf16> loc(#loc1739)
    %3018 = stablehlo.broadcast_in_dim %3006, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %3019 = stablehlo.multiply %3007, %3018 : tensor<16x4x64xbf16> loc(#loc1740)
    %3020 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %3021 = stablehlo.multiply %3019, %3020 : tensor<16x4x64xbf16> loc(#loc1741)
    %3022 = stablehlo.add %3017, %3021 : tensor<16x4x64xbf16> loc(#loc1742)
    %3023 = stablehlo.concatenate %3015, %3022, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %3024 = stablehlo.slice %3004 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %3025 = stablehlo.concatenate %3023, %3024, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %3026 = stablehlo.reshape %3025 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %3027:2 = call @_jax_attn_func(%arg454, %3003, %3026, %2935, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %3028 = stablehlo.dot_general %3027#1, %arg107, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %3029 = stablehlo.reshape %3028 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %3030 = stablehlo.reshape %3029 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %3031 = stablehlo.convert %3030 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %3032 = stablehlo.convert %2908 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %3033 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %3034 = stablehlo.multiply %3032, %3033 : tensor<16x2048xf32> loc(#loc1751)
    %3035 = stablehlo.add %3031, %3034 : tensor<16x2048xf32> loc(#loc1752)
    %3036 = stablehlo.convert %3035 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %3037 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %3038 = stablehlo.power %3035, %3037 : tensor<16x2048xf32> loc(#loc1754)
    %3039 = stablehlo.reduce(%3038 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %3040 = stablehlo.broadcast_in_dim %3039, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %3041 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %3042 = stablehlo.divide %3040, %3041 : tensor<16x1xf32> loc(#loc1757)
    %3043 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %3044 = stablehlo.add %3042, %3043 : tensor<16x1xf32> loc(#loc1758)
    %3045 = stablehlo.rsqrt %3044 : tensor<16x1xf32> loc(#loc1678)
    %3046 = stablehlo.broadcast_in_dim %3045, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %3047 = stablehlo.multiply %3035, %3046 : tensor<16x2048xf32> loc(#loc1759)
    %3048 = stablehlo.convert %3047 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %3049 = stablehlo.broadcast_in_dim %arg105, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %3050 = stablehlo.broadcast_in_dim %3049, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %3051 = stablehlo.multiply %3048, %3050 : tensor<16x2048xbf16> loc(#loc1761)
    %3052 = stablehlo.dot_general %3051, %arg104, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %3053 = stablehlo.reshape %3052 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %3054 = stablehlo.reshape %3053 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %3055 = call @jax_fused_moe_func_padded(%3051, %arg102, %arg103, %3054) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %3056 = stablehlo.convert %3055 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %3057 = stablehlo.convert %3036 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %3058 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %3059 = stablehlo.multiply %3057, %3058 : tensor<16x2048xf32> loc(#loc1766)
    %3060 = stablehlo.add %3056, %3059 : tensor<16x2048xf32> loc(#loc1767)
    %3061 = stablehlo.convert %3060 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %3062 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %3063 = stablehlo.power %3060, %3062 : tensor<16x2048xf32> loc(#loc1768)
    %3064 = stablehlo.reduce(%3063 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %3065 = stablehlo.broadcast_in_dim %3064, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %3066 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %3067 = stablehlo.divide %3065, %3066 : tensor<16x1xf32> loc(#loc1771)
    %3068 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %3069 = stablehlo.add %3067, %3068 : tensor<16x1xf32> loc(#loc1772)
    %3070 = stablehlo.rsqrt %3069 : tensor<16x1xf32> loc(#loc1678)
    %3071 = stablehlo.broadcast_in_dim %3070, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %3072 = stablehlo.multiply %3060, %3071 : tensor<16x2048xf32> loc(#loc1773)
    %3073 = stablehlo.convert %3072 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %3074 = stablehlo.broadcast_in_dim %arg119, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %3075 = stablehlo.broadcast_in_dim %3074, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %3076 = stablehlo.multiply %3073, %3075 : tensor<16x2048xbf16> loc(#loc1775)
    %3077 = stablehlo.dot_general %3076, %arg127, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %3078 = stablehlo.reshape %3077 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %3079 = stablehlo.slice %3078 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %3080 = stablehlo.reshape %3079 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %3081 = stablehlo.slice %3078 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %3082 = stablehlo.reshape %3081 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %3083 = stablehlo.slice %3078 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %3084 = stablehlo.reshape %3083 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %3085 = stablehlo.concatenate %3080, %3082, %3084, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %3086 = stablehlo.slice %3085 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %3087 = stablehlo.slice %3085 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %3088 = stablehlo.slice %3085 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %3089 = stablehlo.reshape %3086 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %3090 = stablehlo.convert %3089 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %3091 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %3092 = stablehlo.power %3090, %3091 : tensor<16x32x128xf32> loc(#loc1690)
    %3093 = stablehlo.reduce(%3092 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %3094 = stablehlo.broadcast_in_dim %3093, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %3095 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %3096 = stablehlo.divide %3094, %3095 : tensor<16x32x1xf32> loc(#loc1693)
    %3097 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %3098 = stablehlo.add %3096, %3097 : tensor<16x32x1xf32> loc(#loc1694)
    %3099 = stablehlo.rsqrt %3098 : tensor<16x32x1xf32> loc(#loc1678)
    %3100 = stablehlo.broadcast_in_dim %3099, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %3101 = stablehlo.multiply %3090, %3100 : tensor<16x32x128xf32> loc(#loc1695)
    %3102 = stablehlo.convert %3101 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %3103 = stablehlo.broadcast_in_dim %arg126, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %3104 = stablehlo.broadcast_in_dim %3103, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %3105 = stablehlo.multiply %3102, %3104 : tensor<16x32x128xbf16> loc(#loc1697)
    %3106 = stablehlo.reshape %3105 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %3107 = stablehlo.reshape %3087 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %3108 = stablehlo.convert %3107 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %3109 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %3110 = stablehlo.power %3108, %3109 : tensor<16x4x128xf32> loc(#loc1700)
    %3111 = stablehlo.reduce(%3110 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %3112 = stablehlo.broadcast_in_dim %3111, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %3113 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %3114 = stablehlo.divide %3112, %3113 : tensor<16x4x1xf32> loc(#loc1703)
    %3115 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %3116 = stablehlo.add %3114, %3115 : tensor<16x4x1xf32> loc(#loc1704)
    %3117 = stablehlo.rsqrt %3116 : tensor<16x4x1xf32> loc(#loc1678)
    %3118 = stablehlo.broadcast_in_dim %3117, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %3119 = stablehlo.multiply %3108, %3118 : tensor<16x4x128xf32> loc(#loc1705)
    %3120 = stablehlo.convert %3119 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %3121 = stablehlo.broadcast_in_dim %arg124, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %3122 = stablehlo.broadcast_in_dim %3121, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %3123 = stablehlo.multiply %3120, %3122 : tensor<16x4x128xbf16> loc(#loc1707)
    %3124 = stablehlo.reshape %3123 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %3125 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %3126 = stablehlo.compare  LT, %arg484, %3125,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %3127 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %3128 = stablehlo.add %arg484, %3127 : tensor<16xi32> loc(#loc1710)
    %3129 = stablehlo.select %3126, %3128, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %3130 = stablehlo.broadcast_in_dim %3129, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %3131 = "stablehlo.gather"(%arg10, %3130) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %3132 = stablehlo.slice %3131 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %3133 = stablehlo.slice %3131 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %3134 = stablehlo.reshape %3106 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %3135 = stablehlo.broadcast_in_dim %3132, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %3136 = stablehlo.broadcast_in_dim %3133, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %3137 = stablehlo.slice %3134 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %3138 = stablehlo.slice %3134 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %3139 = stablehlo.broadcast_in_dim %3135, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %3140 = stablehlo.multiply %3137, %3139 : tensor<16x32x64xbf16> loc(#loc1719)
    %3141 = stablehlo.broadcast_in_dim %3136, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %3142 = stablehlo.multiply %3138, %3141 : tensor<16x32x64xbf16> loc(#loc1720)
    %3143 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %3144 = stablehlo.multiply %3142, %3143 : tensor<16x32x64xbf16> loc(#loc1721)
    %3145 = stablehlo.subtract %3140, %3144 : tensor<16x32x64xbf16> loc(#loc1722)
    %3146 = stablehlo.broadcast_in_dim %3135, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %3147 = stablehlo.multiply %3138, %3146 : tensor<16x32x64xbf16> loc(#loc1723)
    %3148 = stablehlo.broadcast_in_dim %3136, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %3149 = stablehlo.multiply %3137, %3148 : tensor<16x32x64xbf16> loc(#loc1724)
    %3150 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %3151 = stablehlo.multiply %3149, %3150 : tensor<16x32x64xbf16> loc(#loc1725)
    %3152 = stablehlo.add %3147, %3151 : tensor<16x32x64xbf16> loc(#loc1726)
    %3153 = stablehlo.concatenate %3145, %3152, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %3154 = stablehlo.slice %3134 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %3155 = stablehlo.concatenate %3153, %3154, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %3156 = stablehlo.reshape %3155 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %3157 = stablehlo.reshape %3124 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %3158 = stablehlo.broadcast_in_dim %3132, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %3159 = stablehlo.broadcast_in_dim %3133, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %3160 = stablehlo.slice %3157 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %3161 = stablehlo.slice %3157 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %3162 = stablehlo.broadcast_in_dim %3158, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %3163 = stablehlo.multiply %3160, %3162 : tensor<16x4x64xbf16> loc(#loc1735)
    %3164 = stablehlo.broadcast_in_dim %3159, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %3165 = stablehlo.multiply %3161, %3164 : tensor<16x4x64xbf16> loc(#loc1736)
    %3166 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %3167 = stablehlo.multiply %3165, %3166 : tensor<16x4x64xbf16> loc(#loc1737)
    %3168 = stablehlo.subtract %3163, %3167 : tensor<16x4x64xbf16> loc(#loc1738)
    %3169 = stablehlo.broadcast_in_dim %3158, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %3170 = stablehlo.multiply %3161, %3169 : tensor<16x4x64xbf16> loc(#loc1739)
    %3171 = stablehlo.broadcast_in_dim %3159, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %3172 = stablehlo.multiply %3160, %3171 : tensor<16x4x64xbf16> loc(#loc1740)
    %3173 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %3174 = stablehlo.multiply %3172, %3173 : tensor<16x4x64xbf16> loc(#loc1741)
    %3175 = stablehlo.add %3170, %3174 : tensor<16x4x64xbf16> loc(#loc1742)
    %3176 = stablehlo.concatenate %3168, %3175, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %3177 = stablehlo.slice %3157 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %3178 = stablehlo.concatenate %3176, %3177, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %3179 = stablehlo.reshape %3178 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %3180:2 = call @_jax_attn_func(%arg455, %3156, %3179, %3088, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %3181 = stablehlo.dot_general %3180#1, %arg125, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %3182 = stablehlo.reshape %3181 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %3183 = stablehlo.reshape %3182 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %3184 = stablehlo.convert %3183 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %3185 = stablehlo.convert %3061 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %3186 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %3187 = stablehlo.multiply %3185, %3186 : tensor<16x2048xf32> loc(#loc1751)
    %3188 = stablehlo.add %3184, %3187 : tensor<16x2048xf32> loc(#loc1752)
    %3189 = stablehlo.convert %3188 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %3190 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %3191 = stablehlo.power %3188, %3190 : tensor<16x2048xf32> loc(#loc1754)
    %3192 = stablehlo.reduce(%3191 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %3193 = stablehlo.broadcast_in_dim %3192, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %3194 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %3195 = stablehlo.divide %3193, %3194 : tensor<16x1xf32> loc(#loc1757)
    %3196 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %3197 = stablehlo.add %3195, %3196 : tensor<16x1xf32> loc(#loc1758)
    %3198 = stablehlo.rsqrt %3197 : tensor<16x1xf32> loc(#loc1678)
    %3199 = stablehlo.broadcast_in_dim %3198, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %3200 = stablehlo.multiply %3188, %3199 : tensor<16x2048xf32> loc(#loc1759)
    %3201 = stablehlo.convert %3200 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %3202 = stablehlo.broadcast_in_dim %arg123, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %3203 = stablehlo.broadcast_in_dim %3202, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %3204 = stablehlo.multiply %3201, %3203 : tensor<16x2048xbf16> loc(#loc1761)
    %3205 = stablehlo.dot_general %3204, %arg122, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %3206 = stablehlo.reshape %3205 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %3207 = stablehlo.reshape %3206 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %3208 = call @jax_fused_moe_func_padded(%3204, %arg120, %arg121, %3207) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %3209 = stablehlo.convert %3208 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %3210 = stablehlo.convert %3189 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %3211 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %3212 = stablehlo.multiply %3210, %3211 : tensor<16x2048xf32> loc(#loc1766)
    %3213 = stablehlo.add %3209, %3212 : tensor<16x2048xf32> loc(#loc1767)
    %3214 = stablehlo.convert %3213 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %3215 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %3216 = stablehlo.power %3213, %3215 : tensor<16x2048xf32> loc(#loc1768)
    %3217 = stablehlo.reduce(%3216 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %3218 = stablehlo.broadcast_in_dim %3217, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %3219 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %3220 = stablehlo.divide %3218, %3219 : tensor<16x1xf32> loc(#loc1771)
    %3221 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %3222 = stablehlo.add %3220, %3221 : tensor<16x1xf32> loc(#loc1772)
    %3223 = stablehlo.rsqrt %3222 : tensor<16x1xf32> loc(#loc1678)
    %3224 = stablehlo.broadcast_in_dim %3223, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %3225 = stablehlo.multiply %3213, %3224 : tensor<16x2048xf32> loc(#loc1773)
    %3226 = stablehlo.convert %3225 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %3227 = stablehlo.broadcast_in_dim %arg128, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %3228 = stablehlo.broadcast_in_dim %3227, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %3229 = stablehlo.multiply %3226, %3228 : tensor<16x2048xbf16> loc(#loc1775)
    %3230 = stablehlo.dot_general %3229, %arg136, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %3231 = stablehlo.reshape %3230 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %3232 = stablehlo.slice %3231 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %3233 = stablehlo.reshape %3232 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %3234 = stablehlo.slice %3231 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %3235 = stablehlo.reshape %3234 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %3236 = stablehlo.slice %3231 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %3237 = stablehlo.reshape %3236 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %3238 = stablehlo.concatenate %3233, %3235, %3237, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %3239 = stablehlo.slice %3238 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %3240 = stablehlo.slice %3238 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %3241 = stablehlo.slice %3238 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %3242 = stablehlo.reshape %3239 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %3243 = stablehlo.convert %3242 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %3244 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %3245 = stablehlo.power %3243, %3244 : tensor<16x32x128xf32> loc(#loc1690)
    %3246 = stablehlo.reduce(%3245 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %3247 = stablehlo.broadcast_in_dim %3246, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %3248 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %3249 = stablehlo.divide %3247, %3248 : tensor<16x32x1xf32> loc(#loc1693)
    %3250 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %3251 = stablehlo.add %3249, %3250 : tensor<16x32x1xf32> loc(#loc1694)
    %3252 = stablehlo.rsqrt %3251 : tensor<16x32x1xf32> loc(#loc1678)
    %3253 = stablehlo.broadcast_in_dim %3252, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %3254 = stablehlo.multiply %3243, %3253 : tensor<16x32x128xf32> loc(#loc1695)
    %3255 = stablehlo.convert %3254 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %3256 = stablehlo.broadcast_in_dim %arg135, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %3257 = stablehlo.broadcast_in_dim %3256, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %3258 = stablehlo.multiply %3255, %3257 : tensor<16x32x128xbf16> loc(#loc1697)
    %3259 = stablehlo.reshape %3258 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %3260 = stablehlo.reshape %3240 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %3261 = stablehlo.convert %3260 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %3262 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %3263 = stablehlo.power %3261, %3262 : tensor<16x4x128xf32> loc(#loc1700)
    %3264 = stablehlo.reduce(%3263 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %3265 = stablehlo.broadcast_in_dim %3264, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %3266 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %3267 = stablehlo.divide %3265, %3266 : tensor<16x4x1xf32> loc(#loc1703)
    %3268 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %3269 = stablehlo.add %3267, %3268 : tensor<16x4x1xf32> loc(#loc1704)
    %3270 = stablehlo.rsqrt %3269 : tensor<16x4x1xf32> loc(#loc1678)
    %3271 = stablehlo.broadcast_in_dim %3270, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %3272 = stablehlo.multiply %3261, %3271 : tensor<16x4x128xf32> loc(#loc1705)
    %3273 = stablehlo.convert %3272 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %3274 = stablehlo.broadcast_in_dim %arg133, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %3275 = stablehlo.broadcast_in_dim %3274, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %3276 = stablehlo.multiply %3273, %3275 : tensor<16x4x128xbf16> loc(#loc1707)
    %3277 = stablehlo.reshape %3276 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %3278 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %3279 = stablehlo.compare  LT, %arg484, %3278,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %3280 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %3281 = stablehlo.add %arg484, %3280 : tensor<16xi32> loc(#loc1710)
    %3282 = stablehlo.select %3279, %3281, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %3283 = stablehlo.broadcast_in_dim %3282, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %3284 = "stablehlo.gather"(%arg10, %3283) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %3285 = stablehlo.slice %3284 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %3286 = stablehlo.slice %3284 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %3287 = stablehlo.reshape %3259 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %3288 = stablehlo.broadcast_in_dim %3285, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %3289 = stablehlo.broadcast_in_dim %3286, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %3290 = stablehlo.slice %3287 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %3291 = stablehlo.slice %3287 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %3292 = stablehlo.broadcast_in_dim %3288, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %3293 = stablehlo.multiply %3290, %3292 : tensor<16x32x64xbf16> loc(#loc1719)
    %3294 = stablehlo.broadcast_in_dim %3289, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %3295 = stablehlo.multiply %3291, %3294 : tensor<16x32x64xbf16> loc(#loc1720)
    %3296 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %3297 = stablehlo.multiply %3295, %3296 : tensor<16x32x64xbf16> loc(#loc1721)
    %3298 = stablehlo.subtract %3293, %3297 : tensor<16x32x64xbf16> loc(#loc1722)
    %3299 = stablehlo.broadcast_in_dim %3288, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %3300 = stablehlo.multiply %3291, %3299 : tensor<16x32x64xbf16> loc(#loc1723)
    %3301 = stablehlo.broadcast_in_dim %3289, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %3302 = stablehlo.multiply %3290, %3301 : tensor<16x32x64xbf16> loc(#loc1724)
    %3303 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %3304 = stablehlo.multiply %3302, %3303 : tensor<16x32x64xbf16> loc(#loc1725)
    %3305 = stablehlo.add %3300, %3304 : tensor<16x32x64xbf16> loc(#loc1726)
    %3306 = stablehlo.concatenate %3298, %3305, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %3307 = stablehlo.slice %3287 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %3308 = stablehlo.concatenate %3306, %3307, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %3309 = stablehlo.reshape %3308 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %3310 = stablehlo.reshape %3277 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %3311 = stablehlo.broadcast_in_dim %3285, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %3312 = stablehlo.broadcast_in_dim %3286, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %3313 = stablehlo.slice %3310 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %3314 = stablehlo.slice %3310 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %3315 = stablehlo.broadcast_in_dim %3311, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %3316 = stablehlo.multiply %3313, %3315 : tensor<16x4x64xbf16> loc(#loc1735)
    %3317 = stablehlo.broadcast_in_dim %3312, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %3318 = stablehlo.multiply %3314, %3317 : tensor<16x4x64xbf16> loc(#loc1736)
    %3319 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %3320 = stablehlo.multiply %3318, %3319 : tensor<16x4x64xbf16> loc(#loc1737)
    %3321 = stablehlo.subtract %3316, %3320 : tensor<16x4x64xbf16> loc(#loc1738)
    %3322 = stablehlo.broadcast_in_dim %3311, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %3323 = stablehlo.multiply %3314, %3322 : tensor<16x4x64xbf16> loc(#loc1739)
    %3324 = stablehlo.broadcast_in_dim %3312, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %3325 = stablehlo.multiply %3313, %3324 : tensor<16x4x64xbf16> loc(#loc1740)
    %3326 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %3327 = stablehlo.multiply %3325, %3326 : tensor<16x4x64xbf16> loc(#loc1741)
    %3328 = stablehlo.add %3323, %3327 : tensor<16x4x64xbf16> loc(#loc1742)
    %3329 = stablehlo.concatenate %3321, %3328, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %3330 = stablehlo.slice %3310 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %3331 = stablehlo.concatenate %3329, %3330, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %3332 = stablehlo.reshape %3331 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %3333:2 = call @_jax_attn_func(%arg456, %3309, %3332, %3241, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %3334 = stablehlo.dot_general %3333#1, %arg134, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %3335 = stablehlo.reshape %3334 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %3336 = stablehlo.reshape %3335 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %3337 = stablehlo.convert %3336 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %3338 = stablehlo.convert %3214 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %3339 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %3340 = stablehlo.multiply %3338, %3339 : tensor<16x2048xf32> loc(#loc1751)
    %3341 = stablehlo.add %3337, %3340 : tensor<16x2048xf32> loc(#loc1752)
    %3342 = stablehlo.convert %3341 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %3343 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %3344 = stablehlo.power %3341, %3343 : tensor<16x2048xf32> loc(#loc1754)
    %3345 = stablehlo.reduce(%3344 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %3346 = stablehlo.broadcast_in_dim %3345, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %3347 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %3348 = stablehlo.divide %3346, %3347 : tensor<16x1xf32> loc(#loc1757)
    %3349 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %3350 = stablehlo.add %3348, %3349 : tensor<16x1xf32> loc(#loc1758)
    %3351 = stablehlo.rsqrt %3350 : tensor<16x1xf32> loc(#loc1678)
    %3352 = stablehlo.broadcast_in_dim %3351, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %3353 = stablehlo.multiply %3341, %3352 : tensor<16x2048xf32> loc(#loc1759)
    %3354 = stablehlo.convert %3353 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %3355 = stablehlo.broadcast_in_dim %arg132, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %3356 = stablehlo.broadcast_in_dim %3355, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %3357 = stablehlo.multiply %3354, %3356 : tensor<16x2048xbf16> loc(#loc1761)
    %3358 = stablehlo.dot_general %3357, %arg131, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %3359 = stablehlo.reshape %3358 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %3360 = stablehlo.reshape %3359 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %3361 = call @jax_fused_moe_func_padded(%3357, %arg129, %arg130, %3360) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %3362 = stablehlo.convert %3361 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %3363 = stablehlo.convert %3342 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %3364 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %3365 = stablehlo.multiply %3363, %3364 : tensor<16x2048xf32> loc(#loc1766)
    %3366 = stablehlo.add %3362, %3365 : tensor<16x2048xf32> loc(#loc1767)
    %3367 = stablehlo.convert %3366 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %3368 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %3369 = stablehlo.power %3366, %3368 : tensor<16x2048xf32> loc(#loc1768)
    %3370 = stablehlo.reduce(%3369 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %3371 = stablehlo.broadcast_in_dim %3370, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %3372 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %3373 = stablehlo.divide %3371, %3372 : tensor<16x1xf32> loc(#loc1771)
    %3374 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %3375 = stablehlo.add %3373, %3374 : tensor<16x1xf32> loc(#loc1772)
    %3376 = stablehlo.rsqrt %3375 : tensor<16x1xf32> loc(#loc1678)
    %3377 = stablehlo.broadcast_in_dim %3376, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %3378 = stablehlo.multiply %3366, %3377 : tensor<16x2048xf32> loc(#loc1773)
    %3379 = stablehlo.convert %3378 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %3380 = stablehlo.broadcast_in_dim %arg137, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %3381 = stablehlo.broadcast_in_dim %3380, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %3382 = stablehlo.multiply %3379, %3381 : tensor<16x2048xbf16> loc(#loc1775)
    %3383 = stablehlo.dot_general %3382, %arg145, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %3384 = stablehlo.reshape %3383 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %3385 = stablehlo.slice %3384 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %3386 = stablehlo.reshape %3385 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %3387 = stablehlo.slice %3384 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %3388 = stablehlo.reshape %3387 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %3389 = stablehlo.slice %3384 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %3390 = stablehlo.reshape %3389 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %3391 = stablehlo.concatenate %3386, %3388, %3390, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %3392 = stablehlo.slice %3391 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %3393 = stablehlo.slice %3391 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %3394 = stablehlo.slice %3391 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %3395 = stablehlo.reshape %3392 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %3396 = stablehlo.convert %3395 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %3397 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %3398 = stablehlo.power %3396, %3397 : tensor<16x32x128xf32> loc(#loc1690)
    %3399 = stablehlo.reduce(%3398 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %3400 = stablehlo.broadcast_in_dim %3399, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %3401 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %3402 = stablehlo.divide %3400, %3401 : tensor<16x32x1xf32> loc(#loc1693)
    %3403 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %3404 = stablehlo.add %3402, %3403 : tensor<16x32x1xf32> loc(#loc1694)
    %3405 = stablehlo.rsqrt %3404 : tensor<16x32x1xf32> loc(#loc1678)
    %3406 = stablehlo.broadcast_in_dim %3405, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %3407 = stablehlo.multiply %3396, %3406 : tensor<16x32x128xf32> loc(#loc1695)
    %3408 = stablehlo.convert %3407 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %3409 = stablehlo.broadcast_in_dim %arg144, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %3410 = stablehlo.broadcast_in_dim %3409, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %3411 = stablehlo.multiply %3408, %3410 : tensor<16x32x128xbf16> loc(#loc1697)
    %3412 = stablehlo.reshape %3411 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %3413 = stablehlo.reshape %3393 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %3414 = stablehlo.convert %3413 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %3415 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %3416 = stablehlo.power %3414, %3415 : tensor<16x4x128xf32> loc(#loc1700)
    %3417 = stablehlo.reduce(%3416 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %3418 = stablehlo.broadcast_in_dim %3417, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %3419 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %3420 = stablehlo.divide %3418, %3419 : tensor<16x4x1xf32> loc(#loc1703)
    %3421 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %3422 = stablehlo.add %3420, %3421 : tensor<16x4x1xf32> loc(#loc1704)
    %3423 = stablehlo.rsqrt %3422 : tensor<16x4x1xf32> loc(#loc1678)
    %3424 = stablehlo.broadcast_in_dim %3423, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %3425 = stablehlo.multiply %3414, %3424 : tensor<16x4x128xf32> loc(#loc1705)
    %3426 = stablehlo.convert %3425 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %3427 = stablehlo.broadcast_in_dim %arg142, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %3428 = stablehlo.broadcast_in_dim %3427, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %3429 = stablehlo.multiply %3426, %3428 : tensor<16x4x128xbf16> loc(#loc1707)
    %3430 = stablehlo.reshape %3429 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %3431 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %3432 = stablehlo.compare  LT, %arg484, %3431,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %3433 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %3434 = stablehlo.add %arg484, %3433 : tensor<16xi32> loc(#loc1710)
    %3435 = stablehlo.select %3432, %3434, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %3436 = stablehlo.broadcast_in_dim %3435, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %3437 = "stablehlo.gather"(%arg10, %3436) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %3438 = stablehlo.slice %3437 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %3439 = stablehlo.slice %3437 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %3440 = stablehlo.reshape %3412 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %3441 = stablehlo.broadcast_in_dim %3438, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %3442 = stablehlo.broadcast_in_dim %3439, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %3443 = stablehlo.slice %3440 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %3444 = stablehlo.slice %3440 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %3445 = stablehlo.broadcast_in_dim %3441, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %3446 = stablehlo.multiply %3443, %3445 : tensor<16x32x64xbf16> loc(#loc1719)
    %3447 = stablehlo.broadcast_in_dim %3442, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %3448 = stablehlo.multiply %3444, %3447 : tensor<16x32x64xbf16> loc(#loc1720)
    %3449 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %3450 = stablehlo.multiply %3448, %3449 : tensor<16x32x64xbf16> loc(#loc1721)
    %3451 = stablehlo.subtract %3446, %3450 : tensor<16x32x64xbf16> loc(#loc1722)
    %3452 = stablehlo.broadcast_in_dim %3441, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %3453 = stablehlo.multiply %3444, %3452 : tensor<16x32x64xbf16> loc(#loc1723)
    %3454 = stablehlo.broadcast_in_dim %3442, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %3455 = stablehlo.multiply %3443, %3454 : tensor<16x32x64xbf16> loc(#loc1724)
    %3456 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %3457 = stablehlo.multiply %3455, %3456 : tensor<16x32x64xbf16> loc(#loc1725)
    %3458 = stablehlo.add %3453, %3457 : tensor<16x32x64xbf16> loc(#loc1726)
    %3459 = stablehlo.concatenate %3451, %3458, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %3460 = stablehlo.slice %3440 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %3461 = stablehlo.concatenate %3459, %3460, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %3462 = stablehlo.reshape %3461 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %3463 = stablehlo.reshape %3430 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %3464 = stablehlo.broadcast_in_dim %3438, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %3465 = stablehlo.broadcast_in_dim %3439, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %3466 = stablehlo.slice %3463 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %3467 = stablehlo.slice %3463 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %3468 = stablehlo.broadcast_in_dim %3464, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %3469 = stablehlo.multiply %3466, %3468 : tensor<16x4x64xbf16> loc(#loc1735)
    %3470 = stablehlo.broadcast_in_dim %3465, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %3471 = stablehlo.multiply %3467, %3470 : tensor<16x4x64xbf16> loc(#loc1736)
    %3472 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %3473 = stablehlo.multiply %3471, %3472 : tensor<16x4x64xbf16> loc(#loc1737)
    %3474 = stablehlo.subtract %3469, %3473 : tensor<16x4x64xbf16> loc(#loc1738)
    %3475 = stablehlo.broadcast_in_dim %3464, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %3476 = stablehlo.multiply %3467, %3475 : tensor<16x4x64xbf16> loc(#loc1739)
    %3477 = stablehlo.broadcast_in_dim %3465, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %3478 = stablehlo.multiply %3466, %3477 : tensor<16x4x64xbf16> loc(#loc1740)
    %3479 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %3480 = stablehlo.multiply %3478, %3479 : tensor<16x4x64xbf16> loc(#loc1741)
    %3481 = stablehlo.add %3476, %3480 : tensor<16x4x64xbf16> loc(#loc1742)
    %3482 = stablehlo.concatenate %3474, %3481, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %3483 = stablehlo.slice %3463 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %3484 = stablehlo.concatenate %3482, %3483, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %3485 = stablehlo.reshape %3484 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %3486:2 = call @_jax_attn_func(%arg457, %3462, %3485, %3394, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %3487 = stablehlo.dot_general %3486#1, %arg143, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %3488 = stablehlo.reshape %3487 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %3489 = stablehlo.reshape %3488 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %3490 = stablehlo.convert %3489 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %3491 = stablehlo.convert %3367 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %3492 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %3493 = stablehlo.multiply %3491, %3492 : tensor<16x2048xf32> loc(#loc1751)
    %3494 = stablehlo.add %3490, %3493 : tensor<16x2048xf32> loc(#loc1752)
    %3495 = stablehlo.convert %3494 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %3496 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %3497 = stablehlo.power %3494, %3496 : tensor<16x2048xf32> loc(#loc1754)
    %3498 = stablehlo.reduce(%3497 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %3499 = stablehlo.broadcast_in_dim %3498, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %3500 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %3501 = stablehlo.divide %3499, %3500 : tensor<16x1xf32> loc(#loc1757)
    %3502 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %3503 = stablehlo.add %3501, %3502 : tensor<16x1xf32> loc(#loc1758)
    %3504 = stablehlo.rsqrt %3503 : tensor<16x1xf32> loc(#loc1678)
    %3505 = stablehlo.broadcast_in_dim %3504, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %3506 = stablehlo.multiply %3494, %3505 : tensor<16x2048xf32> loc(#loc1759)
    %3507 = stablehlo.convert %3506 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %3508 = stablehlo.broadcast_in_dim %arg141, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %3509 = stablehlo.broadcast_in_dim %3508, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %3510 = stablehlo.multiply %3507, %3509 : tensor<16x2048xbf16> loc(#loc1761)
    %3511 = stablehlo.dot_general %3510, %arg140, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %3512 = stablehlo.reshape %3511 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %3513 = stablehlo.reshape %3512 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %3514 = call @jax_fused_moe_func_padded(%3510, %arg138, %arg139, %3513) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %3515 = stablehlo.convert %3514 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %3516 = stablehlo.convert %3495 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %3517 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %3518 = stablehlo.multiply %3516, %3517 : tensor<16x2048xf32> loc(#loc1766)
    %3519 = stablehlo.add %3515, %3518 : tensor<16x2048xf32> loc(#loc1767)
    %3520 = stablehlo.convert %3519 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %3521 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %3522 = stablehlo.power %3519, %3521 : tensor<16x2048xf32> loc(#loc1768)
    %3523 = stablehlo.reduce(%3522 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %3524 = stablehlo.broadcast_in_dim %3523, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %3525 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %3526 = stablehlo.divide %3524, %3525 : tensor<16x1xf32> loc(#loc1771)
    %3527 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %3528 = stablehlo.add %3526, %3527 : tensor<16x1xf32> loc(#loc1772)
    %3529 = stablehlo.rsqrt %3528 : tensor<16x1xf32> loc(#loc1678)
    %3530 = stablehlo.broadcast_in_dim %3529, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %3531 = stablehlo.multiply %3519, %3530 : tensor<16x2048xf32> loc(#loc1773)
    %3532 = stablehlo.convert %3531 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %3533 = stablehlo.broadcast_in_dim %arg146, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %3534 = stablehlo.broadcast_in_dim %3533, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %3535 = stablehlo.multiply %3532, %3534 : tensor<16x2048xbf16> loc(#loc1775)
    %3536 = stablehlo.dot_general %3535, %arg154, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %3537 = stablehlo.reshape %3536 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %3538 = stablehlo.slice %3537 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %3539 = stablehlo.reshape %3538 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %3540 = stablehlo.slice %3537 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %3541 = stablehlo.reshape %3540 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %3542 = stablehlo.slice %3537 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %3543 = stablehlo.reshape %3542 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %3544 = stablehlo.concatenate %3539, %3541, %3543, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %3545 = stablehlo.slice %3544 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %3546 = stablehlo.slice %3544 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %3547 = stablehlo.slice %3544 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %3548 = stablehlo.reshape %3545 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %3549 = stablehlo.convert %3548 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %3550 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %3551 = stablehlo.power %3549, %3550 : tensor<16x32x128xf32> loc(#loc1690)
    %3552 = stablehlo.reduce(%3551 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %3553 = stablehlo.broadcast_in_dim %3552, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %3554 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %3555 = stablehlo.divide %3553, %3554 : tensor<16x32x1xf32> loc(#loc1693)
    %3556 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %3557 = stablehlo.add %3555, %3556 : tensor<16x32x1xf32> loc(#loc1694)
    %3558 = stablehlo.rsqrt %3557 : tensor<16x32x1xf32> loc(#loc1678)
    %3559 = stablehlo.broadcast_in_dim %3558, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %3560 = stablehlo.multiply %3549, %3559 : tensor<16x32x128xf32> loc(#loc1695)
    %3561 = stablehlo.convert %3560 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %3562 = stablehlo.broadcast_in_dim %arg153, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %3563 = stablehlo.broadcast_in_dim %3562, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %3564 = stablehlo.multiply %3561, %3563 : tensor<16x32x128xbf16> loc(#loc1697)
    %3565 = stablehlo.reshape %3564 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %3566 = stablehlo.reshape %3546 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %3567 = stablehlo.convert %3566 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %3568 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %3569 = stablehlo.power %3567, %3568 : tensor<16x4x128xf32> loc(#loc1700)
    %3570 = stablehlo.reduce(%3569 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %3571 = stablehlo.broadcast_in_dim %3570, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %3572 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %3573 = stablehlo.divide %3571, %3572 : tensor<16x4x1xf32> loc(#loc1703)
    %3574 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %3575 = stablehlo.add %3573, %3574 : tensor<16x4x1xf32> loc(#loc1704)
    %3576 = stablehlo.rsqrt %3575 : tensor<16x4x1xf32> loc(#loc1678)
    %3577 = stablehlo.broadcast_in_dim %3576, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %3578 = stablehlo.multiply %3567, %3577 : tensor<16x4x128xf32> loc(#loc1705)
    %3579 = stablehlo.convert %3578 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %3580 = stablehlo.broadcast_in_dim %arg151, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %3581 = stablehlo.broadcast_in_dim %3580, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %3582 = stablehlo.multiply %3579, %3581 : tensor<16x4x128xbf16> loc(#loc1707)
    %3583 = stablehlo.reshape %3582 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %3584 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %3585 = stablehlo.compare  LT, %arg484, %3584,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %3586 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %3587 = stablehlo.add %arg484, %3586 : tensor<16xi32> loc(#loc1710)
    %3588 = stablehlo.select %3585, %3587, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %3589 = stablehlo.broadcast_in_dim %3588, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %3590 = "stablehlo.gather"(%arg10, %3589) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %3591 = stablehlo.slice %3590 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %3592 = stablehlo.slice %3590 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %3593 = stablehlo.reshape %3565 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %3594 = stablehlo.broadcast_in_dim %3591, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %3595 = stablehlo.broadcast_in_dim %3592, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %3596 = stablehlo.slice %3593 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %3597 = stablehlo.slice %3593 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %3598 = stablehlo.broadcast_in_dim %3594, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %3599 = stablehlo.multiply %3596, %3598 : tensor<16x32x64xbf16> loc(#loc1719)
    %3600 = stablehlo.broadcast_in_dim %3595, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %3601 = stablehlo.multiply %3597, %3600 : tensor<16x32x64xbf16> loc(#loc1720)
    %3602 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %3603 = stablehlo.multiply %3601, %3602 : tensor<16x32x64xbf16> loc(#loc1721)
    %3604 = stablehlo.subtract %3599, %3603 : tensor<16x32x64xbf16> loc(#loc1722)
    %3605 = stablehlo.broadcast_in_dim %3594, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %3606 = stablehlo.multiply %3597, %3605 : tensor<16x32x64xbf16> loc(#loc1723)
    %3607 = stablehlo.broadcast_in_dim %3595, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %3608 = stablehlo.multiply %3596, %3607 : tensor<16x32x64xbf16> loc(#loc1724)
    %3609 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %3610 = stablehlo.multiply %3608, %3609 : tensor<16x32x64xbf16> loc(#loc1725)
    %3611 = stablehlo.add %3606, %3610 : tensor<16x32x64xbf16> loc(#loc1726)
    %3612 = stablehlo.concatenate %3604, %3611, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %3613 = stablehlo.slice %3593 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %3614 = stablehlo.concatenate %3612, %3613, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %3615 = stablehlo.reshape %3614 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %3616 = stablehlo.reshape %3583 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %3617 = stablehlo.broadcast_in_dim %3591, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %3618 = stablehlo.broadcast_in_dim %3592, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %3619 = stablehlo.slice %3616 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %3620 = stablehlo.slice %3616 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %3621 = stablehlo.broadcast_in_dim %3617, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %3622 = stablehlo.multiply %3619, %3621 : tensor<16x4x64xbf16> loc(#loc1735)
    %3623 = stablehlo.broadcast_in_dim %3618, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %3624 = stablehlo.multiply %3620, %3623 : tensor<16x4x64xbf16> loc(#loc1736)
    %3625 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %3626 = stablehlo.multiply %3624, %3625 : tensor<16x4x64xbf16> loc(#loc1737)
    %3627 = stablehlo.subtract %3622, %3626 : tensor<16x4x64xbf16> loc(#loc1738)
    %3628 = stablehlo.broadcast_in_dim %3617, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %3629 = stablehlo.multiply %3620, %3628 : tensor<16x4x64xbf16> loc(#loc1739)
    %3630 = stablehlo.broadcast_in_dim %3618, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %3631 = stablehlo.multiply %3619, %3630 : tensor<16x4x64xbf16> loc(#loc1740)
    %3632 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %3633 = stablehlo.multiply %3631, %3632 : tensor<16x4x64xbf16> loc(#loc1741)
    %3634 = stablehlo.add %3629, %3633 : tensor<16x4x64xbf16> loc(#loc1742)
    %3635 = stablehlo.concatenate %3627, %3634, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %3636 = stablehlo.slice %3616 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %3637 = stablehlo.concatenate %3635, %3636, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %3638 = stablehlo.reshape %3637 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %3639:2 = call @_jax_attn_func(%arg458, %3615, %3638, %3547, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %3640 = stablehlo.dot_general %3639#1, %arg152, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %3641 = stablehlo.reshape %3640 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %3642 = stablehlo.reshape %3641 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %3643 = stablehlo.convert %3642 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %3644 = stablehlo.convert %3520 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %3645 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %3646 = stablehlo.multiply %3644, %3645 : tensor<16x2048xf32> loc(#loc1751)
    %3647 = stablehlo.add %3643, %3646 : tensor<16x2048xf32> loc(#loc1752)
    %3648 = stablehlo.convert %3647 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %3649 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %3650 = stablehlo.power %3647, %3649 : tensor<16x2048xf32> loc(#loc1754)
    %3651 = stablehlo.reduce(%3650 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %3652 = stablehlo.broadcast_in_dim %3651, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %3653 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %3654 = stablehlo.divide %3652, %3653 : tensor<16x1xf32> loc(#loc1757)
    %3655 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %3656 = stablehlo.add %3654, %3655 : tensor<16x1xf32> loc(#loc1758)
    %3657 = stablehlo.rsqrt %3656 : tensor<16x1xf32> loc(#loc1678)
    %3658 = stablehlo.broadcast_in_dim %3657, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %3659 = stablehlo.multiply %3647, %3658 : tensor<16x2048xf32> loc(#loc1759)
    %3660 = stablehlo.convert %3659 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %3661 = stablehlo.broadcast_in_dim %arg150, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %3662 = stablehlo.broadcast_in_dim %3661, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %3663 = stablehlo.multiply %3660, %3662 : tensor<16x2048xbf16> loc(#loc1761)
    %3664 = stablehlo.dot_general %3663, %arg149, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %3665 = stablehlo.reshape %3664 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %3666 = stablehlo.reshape %3665 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %3667 = call @jax_fused_moe_func_padded(%3663, %arg147, %arg148, %3666) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %3668 = stablehlo.convert %3667 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %3669 = stablehlo.convert %3648 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %3670 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %3671 = stablehlo.multiply %3669, %3670 : tensor<16x2048xf32> loc(#loc1766)
    %3672 = stablehlo.add %3668, %3671 : tensor<16x2048xf32> loc(#loc1767)
    %3673 = stablehlo.convert %3672 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %3674 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %3675 = stablehlo.power %3672, %3674 : tensor<16x2048xf32> loc(#loc1768)
    %3676 = stablehlo.reduce(%3675 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %3677 = stablehlo.broadcast_in_dim %3676, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %3678 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %3679 = stablehlo.divide %3677, %3678 : tensor<16x1xf32> loc(#loc1771)
    %3680 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %3681 = stablehlo.add %3679, %3680 : tensor<16x1xf32> loc(#loc1772)
    %3682 = stablehlo.rsqrt %3681 : tensor<16x1xf32> loc(#loc1678)
    %3683 = stablehlo.broadcast_in_dim %3682, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %3684 = stablehlo.multiply %3672, %3683 : tensor<16x2048xf32> loc(#loc1773)
    %3685 = stablehlo.convert %3684 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %3686 = stablehlo.broadcast_in_dim %arg155, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %3687 = stablehlo.broadcast_in_dim %3686, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %3688 = stablehlo.multiply %3685, %3687 : tensor<16x2048xbf16> loc(#loc1775)
    %3689 = stablehlo.dot_general %3688, %arg163, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %3690 = stablehlo.reshape %3689 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %3691 = stablehlo.slice %3690 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %3692 = stablehlo.reshape %3691 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %3693 = stablehlo.slice %3690 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %3694 = stablehlo.reshape %3693 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %3695 = stablehlo.slice %3690 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %3696 = stablehlo.reshape %3695 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %3697 = stablehlo.concatenate %3692, %3694, %3696, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %3698 = stablehlo.slice %3697 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %3699 = stablehlo.slice %3697 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %3700 = stablehlo.slice %3697 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %3701 = stablehlo.reshape %3698 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %3702 = stablehlo.convert %3701 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %3703 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %3704 = stablehlo.power %3702, %3703 : tensor<16x32x128xf32> loc(#loc1690)
    %3705 = stablehlo.reduce(%3704 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %3706 = stablehlo.broadcast_in_dim %3705, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %3707 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %3708 = stablehlo.divide %3706, %3707 : tensor<16x32x1xf32> loc(#loc1693)
    %3709 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %3710 = stablehlo.add %3708, %3709 : tensor<16x32x1xf32> loc(#loc1694)
    %3711 = stablehlo.rsqrt %3710 : tensor<16x32x1xf32> loc(#loc1678)
    %3712 = stablehlo.broadcast_in_dim %3711, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %3713 = stablehlo.multiply %3702, %3712 : tensor<16x32x128xf32> loc(#loc1695)
    %3714 = stablehlo.convert %3713 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %3715 = stablehlo.broadcast_in_dim %arg162, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %3716 = stablehlo.broadcast_in_dim %3715, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %3717 = stablehlo.multiply %3714, %3716 : tensor<16x32x128xbf16> loc(#loc1697)
    %3718 = stablehlo.reshape %3717 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %3719 = stablehlo.reshape %3699 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %3720 = stablehlo.convert %3719 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %3721 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %3722 = stablehlo.power %3720, %3721 : tensor<16x4x128xf32> loc(#loc1700)
    %3723 = stablehlo.reduce(%3722 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %3724 = stablehlo.broadcast_in_dim %3723, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %3725 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %3726 = stablehlo.divide %3724, %3725 : tensor<16x4x1xf32> loc(#loc1703)
    %3727 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %3728 = stablehlo.add %3726, %3727 : tensor<16x4x1xf32> loc(#loc1704)
    %3729 = stablehlo.rsqrt %3728 : tensor<16x4x1xf32> loc(#loc1678)
    %3730 = stablehlo.broadcast_in_dim %3729, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %3731 = stablehlo.multiply %3720, %3730 : tensor<16x4x128xf32> loc(#loc1705)
    %3732 = stablehlo.convert %3731 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %3733 = stablehlo.broadcast_in_dim %arg160, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %3734 = stablehlo.broadcast_in_dim %3733, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %3735 = stablehlo.multiply %3732, %3734 : tensor<16x4x128xbf16> loc(#loc1707)
    %3736 = stablehlo.reshape %3735 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %3737 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %3738 = stablehlo.compare  LT, %arg484, %3737,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %3739 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %3740 = stablehlo.add %arg484, %3739 : tensor<16xi32> loc(#loc1710)
    %3741 = stablehlo.select %3738, %3740, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %3742 = stablehlo.broadcast_in_dim %3741, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %3743 = "stablehlo.gather"(%arg10, %3742) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %3744 = stablehlo.slice %3743 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %3745 = stablehlo.slice %3743 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %3746 = stablehlo.reshape %3718 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %3747 = stablehlo.broadcast_in_dim %3744, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %3748 = stablehlo.broadcast_in_dim %3745, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %3749 = stablehlo.slice %3746 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %3750 = stablehlo.slice %3746 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %3751 = stablehlo.broadcast_in_dim %3747, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %3752 = stablehlo.multiply %3749, %3751 : tensor<16x32x64xbf16> loc(#loc1719)
    %3753 = stablehlo.broadcast_in_dim %3748, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %3754 = stablehlo.multiply %3750, %3753 : tensor<16x32x64xbf16> loc(#loc1720)
    %3755 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %3756 = stablehlo.multiply %3754, %3755 : tensor<16x32x64xbf16> loc(#loc1721)
    %3757 = stablehlo.subtract %3752, %3756 : tensor<16x32x64xbf16> loc(#loc1722)
    %3758 = stablehlo.broadcast_in_dim %3747, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %3759 = stablehlo.multiply %3750, %3758 : tensor<16x32x64xbf16> loc(#loc1723)
    %3760 = stablehlo.broadcast_in_dim %3748, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %3761 = stablehlo.multiply %3749, %3760 : tensor<16x32x64xbf16> loc(#loc1724)
    %3762 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %3763 = stablehlo.multiply %3761, %3762 : tensor<16x32x64xbf16> loc(#loc1725)
    %3764 = stablehlo.add %3759, %3763 : tensor<16x32x64xbf16> loc(#loc1726)
    %3765 = stablehlo.concatenate %3757, %3764, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %3766 = stablehlo.slice %3746 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %3767 = stablehlo.concatenate %3765, %3766, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %3768 = stablehlo.reshape %3767 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %3769 = stablehlo.reshape %3736 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %3770 = stablehlo.broadcast_in_dim %3744, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %3771 = stablehlo.broadcast_in_dim %3745, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %3772 = stablehlo.slice %3769 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %3773 = stablehlo.slice %3769 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %3774 = stablehlo.broadcast_in_dim %3770, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %3775 = stablehlo.multiply %3772, %3774 : tensor<16x4x64xbf16> loc(#loc1735)
    %3776 = stablehlo.broadcast_in_dim %3771, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %3777 = stablehlo.multiply %3773, %3776 : tensor<16x4x64xbf16> loc(#loc1736)
    %3778 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %3779 = stablehlo.multiply %3777, %3778 : tensor<16x4x64xbf16> loc(#loc1737)
    %3780 = stablehlo.subtract %3775, %3779 : tensor<16x4x64xbf16> loc(#loc1738)
    %3781 = stablehlo.broadcast_in_dim %3770, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %3782 = stablehlo.multiply %3773, %3781 : tensor<16x4x64xbf16> loc(#loc1739)
    %3783 = stablehlo.broadcast_in_dim %3771, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %3784 = stablehlo.multiply %3772, %3783 : tensor<16x4x64xbf16> loc(#loc1740)
    %3785 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %3786 = stablehlo.multiply %3784, %3785 : tensor<16x4x64xbf16> loc(#loc1741)
    %3787 = stablehlo.add %3782, %3786 : tensor<16x4x64xbf16> loc(#loc1742)
    %3788 = stablehlo.concatenate %3780, %3787, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %3789 = stablehlo.slice %3769 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %3790 = stablehlo.concatenate %3788, %3789, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %3791 = stablehlo.reshape %3790 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %3792:2 = call @_jax_attn_func(%arg459, %3768, %3791, %3700, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %3793 = stablehlo.dot_general %3792#1, %arg161, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %3794 = stablehlo.reshape %3793 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %3795 = stablehlo.reshape %3794 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %3796 = stablehlo.convert %3795 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %3797 = stablehlo.convert %3673 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %3798 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %3799 = stablehlo.multiply %3797, %3798 : tensor<16x2048xf32> loc(#loc1751)
    %3800 = stablehlo.add %3796, %3799 : tensor<16x2048xf32> loc(#loc1752)
    %3801 = stablehlo.convert %3800 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %3802 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %3803 = stablehlo.power %3800, %3802 : tensor<16x2048xf32> loc(#loc1754)
    %3804 = stablehlo.reduce(%3803 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %3805 = stablehlo.broadcast_in_dim %3804, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %3806 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %3807 = stablehlo.divide %3805, %3806 : tensor<16x1xf32> loc(#loc1757)
    %3808 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %3809 = stablehlo.add %3807, %3808 : tensor<16x1xf32> loc(#loc1758)
    %3810 = stablehlo.rsqrt %3809 : tensor<16x1xf32> loc(#loc1678)
    %3811 = stablehlo.broadcast_in_dim %3810, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %3812 = stablehlo.multiply %3800, %3811 : tensor<16x2048xf32> loc(#loc1759)
    %3813 = stablehlo.convert %3812 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %3814 = stablehlo.broadcast_in_dim %arg159, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %3815 = stablehlo.broadcast_in_dim %3814, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %3816 = stablehlo.multiply %3813, %3815 : tensor<16x2048xbf16> loc(#loc1761)
    %3817 = stablehlo.dot_general %3816, %arg158, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %3818 = stablehlo.reshape %3817 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %3819 = stablehlo.reshape %3818 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %3820 = call @jax_fused_moe_func_padded(%3816, %arg156, %arg157, %3819) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %3821 = stablehlo.convert %3820 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %3822 = stablehlo.convert %3801 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %3823 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %3824 = stablehlo.multiply %3822, %3823 : tensor<16x2048xf32> loc(#loc1766)
    %3825 = stablehlo.add %3821, %3824 : tensor<16x2048xf32> loc(#loc1767)
    %3826 = stablehlo.convert %3825 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %3827 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %3828 = stablehlo.power %3825, %3827 : tensor<16x2048xf32> loc(#loc1768)
    %3829 = stablehlo.reduce(%3828 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %3830 = stablehlo.broadcast_in_dim %3829, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %3831 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %3832 = stablehlo.divide %3830, %3831 : tensor<16x1xf32> loc(#loc1771)
    %3833 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %3834 = stablehlo.add %3832, %3833 : tensor<16x1xf32> loc(#loc1772)
    %3835 = stablehlo.rsqrt %3834 : tensor<16x1xf32> loc(#loc1678)
    %3836 = stablehlo.broadcast_in_dim %3835, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %3837 = stablehlo.multiply %3825, %3836 : tensor<16x2048xf32> loc(#loc1773)
    %3838 = stablehlo.convert %3837 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %3839 = stablehlo.broadcast_in_dim %arg164, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %3840 = stablehlo.broadcast_in_dim %3839, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %3841 = stablehlo.multiply %3838, %3840 : tensor<16x2048xbf16> loc(#loc1775)
    %3842 = stablehlo.dot_general %3841, %arg172, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %3843 = stablehlo.reshape %3842 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %3844 = stablehlo.slice %3843 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %3845 = stablehlo.reshape %3844 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %3846 = stablehlo.slice %3843 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %3847 = stablehlo.reshape %3846 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %3848 = stablehlo.slice %3843 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %3849 = stablehlo.reshape %3848 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %3850 = stablehlo.concatenate %3845, %3847, %3849, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %3851 = stablehlo.slice %3850 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %3852 = stablehlo.slice %3850 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %3853 = stablehlo.slice %3850 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %3854 = stablehlo.reshape %3851 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %3855 = stablehlo.convert %3854 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %3856 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %3857 = stablehlo.power %3855, %3856 : tensor<16x32x128xf32> loc(#loc1690)
    %3858 = stablehlo.reduce(%3857 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %3859 = stablehlo.broadcast_in_dim %3858, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %3860 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %3861 = stablehlo.divide %3859, %3860 : tensor<16x32x1xf32> loc(#loc1693)
    %3862 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %3863 = stablehlo.add %3861, %3862 : tensor<16x32x1xf32> loc(#loc1694)
    %3864 = stablehlo.rsqrt %3863 : tensor<16x32x1xf32> loc(#loc1678)
    %3865 = stablehlo.broadcast_in_dim %3864, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %3866 = stablehlo.multiply %3855, %3865 : tensor<16x32x128xf32> loc(#loc1695)
    %3867 = stablehlo.convert %3866 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %3868 = stablehlo.broadcast_in_dim %arg171, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %3869 = stablehlo.broadcast_in_dim %3868, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %3870 = stablehlo.multiply %3867, %3869 : tensor<16x32x128xbf16> loc(#loc1697)
    %3871 = stablehlo.reshape %3870 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %3872 = stablehlo.reshape %3852 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %3873 = stablehlo.convert %3872 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %3874 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %3875 = stablehlo.power %3873, %3874 : tensor<16x4x128xf32> loc(#loc1700)
    %3876 = stablehlo.reduce(%3875 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %3877 = stablehlo.broadcast_in_dim %3876, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %3878 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %3879 = stablehlo.divide %3877, %3878 : tensor<16x4x1xf32> loc(#loc1703)
    %3880 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %3881 = stablehlo.add %3879, %3880 : tensor<16x4x1xf32> loc(#loc1704)
    %3882 = stablehlo.rsqrt %3881 : tensor<16x4x1xf32> loc(#loc1678)
    %3883 = stablehlo.broadcast_in_dim %3882, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %3884 = stablehlo.multiply %3873, %3883 : tensor<16x4x128xf32> loc(#loc1705)
    %3885 = stablehlo.convert %3884 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %3886 = stablehlo.broadcast_in_dim %arg169, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %3887 = stablehlo.broadcast_in_dim %3886, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %3888 = stablehlo.multiply %3885, %3887 : tensor<16x4x128xbf16> loc(#loc1707)
    %3889 = stablehlo.reshape %3888 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %3890 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %3891 = stablehlo.compare  LT, %arg484, %3890,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %3892 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %3893 = stablehlo.add %arg484, %3892 : tensor<16xi32> loc(#loc1710)
    %3894 = stablehlo.select %3891, %3893, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %3895 = stablehlo.broadcast_in_dim %3894, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %3896 = "stablehlo.gather"(%arg10, %3895) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %3897 = stablehlo.slice %3896 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %3898 = stablehlo.slice %3896 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %3899 = stablehlo.reshape %3871 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %3900 = stablehlo.broadcast_in_dim %3897, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %3901 = stablehlo.broadcast_in_dim %3898, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %3902 = stablehlo.slice %3899 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %3903 = stablehlo.slice %3899 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %3904 = stablehlo.broadcast_in_dim %3900, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %3905 = stablehlo.multiply %3902, %3904 : tensor<16x32x64xbf16> loc(#loc1719)
    %3906 = stablehlo.broadcast_in_dim %3901, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %3907 = stablehlo.multiply %3903, %3906 : tensor<16x32x64xbf16> loc(#loc1720)
    %3908 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %3909 = stablehlo.multiply %3907, %3908 : tensor<16x32x64xbf16> loc(#loc1721)
    %3910 = stablehlo.subtract %3905, %3909 : tensor<16x32x64xbf16> loc(#loc1722)
    %3911 = stablehlo.broadcast_in_dim %3900, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %3912 = stablehlo.multiply %3903, %3911 : tensor<16x32x64xbf16> loc(#loc1723)
    %3913 = stablehlo.broadcast_in_dim %3901, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %3914 = stablehlo.multiply %3902, %3913 : tensor<16x32x64xbf16> loc(#loc1724)
    %3915 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %3916 = stablehlo.multiply %3914, %3915 : tensor<16x32x64xbf16> loc(#loc1725)
    %3917 = stablehlo.add %3912, %3916 : tensor<16x32x64xbf16> loc(#loc1726)
    %3918 = stablehlo.concatenate %3910, %3917, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %3919 = stablehlo.slice %3899 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %3920 = stablehlo.concatenate %3918, %3919, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %3921 = stablehlo.reshape %3920 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %3922 = stablehlo.reshape %3889 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %3923 = stablehlo.broadcast_in_dim %3897, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %3924 = stablehlo.broadcast_in_dim %3898, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %3925 = stablehlo.slice %3922 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %3926 = stablehlo.slice %3922 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %3927 = stablehlo.broadcast_in_dim %3923, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %3928 = stablehlo.multiply %3925, %3927 : tensor<16x4x64xbf16> loc(#loc1735)
    %3929 = stablehlo.broadcast_in_dim %3924, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %3930 = stablehlo.multiply %3926, %3929 : tensor<16x4x64xbf16> loc(#loc1736)
    %3931 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %3932 = stablehlo.multiply %3930, %3931 : tensor<16x4x64xbf16> loc(#loc1737)
    %3933 = stablehlo.subtract %3928, %3932 : tensor<16x4x64xbf16> loc(#loc1738)
    %3934 = stablehlo.broadcast_in_dim %3923, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %3935 = stablehlo.multiply %3926, %3934 : tensor<16x4x64xbf16> loc(#loc1739)
    %3936 = stablehlo.broadcast_in_dim %3924, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %3937 = stablehlo.multiply %3925, %3936 : tensor<16x4x64xbf16> loc(#loc1740)
    %3938 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %3939 = stablehlo.multiply %3937, %3938 : tensor<16x4x64xbf16> loc(#loc1741)
    %3940 = stablehlo.add %3935, %3939 : tensor<16x4x64xbf16> loc(#loc1742)
    %3941 = stablehlo.concatenate %3933, %3940, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %3942 = stablehlo.slice %3922 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %3943 = stablehlo.concatenate %3941, %3942, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %3944 = stablehlo.reshape %3943 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %3945:2 = call @_jax_attn_func(%arg460, %3921, %3944, %3853, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %3946 = stablehlo.dot_general %3945#1, %arg170, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %3947 = stablehlo.reshape %3946 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %3948 = stablehlo.reshape %3947 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %3949 = stablehlo.convert %3948 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %3950 = stablehlo.convert %3826 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %3951 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %3952 = stablehlo.multiply %3950, %3951 : tensor<16x2048xf32> loc(#loc1751)
    %3953 = stablehlo.add %3949, %3952 : tensor<16x2048xf32> loc(#loc1752)
    %3954 = stablehlo.convert %3953 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %3955 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %3956 = stablehlo.power %3953, %3955 : tensor<16x2048xf32> loc(#loc1754)
    %3957 = stablehlo.reduce(%3956 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %3958 = stablehlo.broadcast_in_dim %3957, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %3959 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %3960 = stablehlo.divide %3958, %3959 : tensor<16x1xf32> loc(#loc1757)
    %3961 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %3962 = stablehlo.add %3960, %3961 : tensor<16x1xf32> loc(#loc1758)
    %3963 = stablehlo.rsqrt %3962 : tensor<16x1xf32> loc(#loc1678)
    %3964 = stablehlo.broadcast_in_dim %3963, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %3965 = stablehlo.multiply %3953, %3964 : tensor<16x2048xf32> loc(#loc1759)
    %3966 = stablehlo.convert %3965 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %3967 = stablehlo.broadcast_in_dim %arg168, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %3968 = stablehlo.broadcast_in_dim %3967, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %3969 = stablehlo.multiply %3966, %3968 : tensor<16x2048xbf16> loc(#loc1761)
    %3970 = stablehlo.dot_general %3969, %arg167, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %3971 = stablehlo.reshape %3970 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %3972 = stablehlo.reshape %3971 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %3973 = call @jax_fused_moe_func_padded(%3969, %arg165, %arg166, %3972) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %3974 = stablehlo.convert %3973 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %3975 = stablehlo.convert %3954 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %3976 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %3977 = stablehlo.multiply %3975, %3976 : tensor<16x2048xf32> loc(#loc1766)
    %3978 = stablehlo.add %3974, %3977 : tensor<16x2048xf32> loc(#loc1767)
    %3979 = stablehlo.convert %3978 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %3980 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %3981 = stablehlo.power %3978, %3980 : tensor<16x2048xf32> loc(#loc1768)
    %3982 = stablehlo.reduce(%3981 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %3983 = stablehlo.broadcast_in_dim %3982, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %3984 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %3985 = stablehlo.divide %3983, %3984 : tensor<16x1xf32> loc(#loc1771)
    %3986 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %3987 = stablehlo.add %3985, %3986 : tensor<16x1xf32> loc(#loc1772)
    %3988 = stablehlo.rsqrt %3987 : tensor<16x1xf32> loc(#loc1678)
    %3989 = stablehlo.broadcast_in_dim %3988, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %3990 = stablehlo.multiply %3978, %3989 : tensor<16x2048xf32> loc(#loc1773)
    %3991 = stablehlo.convert %3990 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %3992 = stablehlo.broadcast_in_dim %arg173, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %3993 = stablehlo.broadcast_in_dim %3992, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %3994 = stablehlo.multiply %3991, %3993 : tensor<16x2048xbf16> loc(#loc1775)
    %3995 = stablehlo.dot_general %3994, %arg181, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %3996 = stablehlo.reshape %3995 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %3997 = stablehlo.slice %3996 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %3998 = stablehlo.reshape %3997 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %3999 = stablehlo.slice %3996 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %4000 = stablehlo.reshape %3999 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %4001 = stablehlo.slice %3996 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %4002 = stablehlo.reshape %4001 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %4003 = stablehlo.concatenate %3998, %4000, %4002, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %4004 = stablehlo.slice %4003 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %4005 = stablehlo.slice %4003 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %4006 = stablehlo.slice %4003 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %4007 = stablehlo.reshape %4004 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %4008 = stablehlo.convert %4007 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %4009 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %4010 = stablehlo.power %4008, %4009 : tensor<16x32x128xf32> loc(#loc1690)
    %4011 = stablehlo.reduce(%4010 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %4012 = stablehlo.broadcast_in_dim %4011, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %4013 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %4014 = stablehlo.divide %4012, %4013 : tensor<16x32x1xf32> loc(#loc1693)
    %4015 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %4016 = stablehlo.add %4014, %4015 : tensor<16x32x1xf32> loc(#loc1694)
    %4017 = stablehlo.rsqrt %4016 : tensor<16x32x1xf32> loc(#loc1678)
    %4018 = stablehlo.broadcast_in_dim %4017, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %4019 = stablehlo.multiply %4008, %4018 : tensor<16x32x128xf32> loc(#loc1695)
    %4020 = stablehlo.convert %4019 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %4021 = stablehlo.broadcast_in_dim %arg180, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %4022 = stablehlo.broadcast_in_dim %4021, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %4023 = stablehlo.multiply %4020, %4022 : tensor<16x32x128xbf16> loc(#loc1697)
    %4024 = stablehlo.reshape %4023 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %4025 = stablehlo.reshape %4005 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %4026 = stablehlo.convert %4025 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %4027 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %4028 = stablehlo.power %4026, %4027 : tensor<16x4x128xf32> loc(#loc1700)
    %4029 = stablehlo.reduce(%4028 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %4030 = stablehlo.broadcast_in_dim %4029, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %4031 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %4032 = stablehlo.divide %4030, %4031 : tensor<16x4x1xf32> loc(#loc1703)
    %4033 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %4034 = stablehlo.add %4032, %4033 : tensor<16x4x1xf32> loc(#loc1704)
    %4035 = stablehlo.rsqrt %4034 : tensor<16x4x1xf32> loc(#loc1678)
    %4036 = stablehlo.broadcast_in_dim %4035, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %4037 = stablehlo.multiply %4026, %4036 : tensor<16x4x128xf32> loc(#loc1705)
    %4038 = stablehlo.convert %4037 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %4039 = stablehlo.broadcast_in_dim %arg178, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %4040 = stablehlo.broadcast_in_dim %4039, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %4041 = stablehlo.multiply %4038, %4040 : tensor<16x4x128xbf16> loc(#loc1707)
    %4042 = stablehlo.reshape %4041 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %4043 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %4044 = stablehlo.compare  LT, %arg484, %4043,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %4045 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %4046 = stablehlo.add %arg484, %4045 : tensor<16xi32> loc(#loc1710)
    %4047 = stablehlo.select %4044, %4046, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %4048 = stablehlo.broadcast_in_dim %4047, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %4049 = "stablehlo.gather"(%arg10, %4048) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %4050 = stablehlo.slice %4049 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %4051 = stablehlo.slice %4049 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %4052 = stablehlo.reshape %4024 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %4053 = stablehlo.broadcast_in_dim %4050, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %4054 = stablehlo.broadcast_in_dim %4051, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %4055 = stablehlo.slice %4052 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %4056 = stablehlo.slice %4052 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %4057 = stablehlo.broadcast_in_dim %4053, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %4058 = stablehlo.multiply %4055, %4057 : tensor<16x32x64xbf16> loc(#loc1719)
    %4059 = stablehlo.broadcast_in_dim %4054, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %4060 = stablehlo.multiply %4056, %4059 : tensor<16x32x64xbf16> loc(#loc1720)
    %4061 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %4062 = stablehlo.multiply %4060, %4061 : tensor<16x32x64xbf16> loc(#loc1721)
    %4063 = stablehlo.subtract %4058, %4062 : tensor<16x32x64xbf16> loc(#loc1722)
    %4064 = stablehlo.broadcast_in_dim %4053, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %4065 = stablehlo.multiply %4056, %4064 : tensor<16x32x64xbf16> loc(#loc1723)
    %4066 = stablehlo.broadcast_in_dim %4054, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %4067 = stablehlo.multiply %4055, %4066 : tensor<16x32x64xbf16> loc(#loc1724)
    %4068 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %4069 = stablehlo.multiply %4067, %4068 : tensor<16x32x64xbf16> loc(#loc1725)
    %4070 = stablehlo.add %4065, %4069 : tensor<16x32x64xbf16> loc(#loc1726)
    %4071 = stablehlo.concatenate %4063, %4070, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %4072 = stablehlo.slice %4052 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %4073 = stablehlo.concatenate %4071, %4072, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %4074 = stablehlo.reshape %4073 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %4075 = stablehlo.reshape %4042 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %4076 = stablehlo.broadcast_in_dim %4050, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %4077 = stablehlo.broadcast_in_dim %4051, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %4078 = stablehlo.slice %4075 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %4079 = stablehlo.slice %4075 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %4080 = stablehlo.broadcast_in_dim %4076, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %4081 = stablehlo.multiply %4078, %4080 : tensor<16x4x64xbf16> loc(#loc1735)
    %4082 = stablehlo.broadcast_in_dim %4077, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %4083 = stablehlo.multiply %4079, %4082 : tensor<16x4x64xbf16> loc(#loc1736)
    %4084 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %4085 = stablehlo.multiply %4083, %4084 : tensor<16x4x64xbf16> loc(#loc1737)
    %4086 = stablehlo.subtract %4081, %4085 : tensor<16x4x64xbf16> loc(#loc1738)
    %4087 = stablehlo.broadcast_in_dim %4076, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %4088 = stablehlo.multiply %4079, %4087 : tensor<16x4x64xbf16> loc(#loc1739)
    %4089 = stablehlo.broadcast_in_dim %4077, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %4090 = stablehlo.multiply %4078, %4089 : tensor<16x4x64xbf16> loc(#loc1740)
    %4091 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %4092 = stablehlo.multiply %4090, %4091 : tensor<16x4x64xbf16> loc(#loc1741)
    %4093 = stablehlo.add %4088, %4092 : tensor<16x4x64xbf16> loc(#loc1742)
    %4094 = stablehlo.concatenate %4086, %4093, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %4095 = stablehlo.slice %4075 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %4096 = stablehlo.concatenate %4094, %4095, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %4097 = stablehlo.reshape %4096 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %4098:2 = call @_jax_attn_func(%arg461, %4074, %4097, %4006, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %4099 = stablehlo.dot_general %4098#1, %arg179, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %4100 = stablehlo.reshape %4099 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %4101 = stablehlo.reshape %4100 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %4102 = stablehlo.convert %4101 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %4103 = stablehlo.convert %3979 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %4104 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %4105 = stablehlo.multiply %4103, %4104 : tensor<16x2048xf32> loc(#loc1751)
    %4106 = stablehlo.add %4102, %4105 : tensor<16x2048xf32> loc(#loc1752)
    %4107 = stablehlo.convert %4106 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %4108 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %4109 = stablehlo.power %4106, %4108 : tensor<16x2048xf32> loc(#loc1754)
    %4110 = stablehlo.reduce(%4109 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %4111 = stablehlo.broadcast_in_dim %4110, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %4112 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %4113 = stablehlo.divide %4111, %4112 : tensor<16x1xf32> loc(#loc1757)
    %4114 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %4115 = stablehlo.add %4113, %4114 : tensor<16x1xf32> loc(#loc1758)
    %4116 = stablehlo.rsqrt %4115 : tensor<16x1xf32> loc(#loc1678)
    %4117 = stablehlo.broadcast_in_dim %4116, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %4118 = stablehlo.multiply %4106, %4117 : tensor<16x2048xf32> loc(#loc1759)
    %4119 = stablehlo.convert %4118 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %4120 = stablehlo.broadcast_in_dim %arg177, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %4121 = stablehlo.broadcast_in_dim %4120, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %4122 = stablehlo.multiply %4119, %4121 : tensor<16x2048xbf16> loc(#loc1761)
    %4123 = stablehlo.dot_general %4122, %arg176, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %4124 = stablehlo.reshape %4123 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %4125 = stablehlo.reshape %4124 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %4126 = call @jax_fused_moe_func_padded(%4122, %arg174, %arg175, %4125) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %4127 = stablehlo.convert %4126 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %4128 = stablehlo.convert %4107 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %4129 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %4130 = stablehlo.multiply %4128, %4129 : tensor<16x2048xf32> loc(#loc1766)
    %4131 = stablehlo.add %4127, %4130 : tensor<16x2048xf32> loc(#loc1767)
    %4132 = stablehlo.convert %4131 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %4133 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %4134 = stablehlo.power %4131, %4133 : tensor<16x2048xf32> loc(#loc1768)
    %4135 = stablehlo.reduce(%4134 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %4136 = stablehlo.broadcast_in_dim %4135, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %4137 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %4138 = stablehlo.divide %4136, %4137 : tensor<16x1xf32> loc(#loc1771)
    %4139 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %4140 = stablehlo.add %4138, %4139 : tensor<16x1xf32> loc(#loc1772)
    %4141 = stablehlo.rsqrt %4140 : tensor<16x1xf32> loc(#loc1678)
    %4142 = stablehlo.broadcast_in_dim %4141, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %4143 = stablehlo.multiply %4131, %4142 : tensor<16x2048xf32> loc(#loc1773)
    %4144 = stablehlo.convert %4143 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %4145 = stablehlo.broadcast_in_dim %arg182, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %4146 = stablehlo.broadcast_in_dim %4145, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %4147 = stablehlo.multiply %4144, %4146 : tensor<16x2048xbf16> loc(#loc1775)
    %4148 = stablehlo.dot_general %4147, %arg190, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %4149 = stablehlo.reshape %4148 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %4150 = stablehlo.slice %4149 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %4151 = stablehlo.reshape %4150 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %4152 = stablehlo.slice %4149 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %4153 = stablehlo.reshape %4152 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %4154 = stablehlo.slice %4149 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %4155 = stablehlo.reshape %4154 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %4156 = stablehlo.concatenate %4151, %4153, %4155, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %4157 = stablehlo.slice %4156 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %4158 = stablehlo.slice %4156 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %4159 = stablehlo.slice %4156 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %4160 = stablehlo.reshape %4157 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %4161 = stablehlo.convert %4160 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %4162 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %4163 = stablehlo.power %4161, %4162 : tensor<16x32x128xf32> loc(#loc1690)
    %4164 = stablehlo.reduce(%4163 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %4165 = stablehlo.broadcast_in_dim %4164, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %4166 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %4167 = stablehlo.divide %4165, %4166 : tensor<16x32x1xf32> loc(#loc1693)
    %4168 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %4169 = stablehlo.add %4167, %4168 : tensor<16x32x1xf32> loc(#loc1694)
    %4170 = stablehlo.rsqrt %4169 : tensor<16x32x1xf32> loc(#loc1678)
    %4171 = stablehlo.broadcast_in_dim %4170, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %4172 = stablehlo.multiply %4161, %4171 : tensor<16x32x128xf32> loc(#loc1695)
    %4173 = stablehlo.convert %4172 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %4174 = stablehlo.broadcast_in_dim %arg189, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %4175 = stablehlo.broadcast_in_dim %4174, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %4176 = stablehlo.multiply %4173, %4175 : tensor<16x32x128xbf16> loc(#loc1697)
    %4177 = stablehlo.reshape %4176 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %4178 = stablehlo.reshape %4158 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %4179 = stablehlo.convert %4178 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %4180 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %4181 = stablehlo.power %4179, %4180 : tensor<16x4x128xf32> loc(#loc1700)
    %4182 = stablehlo.reduce(%4181 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %4183 = stablehlo.broadcast_in_dim %4182, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %4184 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %4185 = stablehlo.divide %4183, %4184 : tensor<16x4x1xf32> loc(#loc1703)
    %4186 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %4187 = stablehlo.add %4185, %4186 : tensor<16x4x1xf32> loc(#loc1704)
    %4188 = stablehlo.rsqrt %4187 : tensor<16x4x1xf32> loc(#loc1678)
    %4189 = stablehlo.broadcast_in_dim %4188, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %4190 = stablehlo.multiply %4179, %4189 : tensor<16x4x128xf32> loc(#loc1705)
    %4191 = stablehlo.convert %4190 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %4192 = stablehlo.broadcast_in_dim %arg187, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %4193 = stablehlo.broadcast_in_dim %4192, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %4194 = stablehlo.multiply %4191, %4193 : tensor<16x4x128xbf16> loc(#loc1707)
    %4195 = stablehlo.reshape %4194 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %4196 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %4197 = stablehlo.compare  LT, %arg484, %4196,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %4198 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %4199 = stablehlo.add %arg484, %4198 : tensor<16xi32> loc(#loc1710)
    %4200 = stablehlo.select %4197, %4199, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %4201 = stablehlo.broadcast_in_dim %4200, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %4202 = "stablehlo.gather"(%arg10, %4201) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %4203 = stablehlo.slice %4202 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %4204 = stablehlo.slice %4202 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %4205 = stablehlo.reshape %4177 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %4206 = stablehlo.broadcast_in_dim %4203, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %4207 = stablehlo.broadcast_in_dim %4204, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %4208 = stablehlo.slice %4205 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %4209 = stablehlo.slice %4205 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %4210 = stablehlo.broadcast_in_dim %4206, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %4211 = stablehlo.multiply %4208, %4210 : tensor<16x32x64xbf16> loc(#loc1719)
    %4212 = stablehlo.broadcast_in_dim %4207, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %4213 = stablehlo.multiply %4209, %4212 : tensor<16x32x64xbf16> loc(#loc1720)
    %4214 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %4215 = stablehlo.multiply %4213, %4214 : tensor<16x32x64xbf16> loc(#loc1721)
    %4216 = stablehlo.subtract %4211, %4215 : tensor<16x32x64xbf16> loc(#loc1722)
    %4217 = stablehlo.broadcast_in_dim %4206, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %4218 = stablehlo.multiply %4209, %4217 : tensor<16x32x64xbf16> loc(#loc1723)
    %4219 = stablehlo.broadcast_in_dim %4207, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %4220 = stablehlo.multiply %4208, %4219 : tensor<16x32x64xbf16> loc(#loc1724)
    %4221 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %4222 = stablehlo.multiply %4220, %4221 : tensor<16x32x64xbf16> loc(#loc1725)
    %4223 = stablehlo.add %4218, %4222 : tensor<16x32x64xbf16> loc(#loc1726)
    %4224 = stablehlo.concatenate %4216, %4223, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %4225 = stablehlo.slice %4205 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %4226 = stablehlo.concatenate %4224, %4225, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %4227 = stablehlo.reshape %4226 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %4228 = stablehlo.reshape %4195 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %4229 = stablehlo.broadcast_in_dim %4203, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %4230 = stablehlo.broadcast_in_dim %4204, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %4231 = stablehlo.slice %4228 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %4232 = stablehlo.slice %4228 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %4233 = stablehlo.broadcast_in_dim %4229, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %4234 = stablehlo.multiply %4231, %4233 : tensor<16x4x64xbf16> loc(#loc1735)
    %4235 = stablehlo.broadcast_in_dim %4230, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %4236 = stablehlo.multiply %4232, %4235 : tensor<16x4x64xbf16> loc(#loc1736)
    %4237 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %4238 = stablehlo.multiply %4236, %4237 : tensor<16x4x64xbf16> loc(#loc1737)
    %4239 = stablehlo.subtract %4234, %4238 : tensor<16x4x64xbf16> loc(#loc1738)
    %4240 = stablehlo.broadcast_in_dim %4229, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %4241 = stablehlo.multiply %4232, %4240 : tensor<16x4x64xbf16> loc(#loc1739)
    %4242 = stablehlo.broadcast_in_dim %4230, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %4243 = stablehlo.multiply %4231, %4242 : tensor<16x4x64xbf16> loc(#loc1740)
    %4244 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %4245 = stablehlo.multiply %4243, %4244 : tensor<16x4x64xbf16> loc(#loc1741)
    %4246 = stablehlo.add %4241, %4245 : tensor<16x4x64xbf16> loc(#loc1742)
    %4247 = stablehlo.concatenate %4239, %4246, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %4248 = stablehlo.slice %4228 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %4249 = stablehlo.concatenate %4247, %4248, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %4250 = stablehlo.reshape %4249 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %4251:2 = call @_jax_attn_func(%arg462, %4227, %4250, %4159, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %4252 = stablehlo.dot_general %4251#1, %arg188, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %4253 = stablehlo.reshape %4252 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %4254 = stablehlo.reshape %4253 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %4255 = stablehlo.convert %4254 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %4256 = stablehlo.convert %4132 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %4257 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %4258 = stablehlo.multiply %4256, %4257 : tensor<16x2048xf32> loc(#loc1751)
    %4259 = stablehlo.add %4255, %4258 : tensor<16x2048xf32> loc(#loc1752)
    %4260 = stablehlo.convert %4259 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %4261 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %4262 = stablehlo.power %4259, %4261 : tensor<16x2048xf32> loc(#loc1754)
    %4263 = stablehlo.reduce(%4262 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %4264 = stablehlo.broadcast_in_dim %4263, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %4265 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %4266 = stablehlo.divide %4264, %4265 : tensor<16x1xf32> loc(#loc1757)
    %4267 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %4268 = stablehlo.add %4266, %4267 : tensor<16x1xf32> loc(#loc1758)
    %4269 = stablehlo.rsqrt %4268 : tensor<16x1xf32> loc(#loc1678)
    %4270 = stablehlo.broadcast_in_dim %4269, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %4271 = stablehlo.multiply %4259, %4270 : tensor<16x2048xf32> loc(#loc1759)
    %4272 = stablehlo.convert %4271 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %4273 = stablehlo.broadcast_in_dim %arg186, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %4274 = stablehlo.broadcast_in_dim %4273, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %4275 = stablehlo.multiply %4272, %4274 : tensor<16x2048xbf16> loc(#loc1761)
    %4276 = stablehlo.dot_general %4275, %arg185, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %4277 = stablehlo.reshape %4276 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %4278 = stablehlo.reshape %4277 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %4279 = call @jax_fused_moe_func_padded(%4275, %arg183, %arg184, %4278) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %4280 = stablehlo.convert %4279 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %4281 = stablehlo.convert %4260 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %4282 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %4283 = stablehlo.multiply %4281, %4282 : tensor<16x2048xf32> loc(#loc1766)
    %4284 = stablehlo.add %4280, %4283 : tensor<16x2048xf32> loc(#loc1767)
    %4285 = stablehlo.convert %4284 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %4286 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %4287 = stablehlo.power %4284, %4286 : tensor<16x2048xf32> loc(#loc1768)
    %4288 = stablehlo.reduce(%4287 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %4289 = stablehlo.broadcast_in_dim %4288, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %4290 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %4291 = stablehlo.divide %4289, %4290 : tensor<16x1xf32> loc(#loc1771)
    %4292 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %4293 = stablehlo.add %4291, %4292 : tensor<16x1xf32> loc(#loc1772)
    %4294 = stablehlo.rsqrt %4293 : tensor<16x1xf32> loc(#loc1678)
    %4295 = stablehlo.broadcast_in_dim %4294, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %4296 = stablehlo.multiply %4284, %4295 : tensor<16x2048xf32> loc(#loc1773)
    %4297 = stablehlo.convert %4296 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %4298 = stablehlo.broadcast_in_dim %arg191, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %4299 = stablehlo.broadcast_in_dim %4298, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %4300 = stablehlo.multiply %4297, %4299 : tensor<16x2048xbf16> loc(#loc1775)
    %4301 = stablehlo.dot_general %4300, %arg199, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %4302 = stablehlo.reshape %4301 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %4303 = stablehlo.slice %4302 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %4304 = stablehlo.reshape %4303 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %4305 = stablehlo.slice %4302 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %4306 = stablehlo.reshape %4305 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %4307 = stablehlo.slice %4302 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %4308 = stablehlo.reshape %4307 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %4309 = stablehlo.concatenate %4304, %4306, %4308, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %4310 = stablehlo.slice %4309 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %4311 = stablehlo.slice %4309 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %4312 = stablehlo.slice %4309 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %4313 = stablehlo.reshape %4310 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %4314 = stablehlo.convert %4313 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %4315 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %4316 = stablehlo.power %4314, %4315 : tensor<16x32x128xf32> loc(#loc1690)
    %4317 = stablehlo.reduce(%4316 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %4318 = stablehlo.broadcast_in_dim %4317, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %4319 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %4320 = stablehlo.divide %4318, %4319 : tensor<16x32x1xf32> loc(#loc1693)
    %4321 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %4322 = stablehlo.add %4320, %4321 : tensor<16x32x1xf32> loc(#loc1694)
    %4323 = stablehlo.rsqrt %4322 : tensor<16x32x1xf32> loc(#loc1678)
    %4324 = stablehlo.broadcast_in_dim %4323, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %4325 = stablehlo.multiply %4314, %4324 : tensor<16x32x128xf32> loc(#loc1695)
    %4326 = stablehlo.convert %4325 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %4327 = stablehlo.broadcast_in_dim %arg198, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %4328 = stablehlo.broadcast_in_dim %4327, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %4329 = stablehlo.multiply %4326, %4328 : tensor<16x32x128xbf16> loc(#loc1697)
    %4330 = stablehlo.reshape %4329 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %4331 = stablehlo.reshape %4311 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %4332 = stablehlo.convert %4331 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %4333 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %4334 = stablehlo.power %4332, %4333 : tensor<16x4x128xf32> loc(#loc1700)
    %4335 = stablehlo.reduce(%4334 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %4336 = stablehlo.broadcast_in_dim %4335, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %4337 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %4338 = stablehlo.divide %4336, %4337 : tensor<16x4x1xf32> loc(#loc1703)
    %4339 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %4340 = stablehlo.add %4338, %4339 : tensor<16x4x1xf32> loc(#loc1704)
    %4341 = stablehlo.rsqrt %4340 : tensor<16x4x1xf32> loc(#loc1678)
    %4342 = stablehlo.broadcast_in_dim %4341, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %4343 = stablehlo.multiply %4332, %4342 : tensor<16x4x128xf32> loc(#loc1705)
    %4344 = stablehlo.convert %4343 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %4345 = stablehlo.broadcast_in_dim %arg196, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %4346 = stablehlo.broadcast_in_dim %4345, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %4347 = stablehlo.multiply %4344, %4346 : tensor<16x4x128xbf16> loc(#loc1707)
    %4348 = stablehlo.reshape %4347 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %4349 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %4350 = stablehlo.compare  LT, %arg484, %4349,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %4351 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %4352 = stablehlo.add %arg484, %4351 : tensor<16xi32> loc(#loc1710)
    %4353 = stablehlo.select %4350, %4352, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %4354 = stablehlo.broadcast_in_dim %4353, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %4355 = "stablehlo.gather"(%arg10, %4354) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %4356 = stablehlo.slice %4355 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %4357 = stablehlo.slice %4355 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %4358 = stablehlo.reshape %4330 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %4359 = stablehlo.broadcast_in_dim %4356, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %4360 = stablehlo.broadcast_in_dim %4357, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %4361 = stablehlo.slice %4358 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %4362 = stablehlo.slice %4358 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %4363 = stablehlo.broadcast_in_dim %4359, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %4364 = stablehlo.multiply %4361, %4363 : tensor<16x32x64xbf16> loc(#loc1719)
    %4365 = stablehlo.broadcast_in_dim %4360, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %4366 = stablehlo.multiply %4362, %4365 : tensor<16x32x64xbf16> loc(#loc1720)
    %4367 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %4368 = stablehlo.multiply %4366, %4367 : tensor<16x32x64xbf16> loc(#loc1721)
    %4369 = stablehlo.subtract %4364, %4368 : tensor<16x32x64xbf16> loc(#loc1722)
    %4370 = stablehlo.broadcast_in_dim %4359, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %4371 = stablehlo.multiply %4362, %4370 : tensor<16x32x64xbf16> loc(#loc1723)
    %4372 = stablehlo.broadcast_in_dim %4360, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %4373 = stablehlo.multiply %4361, %4372 : tensor<16x32x64xbf16> loc(#loc1724)
    %4374 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %4375 = stablehlo.multiply %4373, %4374 : tensor<16x32x64xbf16> loc(#loc1725)
    %4376 = stablehlo.add %4371, %4375 : tensor<16x32x64xbf16> loc(#loc1726)
    %4377 = stablehlo.concatenate %4369, %4376, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %4378 = stablehlo.slice %4358 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %4379 = stablehlo.concatenate %4377, %4378, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %4380 = stablehlo.reshape %4379 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %4381 = stablehlo.reshape %4348 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %4382 = stablehlo.broadcast_in_dim %4356, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %4383 = stablehlo.broadcast_in_dim %4357, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %4384 = stablehlo.slice %4381 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %4385 = stablehlo.slice %4381 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %4386 = stablehlo.broadcast_in_dim %4382, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %4387 = stablehlo.multiply %4384, %4386 : tensor<16x4x64xbf16> loc(#loc1735)
    %4388 = stablehlo.broadcast_in_dim %4383, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %4389 = stablehlo.multiply %4385, %4388 : tensor<16x4x64xbf16> loc(#loc1736)
    %4390 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %4391 = stablehlo.multiply %4389, %4390 : tensor<16x4x64xbf16> loc(#loc1737)
    %4392 = stablehlo.subtract %4387, %4391 : tensor<16x4x64xbf16> loc(#loc1738)
    %4393 = stablehlo.broadcast_in_dim %4382, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %4394 = stablehlo.multiply %4385, %4393 : tensor<16x4x64xbf16> loc(#loc1739)
    %4395 = stablehlo.broadcast_in_dim %4383, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %4396 = stablehlo.multiply %4384, %4395 : tensor<16x4x64xbf16> loc(#loc1740)
    %4397 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %4398 = stablehlo.multiply %4396, %4397 : tensor<16x4x64xbf16> loc(#loc1741)
    %4399 = stablehlo.add %4394, %4398 : tensor<16x4x64xbf16> loc(#loc1742)
    %4400 = stablehlo.concatenate %4392, %4399, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %4401 = stablehlo.slice %4381 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %4402 = stablehlo.concatenate %4400, %4401, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %4403 = stablehlo.reshape %4402 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %4404:2 = call @_jax_attn_func(%arg463, %4380, %4403, %4312, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %4405 = stablehlo.dot_general %4404#1, %arg197, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %4406 = stablehlo.reshape %4405 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %4407 = stablehlo.reshape %4406 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %4408 = stablehlo.convert %4407 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %4409 = stablehlo.convert %4285 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %4410 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %4411 = stablehlo.multiply %4409, %4410 : tensor<16x2048xf32> loc(#loc1751)
    %4412 = stablehlo.add %4408, %4411 : tensor<16x2048xf32> loc(#loc1752)
    %4413 = stablehlo.convert %4412 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %4414 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %4415 = stablehlo.power %4412, %4414 : tensor<16x2048xf32> loc(#loc1754)
    %4416 = stablehlo.reduce(%4415 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %4417 = stablehlo.broadcast_in_dim %4416, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %4418 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %4419 = stablehlo.divide %4417, %4418 : tensor<16x1xf32> loc(#loc1757)
    %4420 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %4421 = stablehlo.add %4419, %4420 : tensor<16x1xf32> loc(#loc1758)
    %4422 = stablehlo.rsqrt %4421 : tensor<16x1xf32> loc(#loc1678)
    %4423 = stablehlo.broadcast_in_dim %4422, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %4424 = stablehlo.multiply %4412, %4423 : tensor<16x2048xf32> loc(#loc1759)
    %4425 = stablehlo.convert %4424 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %4426 = stablehlo.broadcast_in_dim %arg195, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %4427 = stablehlo.broadcast_in_dim %4426, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %4428 = stablehlo.multiply %4425, %4427 : tensor<16x2048xbf16> loc(#loc1761)
    %4429 = stablehlo.dot_general %4428, %arg194, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %4430 = stablehlo.reshape %4429 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %4431 = stablehlo.reshape %4430 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %4432 = call @jax_fused_moe_func_padded(%4428, %arg192, %arg193, %4431) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %4433 = stablehlo.convert %4432 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %4434 = stablehlo.convert %4413 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %4435 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %4436 = stablehlo.multiply %4434, %4435 : tensor<16x2048xf32> loc(#loc1766)
    %4437 = stablehlo.add %4433, %4436 : tensor<16x2048xf32> loc(#loc1767)
    %4438 = stablehlo.convert %4437 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %4439 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %4440 = stablehlo.power %4437, %4439 : tensor<16x2048xf32> loc(#loc1768)
    %4441 = stablehlo.reduce(%4440 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %4442 = stablehlo.broadcast_in_dim %4441, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %4443 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %4444 = stablehlo.divide %4442, %4443 : tensor<16x1xf32> loc(#loc1771)
    %4445 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %4446 = stablehlo.add %4444, %4445 : tensor<16x1xf32> loc(#loc1772)
    %4447 = stablehlo.rsqrt %4446 : tensor<16x1xf32> loc(#loc1678)
    %4448 = stablehlo.broadcast_in_dim %4447, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %4449 = stablehlo.multiply %4437, %4448 : tensor<16x2048xf32> loc(#loc1773)
    %4450 = stablehlo.convert %4449 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %4451 = stablehlo.broadcast_in_dim %arg200, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %4452 = stablehlo.broadcast_in_dim %4451, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %4453 = stablehlo.multiply %4450, %4452 : tensor<16x2048xbf16> loc(#loc1775)
    %4454 = stablehlo.dot_general %4453, %arg208, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %4455 = stablehlo.reshape %4454 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %4456 = stablehlo.slice %4455 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %4457 = stablehlo.reshape %4456 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %4458 = stablehlo.slice %4455 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %4459 = stablehlo.reshape %4458 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %4460 = stablehlo.slice %4455 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %4461 = stablehlo.reshape %4460 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %4462 = stablehlo.concatenate %4457, %4459, %4461, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %4463 = stablehlo.slice %4462 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %4464 = stablehlo.slice %4462 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %4465 = stablehlo.slice %4462 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %4466 = stablehlo.reshape %4463 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %4467 = stablehlo.convert %4466 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %4468 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %4469 = stablehlo.power %4467, %4468 : tensor<16x32x128xf32> loc(#loc1690)
    %4470 = stablehlo.reduce(%4469 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %4471 = stablehlo.broadcast_in_dim %4470, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %4472 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %4473 = stablehlo.divide %4471, %4472 : tensor<16x32x1xf32> loc(#loc1693)
    %4474 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %4475 = stablehlo.add %4473, %4474 : tensor<16x32x1xf32> loc(#loc1694)
    %4476 = stablehlo.rsqrt %4475 : tensor<16x32x1xf32> loc(#loc1678)
    %4477 = stablehlo.broadcast_in_dim %4476, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %4478 = stablehlo.multiply %4467, %4477 : tensor<16x32x128xf32> loc(#loc1695)
    %4479 = stablehlo.convert %4478 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %4480 = stablehlo.broadcast_in_dim %arg207, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %4481 = stablehlo.broadcast_in_dim %4480, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %4482 = stablehlo.multiply %4479, %4481 : tensor<16x32x128xbf16> loc(#loc1697)
    %4483 = stablehlo.reshape %4482 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %4484 = stablehlo.reshape %4464 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %4485 = stablehlo.convert %4484 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %4486 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %4487 = stablehlo.power %4485, %4486 : tensor<16x4x128xf32> loc(#loc1700)
    %4488 = stablehlo.reduce(%4487 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %4489 = stablehlo.broadcast_in_dim %4488, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %4490 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %4491 = stablehlo.divide %4489, %4490 : tensor<16x4x1xf32> loc(#loc1703)
    %4492 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %4493 = stablehlo.add %4491, %4492 : tensor<16x4x1xf32> loc(#loc1704)
    %4494 = stablehlo.rsqrt %4493 : tensor<16x4x1xf32> loc(#loc1678)
    %4495 = stablehlo.broadcast_in_dim %4494, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %4496 = stablehlo.multiply %4485, %4495 : tensor<16x4x128xf32> loc(#loc1705)
    %4497 = stablehlo.convert %4496 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %4498 = stablehlo.broadcast_in_dim %arg205, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %4499 = stablehlo.broadcast_in_dim %4498, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %4500 = stablehlo.multiply %4497, %4499 : tensor<16x4x128xbf16> loc(#loc1707)
    %4501 = stablehlo.reshape %4500 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %4502 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %4503 = stablehlo.compare  LT, %arg484, %4502,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %4504 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %4505 = stablehlo.add %arg484, %4504 : tensor<16xi32> loc(#loc1710)
    %4506 = stablehlo.select %4503, %4505, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %4507 = stablehlo.broadcast_in_dim %4506, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %4508 = "stablehlo.gather"(%arg10, %4507) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %4509 = stablehlo.slice %4508 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %4510 = stablehlo.slice %4508 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %4511 = stablehlo.reshape %4483 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %4512 = stablehlo.broadcast_in_dim %4509, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %4513 = stablehlo.broadcast_in_dim %4510, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %4514 = stablehlo.slice %4511 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %4515 = stablehlo.slice %4511 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %4516 = stablehlo.broadcast_in_dim %4512, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %4517 = stablehlo.multiply %4514, %4516 : tensor<16x32x64xbf16> loc(#loc1719)
    %4518 = stablehlo.broadcast_in_dim %4513, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %4519 = stablehlo.multiply %4515, %4518 : tensor<16x32x64xbf16> loc(#loc1720)
    %4520 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %4521 = stablehlo.multiply %4519, %4520 : tensor<16x32x64xbf16> loc(#loc1721)
    %4522 = stablehlo.subtract %4517, %4521 : tensor<16x32x64xbf16> loc(#loc1722)
    %4523 = stablehlo.broadcast_in_dim %4512, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %4524 = stablehlo.multiply %4515, %4523 : tensor<16x32x64xbf16> loc(#loc1723)
    %4525 = stablehlo.broadcast_in_dim %4513, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %4526 = stablehlo.multiply %4514, %4525 : tensor<16x32x64xbf16> loc(#loc1724)
    %4527 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %4528 = stablehlo.multiply %4526, %4527 : tensor<16x32x64xbf16> loc(#loc1725)
    %4529 = stablehlo.add %4524, %4528 : tensor<16x32x64xbf16> loc(#loc1726)
    %4530 = stablehlo.concatenate %4522, %4529, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %4531 = stablehlo.slice %4511 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %4532 = stablehlo.concatenate %4530, %4531, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %4533 = stablehlo.reshape %4532 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %4534 = stablehlo.reshape %4501 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %4535 = stablehlo.broadcast_in_dim %4509, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %4536 = stablehlo.broadcast_in_dim %4510, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %4537 = stablehlo.slice %4534 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %4538 = stablehlo.slice %4534 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %4539 = stablehlo.broadcast_in_dim %4535, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %4540 = stablehlo.multiply %4537, %4539 : tensor<16x4x64xbf16> loc(#loc1735)
    %4541 = stablehlo.broadcast_in_dim %4536, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %4542 = stablehlo.multiply %4538, %4541 : tensor<16x4x64xbf16> loc(#loc1736)
    %4543 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %4544 = stablehlo.multiply %4542, %4543 : tensor<16x4x64xbf16> loc(#loc1737)
    %4545 = stablehlo.subtract %4540, %4544 : tensor<16x4x64xbf16> loc(#loc1738)
    %4546 = stablehlo.broadcast_in_dim %4535, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %4547 = stablehlo.multiply %4538, %4546 : tensor<16x4x64xbf16> loc(#loc1739)
    %4548 = stablehlo.broadcast_in_dim %4536, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %4549 = stablehlo.multiply %4537, %4548 : tensor<16x4x64xbf16> loc(#loc1740)
    %4550 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %4551 = stablehlo.multiply %4549, %4550 : tensor<16x4x64xbf16> loc(#loc1741)
    %4552 = stablehlo.add %4547, %4551 : tensor<16x4x64xbf16> loc(#loc1742)
    %4553 = stablehlo.concatenate %4545, %4552, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %4554 = stablehlo.slice %4534 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %4555 = stablehlo.concatenate %4553, %4554, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %4556 = stablehlo.reshape %4555 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %4557:2 = call @_jax_attn_func(%arg464, %4533, %4556, %4465, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %4558 = stablehlo.dot_general %4557#1, %arg206, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %4559 = stablehlo.reshape %4558 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %4560 = stablehlo.reshape %4559 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %4561 = stablehlo.convert %4560 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %4562 = stablehlo.convert %4438 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %4563 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %4564 = stablehlo.multiply %4562, %4563 : tensor<16x2048xf32> loc(#loc1751)
    %4565 = stablehlo.add %4561, %4564 : tensor<16x2048xf32> loc(#loc1752)
    %4566 = stablehlo.convert %4565 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %4567 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %4568 = stablehlo.power %4565, %4567 : tensor<16x2048xf32> loc(#loc1754)
    %4569 = stablehlo.reduce(%4568 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %4570 = stablehlo.broadcast_in_dim %4569, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %4571 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %4572 = stablehlo.divide %4570, %4571 : tensor<16x1xf32> loc(#loc1757)
    %4573 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %4574 = stablehlo.add %4572, %4573 : tensor<16x1xf32> loc(#loc1758)
    %4575 = stablehlo.rsqrt %4574 : tensor<16x1xf32> loc(#loc1678)
    %4576 = stablehlo.broadcast_in_dim %4575, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %4577 = stablehlo.multiply %4565, %4576 : tensor<16x2048xf32> loc(#loc1759)
    %4578 = stablehlo.convert %4577 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %4579 = stablehlo.broadcast_in_dim %arg204, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %4580 = stablehlo.broadcast_in_dim %4579, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %4581 = stablehlo.multiply %4578, %4580 : tensor<16x2048xbf16> loc(#loc1761)
    %4582 = stablehlo.dot_general %4581, %arg203, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %4583 = stablehlo.reshape %4582 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %4584 = stablehlo.reshape %4583 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %4585 = call @jax_fused_moe_func_padded(%4581, %arg201, %arg202, %4584) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %4586 = stablehlo.convert %4585 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %4587 = stablehlo.convert %4566 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %4588 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %4589 = stablehlo.multiply %4587, %4588 : tensor<16x2048xf32> loc(#loc1766)
    %4590 = stablehlo.add %4586, %4589 : tensor<16x2048xf32> loc(#loc1767)
    %4591 = stablehlo.convert %4590 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %4592 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %4593 = stablehlo.power %4590, %4592 : tensor<16x2048xf32> loc(#loc1768)
    %4594 = stablehlo.reduce(%4593 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %4595 = stablehlo.broadcast_in_dim %4594, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %4596 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %4597 = stablehlo.divide %4595, %4596 : tensor<16x1xf32> loc(#loc1771)
    %4598 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %4599 = stablehlo.add %4597, %4598 : tensor<16x1xf32> loc(#loc1772)
    %4600 = stablehlo.rsqrt %4599 : tensor<16x1xf32> loc(#loc1678)
    %4601 = stablehlo.broadcast_in_dim %4600, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %4602 = stablehlo.multiply %4590, %4601 : tensor<16x2048xf32> loc(#loc1773)
    %4603 = stablehlo.convert %4602 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %4604 = stablehlo.broadcast_in_dim %arg218, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %4605 = stablehlo.broadcast_in_dim %4604, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %4606 = stablehlo.multiply %4603, %4605 : tensor<16x2048xbf16> loc(#loc1775)
    %4607 = stablehlo.dot_general %4606, %arg226, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %4608 = stablehlo.reshape %4607 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %4609 = stablehlo.slice %4608 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %4610 = stablehlo.reshape %4609 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %4611 = stablehlo.slice %4608 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %4612 = stablehlo.reshape %4611 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %4613 = stablehlo.slice %4608 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %4614 = stablehlo.reshape %4613 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %4615 = stablehlo.concatenate %4610, %4612, %4614, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %4616 = stablehlo.slice %4615 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %4617 = stablehlo.slice %4615 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %4618 = stablehlo.slice %4615 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %4619 = stablehlo.reshape %4616 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %4620 = stablehlo.convert %4619 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %4621 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %4622 = stablehlo.power %4620, %4621 : tensor<16x32x128xf32> loc(#loc1690)
    %4623 = stablehlo.reduce(%4622 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %4624 = stablehlo.broadcast_in_dim %4623, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %4625 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %4626 = stablehlo.divide %4624, %4625 : tensor<16x32x1xf32> loc(#loc1693)
    %4627 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %4628 = stablehlo.add %4626, %4627 : tensor<16x32x1xf32> loc(#loc1694)
    %4629 = stablehlo.rsqrt %4628 : tensor<16x32x1xf32> loc(#loc1678)
    %4630 = stablehlo.broadcast_in_dim %4629, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %4631 = stablehlo.multiply %4620, %4630 : tensor<16x32x128xf32> loc(#loc1695)
    %4632 = stablehlo.convert %4631 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %4633 = stablehlo.broadcast_in_dim %arg225, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %4634 = stablehlo.broadcast_in_dim %4633, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %4635 = stablehlo.multiply %4632, %4634 : tensor<16x32x128xbf16> loc(#loc1697)
    %4636 = stablehlo.reshape %4635 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %4637 = stablehlo.reshape %4617 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %4638 = stablehlo.convert %4637 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %4639 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %4640 = stablehlo.power %4638, %4639 : tensor<16x4x128xf32> loc(#loc1700)
    %4641 = stablehlo.reduce(%4640 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %4642 = stablehlo.broadcast_in_dim %4641, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %4643 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %4644 = stablehlo.divide %4642, %4643 : tensor<16x4x1xf32> loc(#loc1703)
    %4645 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %4646 = stablehlo.add %4644, %4645 : tensor<16x4x1xf32> loc(#loc1704)
    %4647 = stablehlo.rsqrt %4646 : tensor<16x4x1xf32> loc(#loc1678)
    %4648 = stablehlo.broadcast_in_dim %4647, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %4649 = stablehlo.multiply %4638, %4648 : tensor<16x4x128xf32> loc(#loc1705)
    %4650 = stablehlo.convert %4649 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %4651 = stablehlo.broadcast_in_dim %arg223, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %4652 = stablehlo.broadcast_in_dim %4651, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %4653 = stablehlo.multiply %4650, %4652 : tensor<16x4x128xbf16> loc(#loc1707)
    %4654 = stablehlo.reshape %4653 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %4655 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %4656 = stablehlo.compare  LT, %arg484, %4655,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %4657 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %4658 = stablehlo.add %arg484, %4657 : tensor<16xi32> loc(#loc1710)
    %4659 = stablehlo.select %4656, %4658, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %4660 = stablehlo.broadcast_in_dim %4659, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %4661 = "stablehlo.gather"(%arg10, %4660) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %4662 = stablehlo.slice %4661 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %4663 = stablehlo.slice %4661 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %4664 = stablehlo.reshape %4636 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %4665 = stablehlo.broadcast_in_dim %4662, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %4666 = stablehlo.broadcast_in_dim %4663, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %4667 = stablehlo.slice %4664 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %4668 = stablehlo.slice %4664 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %4669 = stablehlo.broadcast_in_dim %4665, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %4670 = stablehlo.multiply %4667, %4669 : tensor<16x32x64xbf16> loc(#loc1719)
    %4671 = stablehlo.broadcast_in_dim %4666, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %4672 = stablehlo.multiply %4668, %4671 : tensor<16x32x64xbf16> loc(#loc1720)
    %4673 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %4674 = stablehlo.multiply %4672, %4673 : tensor<16x32x64xbf16> loc(#loc1721)
    %4675 = stablehlo.subtract %4670, %4674 : tensor<16x32x64xbf16> loc(#loc1722)
    %4676 = stablehlo.broadcast_in_dim %4665, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %4677 = stablehlo.multiply %4668, %4676 : tensor<16x32x64xbf16> loc(#loc1723)
    %4678 = stablehlo.broadcast_in_dim %4666, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %4679 = stablehlo.multiply %4667, %4678 : tensor<16x32x64xbf16> loc(#loc1724)
    %4680 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %4681 = stablehlo.multiply %4679, %4680 : tensor<16x32x64xbf16> loc(#loc1725)
    %4682 = stablehlo.add %4677, %4681 : tensor<16x32x64xbf16> loc(#loc1726)
    %4683 = stablehlo.concatenate %4675, %4682, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %4684 = stablehlo.slice %4664 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %4685 = stablehlo.concatenate %4683, %4684, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %4686 = stablehlo.reshape %4685 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %4687 = stablehlo.reshape %4654 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %4688 = stablehlo.broadcast_in_dim %4662, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %4689 = stablehlo.broadcast_in_dim %4663, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %4690 = stablehlo.slice %4687 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %4691 = stablehlo.slice %4687 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %4692 = stablehlo.broadcast_in_dim %4688, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %4693 = stablehlo.multiply %4690, %4692 : tensor<16x4x64xbf16> loc(#loc1735)
    %4694 = stablehlo.broadcast_in_dim %4689, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %4695 = stablehlo.multiply %4691, %4694 : tensor<16x4x64xbf16> loc(#loc1736)
    %4696 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %4697 = stablehlo.multiply %4695, %4696 : tensor<16x4x64xbf16> loc(#loc1737)
    %4698 = stablehlo.subtract %4693, %4697 : tensor<16x4x64xbf16> loc(#loc1738)
    %4699 = stablehlo.broadcast_in_dim %4688, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %4700 = stablehlo.multiply %4691, %4699 : tensor<16x4x64xbf16> loc(#loc1739)
    %4701 = stablehlo.broadcast_in_dim %4689, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %4702 = stablehlo.multiply %4690, %4701 : tensor<16x4x64xbf16> loc(#loc1740)
    %4703 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %4704 = stablehlo.multiply %4702, %4703 : tensor<16x4x64xbf16> loc(#loc1741)
    %4705 = stablehlo.add %4700, %4704 : tensor<16x4x64xbf16> loc(#loc1742)
    %4706 = stablehlo.concatenate %4698, %4705, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %4707 = stablehlo.slice %4687 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %4708 = stablehlo.concatenate %4706, %4707, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %4709 = stablehlo.reshape %4708 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %4710:2 = call @_jax_attn_func(%arg465, %4686, %4709, %4618, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %4711 = stablehlo.dot_general %4710#1, %arg224, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %4712 = stablehlo.reshape %4711 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %4713 = stablehlo.reshape %4712 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %4714 = stablehlo.convert %4713 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %4715 = stablehlo.convert %4591 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %4716 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %4717 = stablehlo.multiply %4715, %4716 : tensor<16x2048xf32> loc(#loc1751)
    %4718 = stablehlo.add %4714, %4717 : tensor<16x2048xf32> loc(#loc1752)
    %4719 = stablehlo.convert %4718 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %4720 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %4721 = stablehlo.power %4718, %4720 : tensor<16x2048xf32> loc(#loc1754)
    %4722 = stablehlo.reduce(%4721 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %4723 = stablehlo.broadcast_in_dim %4722, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %4724 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %4725 = stablehlo.divide %4723, %4724 : tensor<16x1xf32> loc(#loc1757)
    %4726 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %4727 = stablehlo.add %4725, %4726 : tensor<16x1xf32> loc(#loc1758)
    %4728 = stablehlo.rsqrt %4727 : tensor<16x1xf32> loc(#loc1678)
    %4729 = stablehlo.broadcast_in_dim %4728, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %4730 = stablehlo.multiply %4718, %4729 : tensor<16x2048xf32> loc(#loc1759)
    %4731 = stablehlo.convert %4730 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %4732 = stablehlo.broadcast_in_dim %arg222, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %4733 = stablehlo.broadcast_in_dim %4732, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %4734 = stablehlo.multiply %4731, %4733 : tensor<16x2048xbf16> loc(#loc1761)
    %4735 = stablehlo.dot_general %4734, %arg221, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %4736 = stablehlo.reshape %4735 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %4737 = stablehlo.reshape %4736 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %4738 = call @jax_fused_moe_func_padded(%4734, %arg219, %arg220, %4737) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %4739 = stablehlo.convert %4738 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %4740 = stablehlo.convert %4719 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %4741 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %4742 = stablehlo.multiply %4740, %4741 : tensor<16x2048xf32> loc(#loc1766)
    %4743 = stablehlo.add %4739, %4742 : tensor<16x2048xf32> loc(#loc1767)
    %4744 = stablehlo.convert %4743 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %4745 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %4746 = stablehlo.power %4743, %4745 : tensor<16x2048xf32> loc(#loc1768)
    %4747 = stablehlo.reduce(%4746 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %4748 = stablehlo.broadcast_in_dim %4747, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %4749 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %4750 = stablehlo.divide %4748, %4749 : tensor<16x1xf32> loc(#loc1771)
    %4751 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %4752 = stablehlo.add %4750, %4751 : tensor<16x1xf32> loc(#loc1772)
    %4753 = stablehlo.rsqrt %4752 : tensor<16x1xf32> loc(#loc1678)
    %4754 = stablehlo.broadcast_in_dim %4753, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %4755 = stablehlo.multiply %4743, %4754 : tensor<16x2048xf32> loc(#loc1773)
    %4756 = stablehlo.convert %4755 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %4757 = stablehlo.broadcast_in_dim %arg227, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %4758 = stablehlo.broadcast_in_dim %4757, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %4759 = stablehlo.multiply %4756, %4758 : tensor<16x2048xbf16> loc(#loc1775)
    %4760 = stablehlo.dot_general %4759, %arg235, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %4761 = stablehlo.reshape %4760 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %4762 = stablehlo.slice %4761 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %4763 = stablehlo.reshape %4762 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %4764 = stablehlo.slice %4761 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %4765 = stablehlo.reshape %4764 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %4766 = stablehlo.slice %4761 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %4767 = stablehlo.reshape %4766 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %4768 = stablehlo.concatenate %4763, %4765, %4767, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %4769 = stablehlo.slice %4768 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %4770 = stablehlo.slice %4768 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %4771 = stablehlo.slice %4768 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %4772 = stablehlo.reshape %4769 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %4773 = stablehlo.convert %4772 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %4774 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %4775 = stablehlo.power %4773, %4774 : tensor<16x32x128xf32> loc(#loc1690)
    %4776 = stablehlo.reduce(%4775 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %4777 = stablehlo.broadcast_in_dim %4776, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %4778 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %4779 = stablehlo.divide %4777, %4778 : tensor<16x32x1xf32> loc(#loc1693)
    %4780 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %4781 = stablehlo.add %4779, %4780 : tensor<16x32x1xf32> loc(#loc1694)
    %4782 = stablehlo.rsqrt %4781 : tensor<16x32x1xf32> loc(#loc1678)
    %4783 = stablehlo.broadcast_in_dim %4782, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %4784 = stablehlo.multiply %4773, %4783 : tensor<16x32x128xf32> loc(#loc1695)
    %4785 = stablehlo.convert %4784 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %4786 = stablehlo.broadcast_in_dim %arg234, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %4787 = stablehlo.broadcast_in_dim %4786, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %4788 = stablehlo.multiply %4785, %4787 : tensor<16x32x128xbf16> loc(#loc1697)
    %4789 = stablehlo.reshape %4788 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %4790 = stablehlo.reshape %4770 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %4791 = stablehlo.convert %4790 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %4792 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %4793 = stablehlo.power %4791, %4792 : tensor<16x4x128xf32> loc(#loc1700)
    %4794 = stablehlo.reduce(%4793 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %4795 = stablehlo.broadcast_in_dim %4794, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %4796 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %4797 = stablehlo.divide %4795, %4796 : tensor<16x4x1xf32> loc(#loc1703)
    %4798 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %4799 = stablehlo.add %4797, %4798 : tensor<16x4x1xf32> loc(#loc1704)
    %4800 = stablehlo.rsqrt %4799 : tensor<16x4x1xf32> loc(#loc1678)
    %4801 = stablehlo.broadcast_in_dim %4800, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %4802 = stablehlo.multiply %4791, %4801 : tensor<16x4x128xf32> loc(#loc1705)
    %4803 = stablehlo.convert %4802 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %4804 = stablehlo.broadcast_in_dim %arg232, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %4805 = stablehlo.broadcast_in_dim %4804, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %4806 = stablehlo.multiply %4803, %4805 : tensor<16x4x128xbf16> loc(#loc1707)
    %4807 = stablehlo.reshape %4806 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %4808 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %4809 = stablehlo.compare  LT, %arg484, %4808,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %4810 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %4811 = stablehlo.add %arg484, %4810 : tensor<16xi32> loc(#loc1710)
    %4812 = stablehlo.select %4809, %4811, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %4813 = stablehlo.broadcast_in_dim %4812, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %4814 = "stablehlo.gather"(%arg10, %4813) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %4815 = stablehlo.slice %4814 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %4816 = stablehlo.slice %4814 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %4817 = stablehlo.reshape %4789 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %4818 = stablehlo.broadcast_in_dim %4815, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %4819 = stablehlo.broadcast_in_dim %4816, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %4820 = stablehlo.slice %4817 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %4821 = stablehlo.slice %4817 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %4822 = stablehlo.broadcast_in_dim %4818, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %4823 = stablehlo.multiply %4820, %4822 : tensor<16x32x64xbf16> loc(#loc1719)
    %4824 = stablehlo.broadcast_in_dim %4819, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %4825 = stablehlo.multiply %4821, %4824 : tensor<16x32x64xbf16> loc(#loc1720)
    %4826 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %4827 = stablehlo.multiply %4825, %4826 : tensor<16x32x64xbf16> loc(#loc1721)
    %4828 = stablehlo.subtract %4823, %4827 : tensor<16x32x64xbf16> loc(#loc1722)
    %4829 = stablehlo.broadcast_in_dim %4818, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %4830 = stablehlo.multiply %4821, %4829 : tensor<16x32x64xbf16> loc(#loc1723)
    %4831 = stablehlo.broadcast_in_dim %4819, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %4832 = stablehlo.multiply %4820, %4831 : tensor<16x32x64xbf16> loc(#loc1724)
    %4833 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %4834 = stablehlo.multiply %4832, %4833 : tensor<16x32x64xbf16> loc(#loc1725)
    %4835 = stablehlo.add %4830, %4834 : tensor<16x32x64xbf16> loc(#loc1726)
    %4836 = stablehlo.concatenate %4828, %4835, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %4837 = stablehlo.slice %4817 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %4838 = stablehlo.concatenate %4836, %4837, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %4839 = stablehlo.reshape %4838 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %4840 = stablehlo.reshape %4807 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %4841 = stablehlo.broadcast_in_dim %4815, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %4842 = stablehlo.broadcast_in_dim %4816, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %4843 = stablehlo.slice %4840 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %4844 = stablehlo.slice %4840 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %4845 = stablehlo.broadcast_in_dim %4841, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %4846 = stablehlo.multiply %4843, %4845 : tensor<16x4x64xbf16> loc(#loc1735)
    %4847 = stablehlo.broadcast_in_dim %4842, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %4848 = stablehlo.multiply %4844, %4847 : tensor<16x4x64xbf16> loc(#loc1736)
    %4849 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %4850 = stablehlo.multiply %4848, %4849 : tensor<16x4x64xbf16> loc(#loc1737)
    %4851 = stablehlo.subtract %4846, %4850 : tensor<16x4x64xbf16> loc(#loc1738)
    %4852 = stablehlo.broadcast_in_dim %4841, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %4853 = stablehlo.multiply %4844, %4852 : tensor<16x4x64xbf16> loc(#loc1739)
    %4854 = stablehlo.broadcast_in_dim %4842, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %4855 = stablehlo.multiply %4843, %4854 : tensor<16x4x64xbf16> loc(#loc1740)
    %4856 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %4857 = stablehlo.multiply %4855, %4856 : tensor<16x4x64xbf16> loc(#loc1741)
    %4858 = stablehlo.add %4853, %4857 : tensor<16x4x64xbf16> loc(#loc1742)
    %4859 = stablehlo.concatenate %4851, %4858, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %4860 = stablehlo.slice %4840 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %4861 = stablehlo.concatenate %4859, %4860, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %4862 = stablehlo.reshape %4861 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %4863:2 = call @_jax_attn_func(%arg466, %4839, %4862, %4771, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %4864 = stablehlo.dot_general %4863#1, %arg233, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %4865 = stablehlo.reshape %4864 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %4866 = stablehlo.reshape %4865 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %4867 = stablehlo.convert %4866 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %4868 = stablehlo.convert %4744 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %4869 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %4870 = stablehlo.multiply %4868, %4869 : tensor<16x2048xf32> loc(#loc1751)
    %4871 = stablehlo.add %4867, %4870 : tensor<16x2048xf32> loc(#loc1752)
    %4872 = stablehlo.convert %4871 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %4873 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %4874 = stablehlo.power %4871, %4873 : tensor<16x2048xf32> loc(#loc1754)
    %4875 = stablehlo.reduce(%4874 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %4876 = stablehlo.broadcast_in_dim %4875, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %4877 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %4878 = stablehlo.divide %4876, %4877 : tensor<16x1xf32> loc(#loc1757)
    %4879 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %4880 = stablehlo.add %4878, %4879 : tensor<16x1xf32> loc(#loc1758)
    %4881 = stablehlo.rsqrt %4880 : tensor<16x1xf32> loc(#loc1678)
    %4882 = stablehlo.broadcast_in_dim %4881, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %4883 = stablehlo.multiply %4871, %4882 : tensor<16x2048xf32> loc(#loc1759)
    %4884 = stablehlo.convert %4883 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %4885 = stablehlo.broadcast_in_dim %arg231, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %4886 = stablehlo.broadcast_in_dim %4885, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %4887 = stablehlo.multiply %4884, %4886 : tensor<16x2048xbf16> loc(#loc1761)
    %4888 = stablehlo.dot_general %4887, %arg230, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %4889 = stablehlo.reshape %4888 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %4890 = stablehlo.reshape %4889 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %4891 = call @jax_fused_moe_func_padded(%4887, %arg228, %arg229, %4890) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %4892 = stablehlo.convert %4891 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %4893 = stablehlo.convert %4872 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %4894 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %4895 = stablehlo.multiply %4893, %4894 : tensor<16x2048xf32> loc(#loc1766)
    %4896 = stablehlo.add %4892, %4895 : tensor<16x2048xf32> loc(#loc1767)
    %4897 = stablehlo.convert %4896 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %4898 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %4899 = stablehlo.power %4896, %4898 : tensor<16x2048xf32> loc(#loc1768)
    %4900 = stablehlo.reduce(%4899 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %4901 = stablehlo.broadcast_in_dim %4900, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %4902 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %4903 = stablehlo.divide %4901, %4902 : tensor<16x1xf32> loc(#loc1771)
    %4904 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %4905 = stablehlo.add %4903, %4904 : tensor<16x1xf32> loc(#loc1772)
    %4906 = stablehlo.rsqrt %4905 : tensor<16x1xf32> loc(#loc1678)
    %4907 = stablehlo.broadcast_in_dim %4906, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %4908 = stablehlo.multiply %4896, %4907 : tensor<16x2048xf32> loc(#loc1773)
    %4909 = stablehlo.convert %4908 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %4910 = stablehlo.broadcast_in_dim %arg236, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %4911 = stablehlo.broadcast_in_dim %4910, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %4912 = stablehlo.multiply %4909, %4911 : tensor<16x2048xbf16> loc(#loc1775)
    %4913 = stablehlo.dot_general %4912, %arg244, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %4914 = stablehlo.reshape %4913 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %4915 = stablehlo.slice %4914 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %4916 = stablehlo.reshape %4915 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %4917 = stablehlo.slice %4914 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %4918 = stablehlo.reshape %4917 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %4919 = stablehlo.slice %4914 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %4920 = stablehlo.reshape %4919 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %4921 = stablehlo.concatenate %4916, %4918, %4920, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %4922 = stablehlo.slice %4921 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %4923 = stablehlo.slice %4921 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %4924 = stablehlo.slice %4921 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %4925 = stablehlo.reshape %4922 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %4926 = stablehlo.convert %4925 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %4927 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %4928 = stablehlo.power %4926, %4927 : tensor<16x32x128xf32> loc(#loc1690)
    %4929 = stablehlo.reduce(%4928 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %4930 = stablehlo.broadcast_in_dim %4929, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %4931 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %4932 = stablehlo.divide %4930, %4931 : tensor<16x32x1xf32> loc(#loc1693)
    %4933 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %4934 = stablehlo.add %4932, %4933 : tensor<16x32x1xf32> loc(#loc1694)
    %4935 = stablehlo.rsqrt %4934 : tensor<16x32x1xf32> loc(#loc1678)
    %4936 = stablehlo.broadcast_in_dim %4935, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %4937 = stablehlo.multiply %4926, %4936 : tensor<16x32x128xf32> loc(#loc1695)
    %4938 = stablehlo.convert %4937 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %4939 = stablehlo.broadcast_in_dim %arg243, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %4940 = stablehlo.broadcast_in_dim %4939, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %4941 = stablehlo.multiply %4938, %4940 : tensor<16x32x128xbf16> loc(#loc1697)
    %4942 = stablehlo.reshape %4941 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %4943 = stablehlo.reshape %4923 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %4944 = stablehlo.convert %4943 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %4945 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %4946 = stablehlo.power %4944, %4945 : tensor<16x4x128xf32> loc(#loc1700)
    %4947 = stablehlo.reduce(%4946 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %4948 = stablehlo.broadcast_in_dim %4947, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %4949 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %4950 = stablehlo.divide %4948, %4949 : tensor<16x4x1xf32> loc(#loc1703)
    %4951 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %4952 = stablehlo.add %4950, %4951 : tensor<16x4x1xf32> loc(#loc1704)
    %4953 = stablehlo.rsqrt %4952 : tensor<16x4x1xf32> loc(#loc1678)
    %4954 = stablehlo.broadcast_in_dim %4953, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %4955 = stablehlo.multiply %4944, %4954 : tensor<16x4x128xf32> loc(#loc1705)
    %4956 = stablehlo.convert %4955 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %4957 = stablehlo.broadcast_in_dim %arg241, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %4958 = stablehlo.broadcast_in_dim %4957, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %4959 = stablehlo.multiply %4956, %4958 : tensor<16x4x128xbf16> loc(#loc1707)
    %4960 = stablehlo.reshape %4959 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %4961 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %4962 = stablehlo.compare  LT, %arg484, %4961,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %4963 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %4964 = stablehlo.add %arg484, %4963 : tensor<16xi32> loc(#loc1710)
    %4965 = stablehlo.select %4962, %4964, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %4966 = stablehlo.broadcast_in_dim %4965, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %4967 = "stablehlo.gather"(%arg10, %4966) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %4968 = stablehlo.slice %4967 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %4969 = stablehlo.slice %4967 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %4970 = stablehlo.reshape %4942 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %4971 = stablehlo.broadcast_in_dim %4968, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %4972 = stablehlo.broadcast_in_dim %4969, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %4973 = stablehlo.slice %4970 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %4974 = stablehlo.slice %4970 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %4975 = stablehlo.broadcast_in_dim %4971, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %4976 = stablehlo.multiply %4973, %4975 : tensor<16x32x64xbf16> loc(#loc1719)
    %4977 = stablehlo.broadcast_in_dim %4972, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %4978 = stablehlo.multiply %4974, %4977 : tensor<16x32x64xbf16> loc(#loc1720)
    %4979 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %4980 = stablehlo.multiply %4978, %4979 : tensor<16x32x64xbf16> loc(#loc1721)
    %4981 = stablehlo.subtract %4976, %4980 : tensor<16x32x64xbf16> loc(#loc1722)
    %4982 = stablehlo.broadcast_in_dim %4971, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %4983 = stablehlo.multiply %4974, %4982 : tensor<16x32x64xbf16> loc(#loc1723)
    %4984 = stablehlo.broadcast_in_dim %4972, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %4985 = stablehlo.multiply %4973, %4984 : tensor<16x32x64xbf16> loc(#loc1724)
    %4986 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %4987 = stablehlo.multiply %4985, %4986 : tensor<16x32x64xbf16> loc(#loc1725)
    %4988 = stablehlo.add %4983, %4987 : tensor<16x32x64xbf16> loc(#loc1726)
    %4989 = stablehlo.concatenate %4981, %4988, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %4990 = stablehlo.slice %4970 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %4991 = stablehlo.concatenate %4989, %4990, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %4992 = stablehlo.reshape %4991 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %4993 = stablehlo.reshape %4960 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %4994 = stablehlo.broadcast_in_dim %4968, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %4995 = stablehlo.broadcast_in_dim %4969, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %4996 = stablehlo.slice %4993 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %4997 = stablehlo.slice %4993 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %4998 = stablehlo.broadcast_in_dim %4994, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %4999 = stablehlo.multiply %4996, %4998 : tensor<16x4x64xbf16> loc(#loc1735)
    %5000 = stablehlo.broadcast_in_dim %4995, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %5001 = stablehlo.multiply %4997, %5000 : tensor<16x4x64xbf16> loc(#loc1736)
    %5002 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %5003 = stablehlo.multiply %5001, %5002 : tensor<16x4x64xbf16> loc(#loc1737)
    %5004 = stablehlo.subtract %4999, %5003 : tensor<16x4x64xbf16> loc(#loc1738)
    %5005 = stablehlo.broadcast_in_dim %4994, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %5006 = stablehlo.multiply %4997, %5005 : tensor<16x4x64xbf16> loc(#loc1739)
    %5007 = stablehlo.broadcast_in_dim %4995, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %5008 = stablehlo.multiply %4996, %5007 : tensor<16x4x64xbf16> loc(#loc1740)
    %5009 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %5010 = stablehlo.multiply %5008, %5009 : tensor<16x4x64xbf16> loc(#loc1741)
    %5011 = stablehlo.add %5006, %5010 : tensor<16x4x64xbf16> loc(#loc1742)
    %5012 = stablehlo.concatenate %5004, %5011, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %5013 = stablehlo.slice %4993 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %5014 = stablehlo.concatenate %5012, %5013, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %5015 = stablehlo.reshape %5014 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %5016:2 = call @_jax_attn_func(%arg467, %4992, %5015, %4924, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %5017 = stablehlo.dot_general %5016#1, %arg242, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %5018 = stablehlo.reshape %5017 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %5019 = stablehlo.reshape %5018 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %5020 = stablehlo.convert %5019 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %5021 = stablehlo.convert %4897 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %5022 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %5023 = stablehlo.multiply %5021, %5022 : tensor<16x2048xf32> loc(#loc1751)
    %5024 = stablehlo.add %5020, %5023 : tensor<16x2048xf32> loc(#loc1752)
    %5025 = stablehlo.convert %5024 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %5026 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %5027 = stablehlo.power %5024, %5026 : tensor<16x2048xf32> loc(#loc1754)
    %5028 = stablehlo.reduce(%5027 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %5029 = stablehlo.broadcast_in_dim %5028, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %5030 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %5031 = stablehlo.divide %5029, %5030 : tensor<16x1xf32> loc(#loc1757)
    %5032 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %5033 = stablehlo.add %5031, %5032 : tensor<16x1xf32> loc(#loc1758)
    %5034 = stablehlo.rsqrt %5033 : tensor<16x1xf32> loc(#loc1678)
    %5035 = stablehlo.broadcast_in_dim %5034, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %5036 = stablehlo.multiply %5024, %5035 : tensor<16x2048xf32> loc(#loc1759)
    %5037 = stablehlo.convert %5036 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %5038 = stablehlo.broadcast_in_dim %arg240, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %5039 = stablehlo.broadcast_in_dim %5038, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %5040 = stablehlo.multiply %5037, %5039 : tensor<16x2048xbf16> loc(#loc1761)
    %5041 = stablehlo.dot_general %5040, %arg239, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %5042 = stablehlo.reshape %5041 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %5043 = stablehlo.reshape %5042 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %5044 = call @jax_fused_moe_func_padded(%5040, %arg237, %arg238, %5043) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %5045 = stablehlo.convert %5044 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %5046 = stablehlo.convert %5025 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %5047 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %5048 = stablehlo.multiply %5046, %5047 : tensor<16x2048xf32> loc(#loc1766)
    %5049 = stablehlo.add %5045, %5048 : tensor<16x2048xf32> loc(#loc1767)
    %5050 = stablehlo.convert %5049 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %5051 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %5052 = stablehlo.power %5049, %5051 : tensor<16x2048xf32> loc(#loc1768)
    %5053 = stablehlo.reduce(%5052 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %5054 = stablehlo.broadcast_in_dim %5053, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %5055 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %5056 = stablehlo.divide %5054, %5055 : tensor<16x1xf32> loc(#loc1771)
    %5057 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %5058 = stablehlo.add %5056, %5057 : tensor<16x1xf32> loc(#loc1772)
    %5059 = stablehlo.rsqrt %5058 : tensor<16x1xf32> loc(#loc1678)
    %5060 = stablehlo.broadcast_in_dim %5059, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %5061 = stablehlo.multiply %5049, %5060 : tensor<16x2048xf32> loc(#loc1773)
    %5062 = stablehlo.convert %5061 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %5063 = stablehlo.broadcast_in_dim %arg245, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %5064 = stablehlo.broadcast_in_dim %5063, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %5065 = stablehlo.multiply %5062, %5064 : tensor<16x2048xbf16> loc(#loc1775)
    %5066 = stablehlo.dot_general %5065, %arg253, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %5067 = stablehlo.reshape %5066 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %5068 = stablehlo.slice %5067 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %5069 = stablehlo.reshape %5068 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %5070 = stablehlo.slice %5067 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %5071 = stablehlo.reshape %5070 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %5072 = stablehlo.slice %5067 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %5073 = stablehlo.reshape %5072 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %5074 = stablehlo.concatenate %5069, %5071, %5073, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %5075 = stablehlo.slice %5074 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %5076 = stablehlo.slice %5074 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %5077 = stablehlo.slice %5074 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %5078 = stablehlo.reshape %5075 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %5079 = stablehlo.convert %5078 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %5080 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %5081 = stablehlo.power %5079, %5080 : tensor<16x32x128xf32> loc(#loc1690)
    %5082 = stablehlo.reduce(%5081 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %5083 = stablehlo.broadcast_in_dim %5082, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %5084 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %5085 = stablehlo.divide %5083, %5084 : tensor<16x32x1xf32> loc(#loc1693)
    %5086 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %5087 = stablehlo.add %5085, %5086 : tensor<16x32x1xf32> loc(#loc1694)
    %5088 = stablehlo.rsqrt %5087 : tensor<16x32x1xf32> loc(#loc1678)
    %5089 = stablehlo.broadcast_in_dim %5088, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %5090 = stablehlo.multiply %5079, %5089 : tensor<16x32x128xf32> loc(#loc1695)
    %5091 = stablehlo.convert %5090 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %5092 = stablehlo.broadcast_in_dim %arg252, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %5093 = stablehlo.broadcast_in_dim %5092, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %5094 = stablehlo.multiply %5091, %5093 : tensor<16x32x128xbf16> loc(#loc1697)
    %5095 = stablehlo.reshape %5094 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %5096 = stablehlo.reshape %5076 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %5097 = stablehlo.convert %5096 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %5098 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %5099 = stablehlo.power %5097, %5098 : tensor<16x4x128xf32> loc(#loc1700)
    %5100 = stablehlo.reduce(%5099 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %5101 = stablehlo.broadcast_in_dim %5100, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %5102 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %5103 = stablehlo.divide %5101, %5102 : tensor<16x4x1xf32> loc(#loc1703)
    %5104 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %5105 = stablehlo.add %5103, %5104 : tensor<16x4x1xf32> loc(#loc1704)
    %5106 = stablehlo.rsqrt %5105 : tensor<16x4x1xf32> loc(#loc1678)
    %5107 = stablehlo.broadcast_in_dim %5106, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %5108 = stablehlo.multiply %5097, %5107 : tensor<16x4x128xf32> loc(#loc1705)
    %5109 = stablehlo.convert %5108 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %5110 = stablehlo.broadcast_in_dim %arg250, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %5111 = stablehlo.broadcast_in_dim %5110, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %5112 = stablehlo.multiply %5109, %5111 : tensor<16x4x128xbf16> loc(#loc1707)
    %5113 = stablehlo.reshape %5112 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %5114 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %5115 = stablehlo.compare  LT, %arg484, %5114,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %5116 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %5117 = stablehlo.add %arg484, %5116 : tensor<16xi32> loc(#loc1710)
    %5118 = stablehlo.select %5115, %5117, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %5119 = stablehlo.broadcast_in_dim %5118, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %5120 = "stablehlo.gather"(%arg10, %5119) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %5121 = stablehlo.slice %5120 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %5122 = stablehlo.slice %5120 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %5123 = stablehlo.reshape %5095 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %5124 = stablehlo.broadcast_in_dim %5121, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %5125 = stablehlo.broadcast_in_dim %5122, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %5126 = stablehlo.slice %5123 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %5127 = stablehlo.slice %5123 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %5128 = stablehlo.broadcast_in_dim %5124, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %5129 = stablehlo.multiply %5126, %5128 : tensor<16x32x64xbf16> loc(#loc1719)
    %5130 = stablehlo.broadcast_in_dim %5125, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %5131 = stablehlo.multiply %5127, %5130 : tensor<16x32x64xbf16> loc(#loc1720)
    %5132 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %5133 = stablehlo.multiply %5131, %5132 : tensor<16x32x64xbf16> loc(#loc1721)
    %5134 = stablehlo.subtract %5129, %5133 : tensor<16x32x64xbf16> loc(#loc1722)
    %5135 = stablehlo.broadcast_in_dim %5124, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %5136 = stablehlo.multiply %5127, %5135 : tensor<16x32x64xbf16> loc(#loc1723)
    %5137 = stablehlo.broadcast_in_dim %5125, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %5138 = stablehlo.multiply %5126, %5137 : tensor<16x32x64xbf16> loc(#loc1724)
    %5139 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %5140 = stablehlo.multiply %5138, %5139 : tensor<16x32x64xbf16> loc(#loc1725)
    %5141 = stablehlo.add %5136, %5140 : tensor<16x32x64xbf16> loc(#loc1726)
    %5142 = stablehlo.concatenate %5134, %5141, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %5143 = stablehlo.slice %5123 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %5144 = stablehlo.concatenate %5142, %5143, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %5145 = stablehlo.reshape %5144 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %5146 = stablehlo.reshape %5113 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %5147 = stablehlo.broadcast_in_dim %5121, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %5148 = stablehlo.broadcast_in_dim %5122, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %5149 = stablehlo.slice %5146 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %5150 = stablehlo.slice %5146 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %5151 = stablehlo.broadcast_in_dim %5147, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %5152 = stablehlo.multiply %5149, %5151 : tensor<16x4x64xbf16> loc(#loc1735)
    %5153 = stablehlo.broadcast_in_dim %5148, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %5154 = stablehlo.multiply %5150, %5153 : tensor<16x4x64xbf16> loc(#loc1736)
    %5155 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %5156 = stablehlo.multiply %5154, %5155 : tensor<16x4x64xbf16> loc(#loc1737)
    %5157 = stablehlo.subtract %5152, %5156 : tensor<16x4x64xbf16> loc(#loc1738)
    %5158 = stablehlo.broadcast_in_dim %5147, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %5159 = stablehlo.multiply %5150, %5158 : tensor<16x4x64xbf16> loc(#loc1739)
    %5160 = stablehlo.broadcast_in_dim %5148, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %5161 = stablehlo.multiply %5149, %5160 : tensor<16x4x64xbf16> loc(#loc1740)
    %5162 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %5163 = stablehlo.multiply %5161, %5162 : tensor<16x4x64xbf16> loc(#loc1741)
    %5164 = stablehlo.add %5159, %5163 : tensor<16x4x64xbf16> loc(#loc1742)
    %5165 = stablehlo.concatenate %5157, %5164, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %5166 = stablehlo.slice %5146 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %5167 = stablehlo.concatenate %5165, %5166, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %5168 = stablehlo.reshape %5167 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %5169:2 = call @_jax_attn_func(%arg468, %5145, %5168, %5077, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %5170 = stablehlo.dot_general %5169#1, %arg251, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %5171 = stablehlo.reshape %5170 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %5172 = stablehlo.reshape %5171 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %5173 = stablehlo.convert %5172 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %5174 = stablehlo.convert %5050 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %5175 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %5176 = stablehlo.multiply %5174, %5175 : tensor<16x2048xf32> loc(#loc1751)
    %5177 = stablehlo.add %5173, %5176 : tensor<16x2048xf32> loc(#loc1752)
    %5178 = stablehlo.convert %5177 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %5179 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %5180 = stablehlo.power %5177, %5179 : tensor<16x2048xf32> loc(#loc1754)
    %5181 = stablehlo.reduce(%5180 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %5182 = stablehlo.broadcast_in_dim %5181, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %5183 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %5184 = stablehlo.divide %5182, %5183 : tensor<16x1xf32> loc(#loc1757)
    %5185 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %5186 = stablehlo.add %5184, %5185 : tensor<16x1xf32> loc(#loc1758)
    %5187 = stablehlo.rsqrt %5186 : tensor<16x1xf32> loc(#loc1678)
    %5188 = stablehlo.broadcast_in_dim %5187, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %5189 = stablehlo.multiply %5177, %5188 : tensor<16x2048xf32> loc(#loc1759)
    %5190 = stablehlo.convert %5189 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %5191 = stablehlo.broadcast_in_dim %arg249, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %5192 = stablehlo.broadcast_in_dim %5191, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %5193 = stablehlo.multiply %5190, %5192 : tensor<16x2048xbf16> loc(#loc1761)
    %5194 = stablehlo.dot_general %5193, %arg248, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %5195 = stablehlo.reshape %5194 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %5196 = stablehlo.reshape %5195 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %5197 = call @jax_fused_moe_func_padded(%5193, %arg246, %arg247, %5196) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %5198 = stablehlo.convert %5197 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %5199 = stablehlo.convert %5178 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %5200 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %5201 = stablehlo.multiply %5199, %5200 : tensor<16x2048xf32> loc(#loc1766)
    %5202 = stablehlo.add %5198, %5201 : tensor<16x2048xf32> loc(#loc1767)
    %5203 = stablehlo.convert %5202 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %5204 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %5205 = stablehlo.power %5202, %5204 : tensor<16x2048xf32> loc(#loc1768)
    %5206 = stablehlo.reduce(%5205 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %5207 = stablehlo.broadcast_in_dim %5206, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %5208 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %5209 = stablehlo.divide %5207, %5208 : tensor<16x1xf32> loc(#loc1771)
    %5210 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %5211 = stablehlo.add %5209, %5210 : tensor<16x1xf32> loc(#loc1772)
    %5212 = stablehlo.rsqrt %5211 : tensor<16x1xf32> loc(#loc1678)
    %5213 = stablehlo.broadcast_in_dim %5212, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %5214 = stablehlo.multiply %5202, %5213 : tensor<16x2048xf32> loc(#loc1773)
    %5215 = stablehlo.convert %5214 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %5216 = stablehlo.broadcast_in_dim %arg254, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %5217 = stablehlo.broadcast_in_dim %5216, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %5218 = stablehlo.multiply %5215, %5217 : tensor<16x2048xbf16> loc(#loc1775)
    %5219 = stablehlo.dot_general %5218, %arg262, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %5220 = stablehlo.reshape %5219 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %5221 = stablehlo.slice %5220 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %5222 = stablehlo.reshape %5221 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %5223 = stablehlo.slice %5220 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %5224 = stablehlo.reshape %5223 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %5225 = stablehlo.slice %5220 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %5226 = stablehlo.reshape %5225 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %5227 = stablehlo.concatenate %5222, %5224, %5226, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %5228 = stablehlo.slice %5227 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %5229 = stablehlo.slice %5227 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %5230 = stablehlo.slice %5227 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %5231 = stablehlo.reshape %5228 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %5232 = stablehlo.convert %5231 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %5233 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %5234 = stablehlo.power %5232, %5233 : tensor<16x32x128xf32> loc(#loc1690)
    %5235 = stablehlo.reduce(%5234 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %5236 = stablehlo.broadcast_in_dim %5235, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %5237 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %5238 = stablehlo.divide %5236, %5237 : tensor<16x32x1xf32> loc(#loc1693)
    %5239 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %5240 = stablehlo.add %5238, %5239 : tensor<16x32x1xf32> loc(#loc1694)
    %5241 = stablehlo.rsqrt %5240 : tensor<16x32x1xf32> loc(#loc1678)
    %5242 = stablehlo.broadcast_in_dim %5241, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %5243 = stablehlo.multiply %5232, %5242 : tensor<16x32x128xf32> loc(#loc1695)
    %5244 = stablehlo.convert %5243 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %5245 = stablehlo.broadcast_in_dim %arg261, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %5246 = stablehlo.broadcast_in_dim %5245, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %5247 = stablehlo.multiply %5244, %5246 : tensor<16x32x128xbf16> loc(#loc1697)
    %5248 = stablehlo.reshape %5247 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %5249 = stablehlo.reshape %5229 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %5250 = stablehlo.convert %5249 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %5251 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %5252 = stablehlo.power %5250, %5251 : tensor<16x4x128xf32> loc(#loc1700)
    %5253 = stablehlo.reduce(%5252 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %5254 = stablehlo.broadcast_in_dim %5253, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %5255 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %5256 = stablehlo.divide %5254, %5255 : tensor<16x4x1xf32> loc(#loc1703)
    %5257 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %5258 = stablehlo.add %5256, %5257 : tensor<16x4x1xf32> loc(#loc1704)
    %5259 = stablehlo.rsqrt %5258 : tensor<16x4x1xf32> loc(#loc1678)
    %5260 = stablehlo.broadcast_in_dim %5259, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %5261 = stablehlo.multiply %5250, %5260 : tensor<16x4x128xf32> loc(#loc1705)
    %5262 = stablehlo.convert %5261 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %5263 = stablehlo.broadcast_in_dim %arg259, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %5264 = stablehlo.broadcast_in_dim %5263, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %5265 = stablehlo.multiply %5262, %5264 : tensor<16x4x128xbf16> loc(#loc1707)
    %5266 = stablehlo.reshape %5265 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %5267 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %5268 = stablehlo.compare  LT, %arg484, %5267,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %5269 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %5270 = stablehlo.add %arg484, %5269 : tensor<16xi32> loc(#loc1710)
    %5271 = stablehlo.select %5268, %5270, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %5272 = stablehlo.broadcast_in_dim %5271, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %5273 = "stablehlo.gather"(%arg10, %5272) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %5274 = stablehlo.slice %5273 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %5275 = stablehlo.slice %5273 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %5276 = stablehlo.reshape %5248 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %5277 = stablehlo.broadcast_in_dim %5274, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %5278 = stablehlo.broadcast_in_dim %5275, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %5279 = stablehlo.slice %5276 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %5280 = stablehlo.slice %5276 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %5281 = stablehlo.broadcast_in_dim %5277, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %5282 = stablehlo.multiply %5279, %5281 : tensor<16x32x64xbf16> loc(#loc1719)
    %5283 = stablehlo.broadcast_in_dim %5278, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %5284 = stablehlo.multiply %5280, %5283 : tensor<16x32x64xbf16> loc(#loc1720)
    %5285 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %5286 = stablehlo.multiply %5284, %5285 : tensor<16x32x64xbf16> loc(#loc1721)
    %5287 = stablehlo.subtract %5282, %5286 : tensor<16x32x64xbf16> loc(#loc1722)
    %5288 = stablehlo.broadcast_in_dim %5277, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %5289 = stablehlo.multiply %5280, %5288 : tensor<16x32x64xbf16> loc(#loc1723)
    %5290 = stablehlo.broadcast_in_dim %5278, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %5291 = stablehlo.multiply %5279, %5290 : tensor<16x32x64xbf16> loc(#loc1724)
    %5292 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %5293 = stablehlo.multiply %5291, %5292 : tensor<16x32x64xbf16> loc(#loc1725)
    %5294 = stablehlo.add %5289, %5293 : tensor<16x32x64xbf16> loc(#loc1726)
    %5295 = stablehlo.concatenate %5287, %5294, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %5296 = stablehlo.slice %5276 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %5297 = stablehlo.concatenate %5295, %5296, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %5298 = stablehlo.reshape %5297 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %5299 = stablehlo.reshape %5266 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %5300 = stablehlo.broadcast_in_dim %5274, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %5301 = stablehlo.broadcast_in_dim %5275, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %5302 = stablehlo.slice %5299 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %5303 = stablehlo.slice %5299 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %5304 = stablehlo.broadcast_in_dim %5300, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %5305 = stablehlo.multiply %5302, %5304 : tensor<16x4x64xbf16> loc(#loc1735)
    %5306 = stablehlo.broadcast_in_dim %5301, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %5307 = stablehlo.multiply %5303, %5306 : tensor<16x4x64xbf16> loc(#loc1736)
    %5308 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %5309 = stablehlo.multiply %5307, %5308 : tensor<16x4x64xbf16> loc(#loc1737)
    %5310 = stablehlo.subtract %5305, %5309 : tensor<16x4x64xbf16> loc(#loc1738)
    %5311 = stablehlo.broadcast_in_dim %5300, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %5312 = stablehlo.multiply %5303, %5311 : tensor<16x4x64xbf16> loc(#loc1739)
    %5313 = stablehlo.broadcast_in_dim %5301, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %5314 = stablehlo.multiply %5302, %5313 : tensor<16x4x64xbf16> loc(#loc1740)
    %5315 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %5316 = stablehlo.multiply %5314, %5315 : tensor<16x4x64xbf16> loc(#loc1741)
    %5317 = stablehlo.add %5312, %5316 : tensor<16x4x64xbf16> loc(#loc1742)
    %5318 = stablehlo.concatenate %5310, %5317, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %5319 = stablehlo.slice %5299 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %5320 = stablehlo.concatenate %5318, %5319, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %5321 = stablehlo.reshape %5320 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %5322:2 = call @_jax_attn_func(%arg469, %5298, %5321, %5230, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %5323 = stablehlo.dot_general %5322#1, %arg260, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %5324 = stablehlo.reshape %5323 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %5325 = stablehlo.reshape %5324 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %5326 = stablehlo.convert %5325 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %5327 = stablehlo.convert %5203 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %5328 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %5329 = stablehlo.multiply %5327, %5328 : tensor<16x2048xf32> loc(#loc1751)
    %5330 = stablehlo.add %5326, %5329 : tensor<16x2048xf32> loc(#loc1752)
    %5331 = stablehlo.convert %5330 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %5332 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %5333 = stablehlo.power %5330, %5332 : tensor<16x2048xf32> loc(#loc1754)
    %5334 = stablehlo.reduce(%5333 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %5335 = stablehlo.broadcast_in_dim %5334, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %5336 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %5337 = stablehlo.divide %5335, %5336 : tensor<16x1xf32> loc(#loc1757)
    %5338 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %5339 = stablehlo.add %5337, %5338 : tensor<16x1xf32> loc(#loc1758)
    %5340 = stablehlo.rsqrt %5339 : tensor<16x1xf32> loc(#loc1678)
    %5341 = stablehlo.broadcast_in_dim %5340, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %5342 = stablehlo.multiply %5330, %5341 : tensor<16x2048xf32> loc(#loc1759)
    %5343 = stablehlo.convert %5342 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %5344 = stablehlo.broadcast_in_dim %arg258, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %5345 = stablehlo.broadcast_in_dim %5344, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %5346 = stablehlo.multiply %5343, %5345 : tensor<16x2048xbf16> loc(#loc1761)
    %5347 = stablehlo.dot_general %5346, %arg257, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %5348 = stablehlo.reshape %5347 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %5349 = stablehlo.reshape %5348 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %5350 = call @jax_fused_moe_func_padded(%5346, %arg255, %arg256, %5349) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %5351 = stablehlo.convert %5350 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %5352 = stablehlo.convert %5331 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %5353 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %5354 = stablehlo.multiply %5352, %5353 : tensor<16x2048xf32> loc(#loc1766)
    %5355 = stablehlo.add %5351, %5354 : tensor<16x2048xf32> loc(#loc1767)
    %5356 = stablehlo.convert %5355 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %5357 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %5358 = stablehlo.power %5355, %5357 : tensor<16x2048xf32> loc(#loc1768)
    %5359 = stablehlo.reduce(%5358 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %5360 = stablehlo.broadcast_in_dim %5359, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %5361 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %5362 = stablehlo.divide %5360, %5361 : tensor<16x1xf32> loc(#loc1771)
    %5363 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %5364 = stablehlo.add %5362, %5363 : tensor<16x1xf32> loc(#loc1772)
    %5365 = stablehlo.rsqrt %5364 : tensor<16x1xf32> loc(#loc1678)
    %5366 = stablehlo.broadcast_in_dim %5365, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %5367 = stablehlo.multiply %5355, %5366 : tensor<16x2048xf32> loc(#loc1773)
    %5368 = stablehlo.convert %5367 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %5369 = stablehlo.broadcast_in_dim %arg263, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %5370 = stablehlo.broadcast_in_dim %5369, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %5371 = stablehlo.multiply %5368, %5370 : tensor<16x2048xbf16> loc(#loc1775)
    %5372 = stablehlo.dot_general %5371, %arg271, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %5373 = stablehlo.reshape %5372 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %5374 = stablehlo.slice %5373 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %5375 = stablehlo.reshape %5374 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %5376 = stablehlo.slice %5373 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %5377 = stablehlo.reshape %5376 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %5378 = stablehlo.slice %5373 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %5379 = stablehlo.reshape %5378 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %5380 = stablehlo.concatenate %5375, %5377, %5379, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %5381 = stablehlo.slice %5380 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %5382 = stablehlo.slice %5380 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %5383 = stablehlo.slice %5380 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %5384 = stablehlo.reshape %5381 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %5385 = stablehlo.convert %5384 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %5386 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %5387 = stablehlo.power %5385, %5386 : tensor<16x32x128xf32> loc(#loc1690)
    %5388 = stablehlo.reduce(%5387 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %5389 = stablehlo.broadcast_in_dim %5388, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %5390 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %5391 = stablehlo.divide %5389, %5390 : tensor<16x32x1xf32> loc(#loc1693)
    %5392 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %5393 = stablehlo.add %5391, %5392 : tensor<16x32x1xf32> loc(#loc1694)
    %5394 = stablehlo.rsqrt %5393 : tensor<16x32x1xf32> loc(#loc1678)
    %5395 = stablehlo.broadcast_in_dim %5394, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %5396 = stablehlo.multiply %5385, %5395 : tensor<16x32x128xf32> loc(#loc1695)
    %5397 = stablehlo.convert %5396 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %5398 = stablehlo.broadcast_in_dim %arg270, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %5399 = stablehlo.broadcast_in_dim %5398, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %5400 = stablehlo.multiply %5397, %5399 : tensor<16x32x128xbf16> loc(#loc1697)
    %5401 = stablehlo.reshape %5400 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %5402 = stablehlo.reshape %5382 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %5403 = stablehlo.convert %5402 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %5404 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %5405 = stablehlo.power %5403, %5404 : tensor<16x4x128xf32> loc(#loc1700)
    %5406 = stablehlo.reduce(%5405 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %5407 = stablehlo.broadcast_in_dim %5406, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %5408 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %5409 = stablehlo.divide %5407, %5408 : tensor<16x4x1xf32> loc(#loc1703)
    %5410 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %5411 = stablehlo.add %5409, %5410 : tensor<16x4x1xf32> loc(#loc1704)
    %5412 = stablehlo.rsqrt %5411 : tensor<16x4x1xf32> loc(#loc1678)
    %5413 = stablehlo.broadcast_in_dim %5412, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %5414 = stablehlo.multiply %5403, %5413 : tensor<16x4x128xf32> loc(#loc1705)
    %5415 = stablehlo.convert %5414 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %5416 = stablehlo.broadcast_in_dim %arg268, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %5417 = stablehlo.broadcast_in_dim %5416, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %5418 = stablehlo.multiply %5415, %5417 : tensor<16x4x128xbf16> loc(#loc1707)
    %5419 = stablehlo.reshape %5418 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %5420 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %5421 = stablehlo.compare  LT, %arg484, %5420,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %5422 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %5423 = stablehlo.add %arg484, %5422 : tensor<16xi32> loc(#loc1710)
    %5424 = stablehlo.select %5421, %5423, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %5425 = stablehlo.broadcast_in_dim %5424, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %5426 = "stablehlo.gather"(%arg10, %5425) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %5427 = stablehlo.slice %5426 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %5428 = stablehlo.slice %5426 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %5429 = stablehlo.reshape %5401 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %5430 = stablehlo.broadcast_in_dim %5427, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %5431 = stablehlo.broadcast_in_dim %5428, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %5432 = stablehlo.slice %5429 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %5433 = stablehlo.slice %5429 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %5434 = stablehlo.broadcast_in_dim %5430, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %5435 = stablehlo.multiply %5432, %5434 : tensor<16x32x64xbf16> loc(#loc1719)
    %5436 = stablehlo.broadcast_in_dim %5431, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %5437 = stablehlo.multiply %5433, %5436 : tensor<16x32x64xbf16> loc(#loc1720)
    %5438 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %5439 = stablehlo.multiply %5437, %5438 : tensor<16x32x64xbf16> loc(#loc1721)
    %5440 = stablehlo.subtract %5435, %5439 : tensor<16x32x64xbf16> loc(#loc1722)
    %5441 = stablehlo.broadcast_in_dim %5430, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %5442 = stablehlo.multiply %5433, %5441 : tensor<16x32x64xbf16> loc(#loc1723)
    %5443 = stablehlo.broadcast_in_dim %5431, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %5444 = stablehlo.multiply %5432, %5443 : tensor<16x32x64xbf16> loc(#loc1724)
    %5445 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %5446 = stablehlo.multiply %5444, %5445 : tensor<16x32x64xbf16> loc(#loc1725)
    %5447 = stablehlo.add %5442, %5446 : tensor<16x32x64xbf16> loc(#loc1726)
    %5448 = stablehlo.concatenate %5440, %5447, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %5449 = stablehlo.slice %5429 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %5450 = stablehlo.concatenate %5448, %5449, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %5451 = stablehlo.reshape %5450 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %5452 = stablehlo.reshape %5419 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %5453 = stablehlo.broadcast_in_dim %5427, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %5454 = stablehlo.broadcast_in_dim %5428, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %5455 = stablehlo.slice %5452 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %5456 = stablehlo.slice %5452 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %5457 = stablehlo.broadcast_in_dim %5453, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %5458 = stablehlo.multiply %5455, %5457 : tensor<16x4x64xbf16> loc(#loc1735)
    %5459 = stablehlo.broadcast_in_dim %5454, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %5460 = stablehlo.multiply %5456, %5459 : tensor<16x4x64xbf16> loc(#loc1736)
    %5461 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %5462 = stablehlo.multiply %5460, %5461 : tensor<16x4x64xbf16> loc(#loc1737)
    %5463 = stablehlo.subtract %5458, %5462 : tensor<16x4x64xbf16> loc(#loc1738)
    %5464 = stablehlo.broadcast_in_dim %5453, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %5465 = stablehlo.multiply %5456, %5464 : tensor<16x4x64xbf16> loc(#loc1739)
    %5466 = stablehlo.broadcast_in_dim %5454, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %5467 = stablehlo.multiply %5455, %5466 : tensor<16x4x64xbf16> loc(#loc1740)
    %5468 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %5469 = stablehlo.multiply %5467, %5468 : tensor<16x4x64xbf16> loc(#loc1741)
    %5470 = stablehlo.add %5465, %5469 : tensor<16x4x64xbf16> loc(#loc1742)
    %5471 = stablehlo.concatenate %5463, %5470, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %5472 = stablehlo.slice %5452 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %5473 = stablehlo.concatenate %5471, %5472, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %5474 = stablehlo.reshape %5473 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %5475:2 = call @_jax_attn_func(%arg470, %5451, %5474, %5383, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %5476 = stablehlo.dot_general %5475#1, %arg269, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %5477 = stablehlo.reshape %5476 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %5478 = stablehlo.reshape %5477 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %5479 = stablehlo.convert %5478 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %5480 = stablehlo.convert %5356 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %5481 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %5482 = stablehlo.multiply %5480, %5481 : tensor<16x2048xf32> loc(#loc1751)
    %5483 = stablehlo.add %5479, %5482 : tensor<16x2048xf32> loc(#loc1752)
    %5484 = stablehlo.convert %5483 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %5485 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %5486 = stablehlo.power %5483, %5485 : tensor<16x2048xf32> loc(#loc1754)
    %5487 = stablehlo.reduce(%5486 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %5488 = stablehlo.broadcast_in_dim %5487, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %5489 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %5490 = stablehlo.divide %5488, %5489 : tensor<16x1xf32> loc(#loc1757)
    %5491 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %5492 = stablehlo.add %5490, %5491 : tensor<16x1xf32> loc(#loc1758)
    %5493 = stablehlo.rsqrt %5492 : tensor<16x1xf32> loc(#loc1678)
    %5494 = stablehlo.broadcast_in_dim %5493, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %5495 = stablehlo.multiply %5483, %5494 : tensor<16x2048xf32> loc(#loc1759)
    %5496 = stablehlo.convert %5495 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %5497 = stablehlo.broadcast_in_dim %arg267, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %5498 = stablehlo.broadcast_in_dim %5497, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %5499 = stablehlo.multiply %5496, %5498 : tensor<16x2048xbf16> loc(#loc1761)
    %5500 = stablehlo.dot_general %5499, %arg266, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %5501 = stablehlo.reshape %5500 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %5502 = stablehlo.reshape %5501 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %5503 = call @jax_fused_moe_func_padded(%5499, %arg264, %arg265, %5502) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %5504 = stablehlo.convert %5503 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %5505 = stablehlo.convert %5484 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %5506 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %5507 = stablehlo.multiply %5505, %5506 : tensor<16x2048xf32> loc(#loc1766)
    %5508 = stablehlo.add %5504, %5507 : tensor<16x2048xf32> loc(#loc1767)
    %5509 = stablehlo.convert %5508 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %5510 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %5511 = stablehlo.power %5508, %5510 : tensor<16x2048xf32> loc(#loc1768)
    %5512 = stablehlo.reduce(%5511 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %5513 = stablehlo.broadcast_in_dim %5512, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %5514 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %5515 = stablehlo.divide %5513, %5514 : tensor<16x1xf32> loc(#loc1771)
    %5516 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %5517 = stablehlo.add %5515, %5516 : tensor<16x1xf32> loc(#loc1772)
    %5518 = stablehlo.rsqrt %5517 : tensor<16x1xf32> loc(#loc1678)
    %5519 = stablehlo.broadcast_in_dim %5518, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %5520 = stablehlo.multiply %5508, %5519 : tensor<16x2048xf32> loc(#loc1773)
    %5521 = stablehlo.convert %5520 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %5522 = stablehlo.broadcast_in_dim %arg272, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %5523 = stablehlo.broadcast_in_dim %5522, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %5524 = stablehlo.multiply %5521, %5523 : tensor<16x2048xbf16> loc(#loc1775)
    %5525 = stablehlo.dot_general %5524, %arg280, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %5526 = stablehlo.reshape %5525 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %5527 = stablehlo.slice %5526 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %5528 = stablehlo.reshape %5527 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %5529 = stablehlo.slice %5526 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %5530 = stablehlo.reshape %5529 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %5531 = stablehlo.slice %5526 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %5532 = stablehlo.reshape %5531 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %5533 = stablehlo.concatenate %5528, %5530, %5532, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %5534 = stablehlo.slice %5533 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %5535 = stablehlo.slice %5533 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %5536 = stablehlo.slice %5533 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %5537 = stablehlo.reshape %5534 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %5538 = stablehlo.convert %5537 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %5539 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %5540 = stablehlo.power %5538, %5539 : tensor<16x32x128xf32> loc(#loc1690)
    %5541 = stablehlo.reduce(%5540 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %5542 = stablehlo.broadcast_in_dim %5541, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %5543 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %5544 = stablehlo.divide %5542, %5543 : tensor<16x32x1xf32> loc(#loc1693)
    %5545 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %5546 = stablehlo.add %5544, %5545 : tensor<16x32x1xf32> loc(#loc1694)
    %5547 = stablehlo.rsqrt %5546 : tensor<16x32x1xf32> loc(#loc1678)
    %5548 = stablehlo.broadcast_in_dim %5547, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %5549 = stablehlo.multiply %5538, %5548 : tensor<16x32x128xf32> loc(#loc1695)
    %5550 = stablehlo.convert %5549 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %5551 = stablehlo.broadcast_in_dim %arg279, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %5552 = stablehlo.broadcast_in_dim %5551, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %5553 = stablehlo.multiply %5550, %5552 : tensor<16x32x128xbf16> loc(#loc1697)
    %5554 = stablehlo.reshape %5553 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %5555 = stablehlo.reshape %5535 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %5556 = stablehlo.convert %5555 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %5557 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %5558 = stablehlo.power %5556, %5557 : tensor<16x4x128xf32> loc(#loc1700)
    %5559 = stablehlo.reduce(%5558 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %5560 = stablehlo.broadcast_in_dim %5559, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %5561 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %5562 = stablehlo.divide %5560, %5561 : tensor<16x4x1xf32> loc(#loc1703)
    %5563 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %5564 = stablehlo.add %5562, %5563 : tensor<16x4x1xf32> loc(#loc1704)
    %5565 = stablehlo.rsqrt %5564 : tensor<16x4x1xf32> loc(#loc1678)
    %5566 = stablehlo.broadcast_in_dim %5565, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %5567 = stablehlo.multiply %5556, %5566 : tensor<16x4x128xf32> loc(#loc1705)
    %5568 = stablehlo.convert %5567 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %5569 = stablehlo.broadcast_in_dim %arg277, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %5570 = stablehlo.broadcast_in_dim %5569, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %5571 = stablehlo.multiply %5568, %5570 : tensor<16x4x128xbf16> loc(#loc1707)
    %5572 = stablehlo.reshape %5571 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %5573 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %5574 = stablehlo.compare  LT, %arg484, %5573,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %5575 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %5576 = stablehlo.add %arg484, %5575 : tensor<16xi32> loc(#loc1710)
    %5577 = stablehlo.select %5574, %5576, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %5578 = stablehlo.broadcast_in_dim %5577, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %5579 = "stablehlo.gather"(%arg10, %5578) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %5580 = stablehlo.slice %5579 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %5581 = stablehlo.slice %5579 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %5582 = stablehlo.reshape %5554 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %5583 = stablehlo.broadcast_in_dim %5580, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %5584 = stablehlo.broadcast_in_dim %5581, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %5585 = stablehlo.slice %5582 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %5586 = stablehlo.slice %5582 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %5587 = stablehlo.broadcast_in_dim %5583, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %5588 = stablehlo.multiply %5585, %5587 : tensor<16x32x64xbf16> loc(#loc1719)
    %5589 = stablehlo.broadcast_in_dim %5584, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %5590 = stablehlo.multiply %5586, %5589 : tensor<16x32x64xbf16> loc(#loc1720)
    %5591 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %5592 = stablehlo.multiply %5590, %5591 : tensor<16x32x64xbf16> loc(#loc1721)
    %5593 = stablehlo.subtract %5588, %5592 : tensor<16x32x64xbf16> loc(#loc1722)
    %5594 = stablehlo.broadcast_in_dim %5583, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %5595 = stablehlo.multiply %5586, %5594 : tensor<16x32x64xbf16> loc(#loc1723)
    %5596 = stablehlo.broadcast_in_dim %5584, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %5597 = stablehlo.multiply %5585, %5596 : tensor<16x32x64xbf16> loc(#loc1724)
    %5598 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %5599 = stablehlo.multiply %5597, %5598 : tensor<16x32x64xbf16> loc(#loc1725)
    %5600 = stablehlo.add %5595, %5599 : tensor<16x32x64xbf16> loc(#loc1726)
    %5601 = stablehlo.concatenate %5593, %5600, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %5602 = stablehlo.slice %5582 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %5603 = stablehlo.concatenate %5601, %5602, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %5604 = stablehlo.reshape %5603 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %5605 = stablehlo.reshape %5572 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %5606 = stablehlo.broadcast_in_dim %5580, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %5607 = stablehlo.broadcast_in_dim %5581, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %5608 = stablehlo.slice %5605 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %5609 = stablehlo.slice %5605 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %5610 = stablehlo.broadcast_in_dim %5606, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %5611 = stablehlo.multiply %5608, %5610 : tensor<16x4x64xbf16> loc(#loc1735)
    %5612 = stablehlo.broadcast_in_dim %5607, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %5613 = stablehlo.multiply %5609, %5612 : tensor<16x4x64xbf16> loc(#loc1736)
    %5614 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %5615 = stablehlo.multiply %5613, %5614 : tensor<16x4x64xbf16> loc(#loc1737)
    %5616 = stablehlo.subtract %5611, %5615 : tensor<16x4x64xbf16> loc(#loc1738)
    %5617 = stablehlo.broadcast_in_dim %5606, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %5618 = stablehlo.multiply %5609, %5617 : tensor<16x4x64xbf16> loc(#loc1739)
    %5619 = stablehlo.broadcast_in_dim %5607, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %5620 = stablehlo.multiply %5608, %5619 : tensor<16x4x64xbf16> loc(#loc1740)
    %5621 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %5622 = stablehlo.multiply %5620, %5621 : tensor<16x4x64xbf16> loc(#loc1741)
    %5623 = stablehlo.add %5618, %5622 : tensor<16x4x64xbf16> loc(#loc1742)
    %5624 = stablehlo.concatenate %5616, %5623, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %5625 = stablehlo.slice %5605 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %5626 = stablehlo.concatenate %5624, %5625, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %5627 = stablehlo.reshape %5626 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %5628:2 = call @_jax_attn_func(%arg471, %5604, %5627, %5536, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %5629 = stablehlo.dot_general %5628#1, %arg278, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %5630 = stablehlo.reshape %5629 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %5631 = stablehlo.reshape %5630 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %5632 = stablehlo.convert %5631 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %5633 = stablehlo.convert %5509 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %5634 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %5635 = stablehlo.multiply %5633, %5634 : tensor<16x2048xf32> loc(#loc1751)
    %5636 = stablehlo.add %5632, %5635 : tensor<16x2048xf32> loc(#loc1752)
    %5637 = stablehlo.convert %5636 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %5638 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %5639 = stablehlo.power %5636, %5638 : tensor<16x2048xf32> loc(#loc1754)
    %5640 = stablehlo.reduce(%5639 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %5641 = stablehlo.broadcast_in_dim %5640, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %5642 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %5643 = stablehlo.divide %5641, %5642 : tensor<16x1xf32> loc(#loc1757)
    %5644 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %5645 = stablehlo.add %5643, %5644 : tensor<16x1xf32> loc(#loc1758)
    %5646 = stablehlo.rsqrt %5645 : tensor<16x1xf32> loc(#loc1678)
    %5647 = stablehlo.broadcast_in_dim %5646, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %5648 = stablehlo.multiply %5636, %5647 : tensor<16x2048xf32> loc(#loc1759)
    %5649 = stablehlo.convert %5648 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %5650 = stablehlo.broadcast_in_dim %arg276, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %5651 = stablehlo.broadcast_in_dim %5650, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %5652 = stablehlo.multiply %5649, %5651 : tensor<16x2048xbf16> loc(#loc1761)
    %5653 = stablehlo.dot_general %5652, %arg275, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %5654 = stablehlo.reshape %5653 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %5655 = stablehlo.reshape %5654 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %5656 = call @jax_fused_moe_func_padded(%5652, %arg273, %arg274, %5655) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %5657 = stablehlo.convert %5656 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %5658 = stablehlo.convert %5637 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %5659 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %5660 = stablehlo.multiply %5658, %5659 : tensor<16x2048xf32> loc(#loc1766)
    %5661 = stablehlo.add %5657, %5660 : tensor<16x2048xf32> loc(#loc1767)
    %5662 = stablehlo.convert %5661 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %5663 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %5664 = stablehlo.power %5661, %5663 : tensor<16x2048xf32> loc(#loc1768)
    %5665 = stablehlo.reduce(%5664 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %5666 = stablehlo.broadcast_in_dim %5665, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %5667 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %5668 = stablehlo.divide %5666, %5667 : tensor<16x1xf32> loc(#loc1771)
    %5669 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %5670 = stablehlo.add %5668, %5669 : tensor<16x1xf32> loc(#loc1772)
    %5671 = stablehlo.rsqrt %5670 : tensor<16x1xf32> loc(#loc1678)
    %5672 = stablehlo.broadcast_in_dim %5671, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %5673 = stablehlo.multiply %5661, %5672 : tensor<16x2048xf32> loc(#loc1773)
    %5674 = stablehlo.convert %5673 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %5675 = stablehlo.broadcast_in_dim %arg281, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %5676 = stablehlo.broadcast_in_dim %5675, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %5677 = stablehlo.multiply %5674, %5676 : tensor<16x2048xbf16> loc(#loc1775)
    %5678 = stablehlo.dot_general %5677, %arg289, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %5679 = stablehlo.reshape %5678 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %5680 = stablehlo.slice %5679 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %5681 = stablehlo.reshape %5680 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %5682 = stablehlo.slice %5679 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %5683 = stablehlo.reshape %5682 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %5684 = stablehlo.slice %5679 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %5685 = stablehlo.reshape %5684 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %5686 = stablehlo.concatenate %5681, %5683, %5685, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %5687 = stablehlo.slice %5686 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %5688 = stablehlo.slice %5686 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %5689 = stablehlo.slice %5686 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %5690 = stablehlo.reshape %5687 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %5691 = stablehlo.convert %5690 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %5692 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %5693 = stablehlo.power %5691, %5692 : tensor<16x32x128xf32> loc(#loc1690)
    %5694 = stablehlo.reduce(%5693 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %5695 = stablehlo.broadcast_in_dim %5694, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %5696 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %5697 = stablehlo.divide %5695, %5696 : tensor<16x32x1xf32> loc(#loc1693)
    %5698 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %5699 = stablehlo.add %5697, %5698 : tensor<16x32x1xf32> loc(#loc1694)
    %5700 = stablehlo.rsqrt %5699 : tensor<16x32x1xf32> loc(#loc1678)
    %5701 = stablehlo.broadcast_in_dim %5700, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %5702 = stablehlo.multiply %5691, %5701 : tensor<16x32x128xf32> loc(#loc1695)
    %5703 = stablehlo.convert %5702 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %5704 = stablehlo.broadcast_in_dim %arg288, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %5705 = stablehlo.broadcast_in_dim %5704, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %5706 = stablehlo.multiply %5703, %5705 : tensor<16x32x128xbf16> loc(#loc1697)
    %5707 = stablehlo.reshape %5706 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %5708 = stablehlo.reshape %5688 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %5709 = stablehlo.convert %5708 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %5710 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %5711 = stablehlo.power %5709, %5710 : tensor<16x4x128xf32> loc(#loc1700)
    %5712 = stablehlo.reduce(%5711 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %5713 = stablehlo.broadcast_in_dim %5712, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %5714 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %5715 = stablehlo.divide %5713, %5714 : tensor<16x4x1xf32> loc(#loc1703)
    %5716 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %5717 = stablehlo.add %5715, %5716 : tensor<16x4x1xf32> loc(#loc1704)
    %5718 = stablehlo.rsqrt %5717 : tensor<16x4x1xf32> loc(#loc1678)
    %5719 = stablehlo.broadcast_in_dim %5718, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %5720 = stablehlo.multiply %5709, %5719 : tensor<16x4x128xf32> loc(#loc1705)
    %5721 = stablehlo.convert %5720 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %5722 = stablehlo.broadcast_in_dim %arg286, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %5723 = stablehlo.broadcast_in_dim %5722, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %5724 = stablehlo.multiply %5721, %5723 : tensor<16x4x128xbf16> loc(#loc1707)
    %5725 = stablehlo.reshape %5724 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %5726 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %5727 = stablehlo.compare  LT, %arg484, %5726,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %5728 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %5729 = stablehlo.add %arg484, %5728 : tensor<16xi32> loc(#loc1710)
    %5730 = stablehlo.select %5727, %5729, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %5731 = stablehlo.broadcast_in_dim %5730, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %5732 = "stablehlo.gather"(%arg10, %5731) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %5733 = stablehlo.slice %5732 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %5734 = stablehlo.slice %5732 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %5735 = stablehlo.reshape %5707 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %5736 = stablehlo.broadcast_in_dim %5733, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %5737 = stablehlo.broadcast_in_dim %5734, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %5738 = stablehlo.slice %5735 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %5739 = stablehlo.slice %5735 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %5740 = stablehlo.broadcast_in_dim %5736, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %5741 = stablehlo.multiply %5738, %5740 : tensor<16x32x64xbf16> loc(#loc1719)
    %5742 = stablehlo.broadcast_in_dim %5737, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %5743 = stablehlo.multiply %5739, %5742 : tensor<16x32x64xbf16> loc(#loc1720)
    %5744 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %5745 = stablehlo.multiply %5743, %5744 : tensor<16x32x64xbf16> loc(#loc1721)
    %5746 = stablehlo.subtract %5741, %5745 : tensor<16x32x64xbf16> loc(#loc1722)
    %5747 = stablehlo.broadcast_in_dim %5736, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %5748 = stablehlo.multiply %5739, %5747 : tensor<16x32x64xbf16> loc(#loc1723)
    %5749 = stablehlo.broadcast_in_dim %5737, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %5750 = stablehlo.multiply %5738, %5749 : tensor<16x32x64xbf16> loc(#loc1724)
    %5751 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %5752 = stablehlo.multiply %5750, %5751 : tensor<16x32x64xbf16> loc(#loc1725)
    %5753 = stablehlo.add %5748, %5752 : tensor<16x32x64xbf16> loc(#loc1726)
    %5754 = stablehlo.concatenate %5746, %5753, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %5755 = stablehlo.slice %5735 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %5756 = stablehlo.concatenate %5754, %5755, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %5757 = stablehlo.reshape %5756 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %5758 = stablehlo.reshape %5725 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %5759 = stablehlo.broadcast_in_dim %5733, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %5760 = stablehlo.broadcast_in_dim %5734, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %5761 = stablehlo.slice %5758 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %5762 = stablehlo.slice %5758 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %5763 = stablehlo.broadcast_in_dim %5759, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %5764 = stablehlo.multiply %5761, %5763 : tensor<16x4x64xbf16> loc(#loc1735)
    %5765 = stablehlo.broadcast_in_dim %5760, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %5766 = stablehlo.multiply %5762, %5765 : tensor<16x4x64xbf16> loc(#loc1736)
    %5767 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %5768 = stablehlo.multiply %5766, %5767 : tensor<16x4x64xbf16> loc(#loc1737)
    %5769 = stablehlo.subtract %5764, %5768 : tensor<16x4x64xbf16> loc(#loc1738)
    %5770 = stablehlo.broadcast_in_dim %5759, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %5771 = stablehlo.multiply %5762, %5770 : tensor<16x4x64xbf16> loc(#loc1739)
    %5772 = stablehlo.broadcast_in_dim %5760, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %5773 = stablehlo.multiply %5761, %5772 : tensor<16x4x64xbf16> loc(#loc1740)
    %5774 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %5775 = stablehlo.multiply %5773, %5774 : tensor<16x4x64xbf16> loc(#loc1741)
    %5776 = stablehlo.add %5771, %5775 : tensor<16x4x64xbf16> loc(#loc1742)
    %5777 = stablehlo.concatenate %5769, %5776, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %5778 = stablehlo.slice %5758 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %5779 = stablehlo.concatenate %5777, %5778, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %5780 = stablehlo.reshape %5779 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %5781:2 = call @_jax_attn_func(%arg472, %5757, %5780, %5689, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %5782 = stablehlo.dot_general %5781#1, %arg287, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %5783 = stablehlo.reshape %5782 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %5784 = stablehlo.reshape %5783 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %5785 = stablehlo.convert %5784 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %5786 = stablehlo.convert %5662 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %5787 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %5788 = stablehlo.multiply %5786, %5787 : tensor<16x2048xf32> loc(#loc1751)
    %5789 = stablehlo.add %5785, %5788 : tensor<16x2048xf32> loc(#loc1752)
    %5790 = stablehlo.convert %5789 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %5791 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %5792 = stablehlo.power %5789, %5791 : tensor<16x2048xf32> loc(#loc1754)
    %5793 = stablehlo.reduce(%5792 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %5794 = stablehlo.broadcast_in_dim %5793, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %5795 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %5796 = stablehlo.divide %5794, %5795 : tensor<16x1xf32> loc(#loc1757)
    %5797 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %5798 = stablehlo.add %5796, %5797 : tensor<16x1xf32> loc(#loc1758)
    %5799 = stablehlo.rsqrt %5798 : tensor<16x1xf32> loc(#loc1678)
    %5800 = stablehlo.broadcast_in_dim %5799, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %5801 = stablehlo.multiply %5789, %5800 : tensor<16x2048xf32> loc(#loc1759)
    %5802 = stablehlo.convert %5801 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %5803 = stablehlo.broadcast_in_dim %arg285, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %5804 = stablehlo.broadcast_in_dim %5803, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %5805 = stablehlo.multiply %5802, %5804 : tensor<16x2048xbf16> loc(#loc1761)
    %5806 = stablehlo.dot_general %5805, %arg284, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %5807 = stablehlo.reshape %5806 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %5808 = stablehlo.reshape %5807 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %5809 = call @jax_fused_moe_func_padded(%5805, %arg282, %arg283, %5808) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %5810 = stablehlo.convert %5809 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %5811 = stablehlo.convert %5790 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %5812 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %5813 = stablehlo.multiply %5811, %5812 : tensor<16x2048xf32> loc(#loc1766)
    %5814 = stablehlo.add %5810, %5813 : tensor<16x2048xf32> loc(#loc1767)
    %5815 = stablehlo.convert %5814 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %5816 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %5817 = stablehlo.power %5814, %5816 : tensor<16x2048xf32> loc(#loc1768)
    %5818 = stablehlo.reduce(%5817 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %5819 = stablehlo.broadcast_in_dim %5818, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %5820 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %5821 = stablehlo.divide %5819, %5820 : tensor<16x1xf32> loc(#loc1771)
    %5822 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %5823 = stablehlo.add %5821, %5822 : tensor<16x1xf32> loc(#loc1772)
    %5824 = stablehlo.rsqrt %5823 : tensor<16x1xf32> loc(#loc1678)
    %5825 = stablehlo.broadcast_in_dim %5824, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %5826 = stablehlo.multiply %5814, %5825 : tensor<16x2048xf32> loc(#loc1773)
    %5827 = stablehlo.convert %5826 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %5828 = stablehlo.broadcast_in_dim %arg290, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %5829 = stablehlo.broadcast_in_dim %5828, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %5830 = stablehlo.multiply %5827, %5829 : tensor<16x2048xbf16> loc(#loc1775)
    %5831 = stablehlo.dot_general %5830, %arg298, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %5832 = stablehlo.reshape %5831 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %5833 = stablehlo.slice %5832 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %5834 = stablehlo.reshape %5833 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %5835 = stablehlo.slice %5832 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %5836 = stablehlo.reshape %5835 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %5837 = stablehlo.slice %5832 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %5838 = stablehlo.reshape %5837 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %5839 = stablehlo.concatenate %5834, %5836, %5838, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %5840 = stablehlo.slice %5839 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %5841 = stablehlo.slice %5839 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %5842 = stablehlo.slice %5839 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %5843 = stablehlo.reshape %5840 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %5844 = stablehlo.convert %5843 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %5845 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %5846 = stablehlo.power %5844, %5845 : tensor<16x32x128xf32> loc(#loc1690)
    %5847 = stablehlo.reduce(%5846 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %5848 = stablehlo.broadcast_in_dim %5847, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %5849 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %5850 = stablehlo.divide %5848, %5849 : tensor<16x32x1xf32> loc(#loc1693)
    %5851 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %5852 = stablehlo.add %5850, %5851 : tensor<16x32x1xf32> loc(#loc1694)
    %5853 = stablehlo.rsqrt %5852 : tensor<16x32x1xf32> loc(#loc1678)
    %5854 = stablehlo.broadcast_in_dim %5853, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %5855 = stablehlo.multiply %5844, %5854 : tensor<16x32x128xf32> loc(#loc1695)
    %5856 = stablehlo.convert %5855 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %5857 = stablehlo.broadcast_in_dim %arg297, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %5858 = stablehlo.broadcast_in_dim %5857, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %5859 = stablehlo.multiply %5856, %5858 : tensor<16x32x128xbf16> loc(#loc1697)
    %5860 = stablehlo.reshape %5859 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %5861 = stablehlo.reshape %5841 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %5862 = stablehlo.convert %5861 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %5863 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %5864 = stablehlo.power %5862, %5863 : tensor<16x4x128xf32> loc(#loc1700)
    %5865 = stablehlo.reduce(%5864 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %5866 = stablehlo.broadcast_in_dim %5865, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %5867 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %5868 = stablehlo.divide %5866, %5867 : tensor<16x4x1xf32> loc(#loc1703)
    %5869 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %5870 = stablehlo.add %5868, %5869 : tensor<16x4x1xf32> loc(#loc1704)
    %5871 = stablehlo.rsqrt %5870 : tensor<16x4x1xf32> loc(#loc1678)
    %5872 = stablehlo.broadcast_in_dim %5871, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %5873 = stablehlo.multiply %5862, %5872 : tensor<16x4x128xf32> loc(#loc1705)
    %5874 = stablehlo.convert %5873 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %5875 = stablehlo.broadcast_in_dim %arg295, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %5876 = stablehlo.broadcast_in_dim %5875, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %5877 = stablehlo.multiply %5874, %5876 : tensor<16x4x128xbf16> loc(#loc1707)
    %5878 = stablehlo.reshape %5877 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %5879 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %5880 = stablehlo.compare  LT, %arg484, %5879,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %5881 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %5882 = stablehlo.add %arg484, %5881 : tensor<16xi32> loc(#loc1710)
    %5883 = stablehlo.select %5880, %5882, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %5884 = stablehlo.broadcast_in_dim %5883, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %5885 = "stablehlo.gather"(%arg10, %5884) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %5886 = stablehlo.slice %5885 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %5887 = stablehlo.slice %5885 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %5888 = stablehlo.reshape %5860 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %5889 = stablehlo.broadcast_in_dim %5886, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %5890 = stablehlo.broadcast_in_dim %5887, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %5891 = stablehlo.slice %5888 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %5892 = stablehlo.slice %5888 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %5893 = stablehlo.broadcast_in_dim %5889, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %5894 = stablehlo.multiply %5891, %5893 : tensor<16x32x64xbf16> loc(#loc1719)
    %5895 = stablehlo.broadcast_in_dim %5890, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %5896 = stablehlo.multiply %5892, %5895 : tensor<16x32x64xbf16> loc(#loc1720)
    %5897 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %5898 = stablehlo.multiply %5896, %5897 : tensor<16x32x64xbf16> loc(#loc1721)
    %5899 = stablehlo.subtract %5894, %5898 : tensor<16x32x64xbf16> loc(#loc1722)
    %5900 = stablehlo.broadcast_in_dim %5889, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %5901 = stablehlo.multiply %5892, %5900 : tensor<16x32x64xbf16> loc(#loc1723)
    %5902 = stablehlo.broadcast_in_dim %5890, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %5903 = stablehlo.multiply %5891, %5902 : tensor<16x32x64xbf16> loc(#loc1724)
    %5904 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %5905 = stablehlo.multiply %5903, %5904 : tensor<16x32x64xbf16> loc(#loc1725)
    %5906 = stablehlo.add %5901, %5905 : tensor<16x32x64xbf16> loc(#loc1726)
    %5907 = stablehlo.concatenate %5899, %5906, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %5908 = stablehlo.slice %5888 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %5909 = stablehlo.concatenate %5907, %5908, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %5910 = stablehlo.reshape %5909 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %5911 = stablehlo.reshape %5878 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %5912 = stablehlo.broadcast_in_dim %5886, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %5913 = stablehlo.broadcast_in_dim %5887, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %5914 = stablehlo.slice %5911 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %5915 = stablehlo.slice %5911 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %5916 = stablehlo.broadcast_in_dim %5912, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %5917 = stablehlo.multiply %5914, %5916 : tensor<16x4x64xbf16> loc(#loc1735)
    %5918 = stablehlo.broadcast_in_dim %5913, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %5919 = stablehlo.multiply %5915, %5918 : tensor<16x4x64xbf16> loc(#loc1736)
    %5920 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %5921 = stablehlo.multiply %5919, %5920 : tensor<16x4x64xbf16> loc(#loc1737)
    %5922 = stablehlo.subtract %5917, %5921 : tensor<16x4x64xbf16> loc(#loc1738)
    %5923 = stablehlo.broadcast_in_dim %5912, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %5924 = stablehlo.multiply %5915, %5923 : tensor<16x4x64xbf16> loc(#loc1739)
    %5925 = stablehlo.broadcast_in_dim %5913, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %5926 = stablehlo.multiply %5914, %5925 : tensor<16x4x64xbf16> loc(#loc1740)
    %5927 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %5928 = stablehlo.multiply %5926, %5927 : tensor<16x4x64xbf16> loc(#loc1741)
    %5929 = stablehlo.add %5924, %5928 : tensor<16x4x64xbf16> loc(#loc1742)
    %5930 = stablehlo.concatenate %5922, %5929, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %5931 = stablehlo.slice %5911 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %5932 = stablehlo.concatenate %5930, %5931, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %5933 = stablehlo.reshape %5932 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %5934:2 = call @_jax_attn_func(%arg473, %5910, %5933, %5842, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %5935 = stablehlo.dot_general %5934#1, %arg296, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %5936 = stablehlo.reshape %5935 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %5937 = stablehlo.reshape %5936 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %5938 = stablehlo.convert %5937 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %5939 = stablehlo.convert %5815 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %5940 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %5941 = stablehlo.multiply %5939, %5940 : tensor<16x2048xf32> loc(#loc1751)
    %5942 = stablehlo.add %5938, %5941 : tensor<16x2048xf32> loc(#loc1752)
    %5943 = stablehlo.convert %5942 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %5944 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %5945 = stablehlo.power %5942, %5944 : tensor<16x2048xf32> loc(#loc1754)
    %5946 = stablehlo.reduce(%5945 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %5947 = stablehlo.broadcast_in_dim %5946, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %5948 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %5949 = stablehlo.divide %5947, %5948 : tensor<16x1xf32> loc(#loc1757)
    %5950 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %5951 = stablehlo.add %5949, %5950 : tensor<16x1xf32> loc(#loc1758)
    %5952 = stablehlo.rsqrt %5951 : tensor<16x1xf32> loc(#loc1678)
    %5953 = stablehlo.broadcast_in_dim %5952, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %5954 = stablehlo.multiply %5942, %5953 : tensor<16x2048xf32> loc(#loc1759)
    %5955 = stablehlo.convert %5954 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %5956 = stablehlo.broadcast_in_dim %arg294, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %5957 = stablehlo.broadcast_in_dim %5956, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %5958 = stablehlo.multiply %5955, %5957 : tensor<16x2048xbf16> loc(#loc1761)
    %5959 = stablehlo.dot_general %5958, %arg293, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %5960 = stablehlo.reshape %5959 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %5961 = stablehlo.reshape %5960 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %5962 = call @jax_fused_moe_func_padded(%5958, %arg291, %arg292, %5961) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %5963 = stablehlo.convert %5962 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %5964 = stablehlo.convert %5943 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %5965 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %5966 = stablehlo.multiply %5964, %5965 : tensor<16x2048xf32> loc(#loc1766)
    %5967 = stablehlo.add %5963, %5966 : tensor<16x2048xf32> loc(#loc1767)
    %5968 = stablehlo.convert %5967 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %5969 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %5970 = stablehlo.power %5967, %5969 : tensor<16x2048xf32> loc(#loc1768)
    %5971 = stablehlo.reduce(%5970 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %5972 = stablehlo.broadcast_in_dim %5971, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %5973 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %5974 = stablehlo.divide %5972, %5973 : tensor<16x1xf32> loc(#loc1771)
    %5975 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %5976 = stablehlo.add %5974, %5975 : tensor<16x1xf32> loc(#loc1772)
    %5977 = stablehlo.rsqrt %5976 : tensor<16x1xf32> loc(#loc1678)
    %5978 = stablehlo.broadcast_in_dim %5977, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %5979 = stablehlo.multiply %5967, %5978 : tensor<16x2048xf32> loc(#loc1773)
    %5980 = stablehlo.convert %5979 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %5981 = stablehlo.broadcast_in_dim %arg299, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %5982 = stablehlo.broadcast_in_dim %5981, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %5983 = stablehlo.multiply %5980, %5982 : tensor<16x2048xbf16> loc(#loc1775)
    %5984 = stablehlo.dot_general %5983, %arg307, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %5985 = stablehlo.reshape %5984 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %5986 = stablehlo.slice %5985 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %5987 = stablehlo.reshape %5986 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %5988 = stablehlo.slice %5985 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %5989 = stablehlo.reshape %5988 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %5990 = stablehlo.slice %5985 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %5991 = stablehlo.reshape %5990 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %5992 = stablehlo.concatenate %5987, %5989, %5991, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %5993 = stablehlo.slice %5992 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %5994 = stablehlo.slice %5992 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %5995 = stablehlo.slice %5992 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %5996 = stablehlo.reshape %5993 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %5997 = stablehlo.convert %5996 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %5998 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %5999 = stablehlo.power %5997, %5998 : tensor<16x32x128xf32> loc(#loc1690)
    %6000 = stablehlo.reduce(%5999 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %6001 = stablehlo.broadcast_in_dim %6000, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %6002 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %6003 = stablehlo.divide %6001, %6002 : tensor<16x32x1xf32> loc(#loc1693)
    %6004 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %6005 = stablehlo.add %6003, %6004 : tensor<16x32x1xf32> loc(#loc1694)
    %6006 = stablehlo.rsqrt %6005 : tensor<16x32x1xf32> loc(#loc1678)
    %6007 = stablehlo.broadcast_in_dim %6006, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %6008 = stablehlo.multiply %5997, %6007 : tensor<16x32x128xf32> loc(#loc1695)
    %6009 = stablehlo.convert %6008 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %6010 = stablehlo.broadcast_in_dim %arg306, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %6011 = stablehlo.broadcast_in_dim %6010, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %6012 = stablehlo.multiply %6009, %6011 : tensor<16x32x128xbf16> loc(#loc1697)
    %6013 = stablehlo.reshape %6012 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %6014 = stablehlo.reshape %5994 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %6015 = stablehlo.convert %6014 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %6016 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %6017 = stablehlo.power %6015, %6016 : tensor<16x4x128xf32> loc(#loc1700)
    %6018 = stablehlo.reduce(%6017 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %6019 = stablehlo.broadcast_in_dim %6018, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %6020 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %6021 = stablehlo.divide %6019, %6020 : tensor<16x4x1xf32> loc(#loc1703)
    %6022 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %6023 = stablehlo.add %6021, %6022 : tensor<16x4x1xf32> loc(#loc1704)
    %6024 = stablehlo.rsqrt %6023 : tensor<16x4x1xf32> loc(#loc1678)
    %6025 = stablehlo.broadcast_in_dim %6024, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %6026 = stablehlo.multiply %6015, %6025 : tensor<16x4x128xf32> loc(#loc1705)
    %6027 = stablehlo.convert %6026 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %6028 = stablehlo.broadcast_in_dim %arg304, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %6029 = stablehlo.broadcast_in_dim %6028, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %6030 = stablehlo.multiply %6027, %6029 : tensor<16x4x128xbf16> loc(#loc1707)
    %6031 = stablehlo.reshape %6030 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %6032 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %6033 = stablehlo.compare  LT, %arg484, %6032,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %6034 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %6035 = stablehlo.add %arg484, %6034 : tensor<16xi32> loc(#loc1710)
    %6036 = stablehlo.select %6033, %6035, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %6037 = stablehlo.broadcast_in_dim %6036, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %6038 = "stablehlo.gather"(%arg10, %6037) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %6039 = stablehlo.slice %6038 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %6040 = stablehlo.slice %6038 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %6041 = stablehlo.reshape %6013 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %6042 = stablehlo.broadcast_in_dim %6039, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %6043 = stablehlo.broadcast_in_dim %6040, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %6044 = stablehlo.slice %6041 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %6045 = stablehlo.slice %6041 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %6046 = stablehlo.broadcast_in_dim %6042, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %6047 = stablehlo.multiply %6044, %6046 : tensor<16x32x64xbf16> loc(#loc1719)
    %6048 = stablehlo.broadcast_in_dim %6043, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %6049 = stablehlo.multiply %6045, %6048 : tensor<16x32x64xbf16> loc(#loc1720)
    %6050 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %6051 = stablehlo.multiply %6049, %6050 : tensor<16x32x64xbf16> loc(#loc1721)
    %6052 = stablehlo.subtract %6047, %6051 : tensor<16x32x64xbf16> loc(#loc1722)
    %6053 = stablehlo.broadcast_in_dim %6042, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %6054 = stablehlo.multiply %6045, %6053 : tensor<16x32x64xbf16> loc(#loc1723)
    %6055 = stablehlo.broadcast_in_dim %6043, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %6056 = stablehlo.multiply %6044, %6055 : tensor<16x32x64xbf16> loc(#loc1724)
    %6057 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %6058 = stablehlo.multiply %6056, %6057 : tensor<16x32x64xbf16> loc(#loc1725)
    %6059 = stablehlo.add %6054, %6058 : tensor<16x32x64xbf16> loc(#loc1726)
    %6060 = stablehlo.concatenate %6052, %6059, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %6061 = stablehlo.slice %6041 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %6062 = stablehlo.concatenate %6060, %6061, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %6063 = stablehlo.reshape %6062 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %6064 = stablehlo.reshape %6031 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %6065 = stablehlo.broadcast_in_dim %6039, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %6066 = stablehlo.broadcast_in_dim %6040, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %6067 = stablehlo.slice %6064 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %6068 = stablehlo.slice %6064 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %6069 = stablehlo.broadcast_in_dim %6065, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %6070 = stablehlo.multiply %6067, %6069 : tensor<16x4x64xbf16> loc(#loc1735)
    %6071 = stablehlo.broadcast_in_dim %6066, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %6072 = stablehlo.multiply %6068, %6071 : tensor<16x4x64xbf16> loc(#loc1736)
    %6073 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %6074 = stablehlo.multiply %6072, %6073 : tensor<16x4x64xbf16> loc(#loc1737)
    %6075 = stablehlo.subtract %6070, %6074 : tensor<16x4x64xbf16> loc(#loc1738)
    %6076 = stablehlo.broadcast_in_dim %6065, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %6077 = stablehlo.multiply %6068, %6076 : tensor<16x4x64xbf16> loc(#loc1739)
    %6078 = stablehlo.broadcast_in_dim %6066, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %6079 = stablehlo.multiply %6067, %6078 : tensor<16x4x64xbf16> loc(#loc1740)
    %6080 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %6081 = stablehlo.multiply %6079, %6080 : tensor<16x4x64xbf16> loc(#loc1741)
    %6082 = stablehlo.add %6077, %6081 : tensor<16x4x64xbf16> loc(#loc1742)
    %6083 = stablehlo.concatenate %6075, %6082, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %6084 = stablehlo.slice %6064 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %6085 = stablehlo.concatenate %6083, %6084, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %6086 = stablehlo.reshape %6085 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %6087:2 = call @_jax_attn_func(%arg474, %6063, %6086, %5995, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %6088 = stablehlo.dot_general %6087#1, %arg305, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %6089 = stablehlo.reshape %6088 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %6090 = stablehlo.reshape %6089 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %6091 = stablehlo.convert %6090 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %6092 = stablehlo.convert %5968 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %6093 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %6094 = stablehlo.multiply %6092, %6093 : tensor<16x2048xf32> loc(#loc1751)
    %6095 = stablehlo.add %6091, %6094 : tensor<16x2048xf32> loc(#loc1752)
    %6096 = stablehlo.convert %6095 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %6097 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %6098 = stablehlo.power %6095, %6097 : tensor<16x2048xf32> loc(#loc1754)
    %6099 = stablehlo.reduce(%6098 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %6100 = stablehlo.broadcast_in_dim %6099, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %6101 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %6102 = stablehlo.divide %6100, %6101 : tensor<16x1xf32> loc(#loc1757)
    %6103 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %6104 = stablehlo.add %6102, %6103 : tensor<16x1xf32> loc(#loc1758)
    %6105 = stablehlo.rsqrt %6104 : tensor<16x1xf32> loc(#loc1678)
    %6106 = stablehlo.broadcast_in_dim %6105, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %6107 = stablehlo.multiply %6095, %6106 : tensor<16x2048xf32> loc(#loc1759)
    %6108 = stablehlo.convert %6107 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %6109 = stablehlo.broadcast_in_dim %arg303, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %6110 = stablehlo.broadcast_in_dim %6109, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %6111 = stablehlo.multiply %6108, %6110 : tensor<16x2048xbf16> loc(#loc1761)
    %6112 = stablehlo.dot_general %6111, %arg302, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %6113 = stablehlo.reshape %6112 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %6114 = stablehlo.reshape %6113 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %6115 = call @jax_fused_moe_func_padded(%6111, %arg300, %arg301, %6114) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %6116 = stablehlo.convert %6115 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %6117 = stablehlo.convert %6096 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %6118 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %6119 = stablehlo.multiply %6117, %6118 : tensor<16x2048xf32> loc(#loc1766)
    %6120 = stablehlo.add %6116, %6119 : tensor<16x2048xf32> loc(#loc1767)
    %6121 = stablehlo.convert %6120 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %6122 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %6123 = stablehlo.power %6120, %6122 : tensor<16x2048xf32> loc(#loc1768)
    %6124 = stablehlo.reduce(%6123 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %6125 = stablehlo.broadcast_in_dim %6124, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %6126 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %6127 = stablehlo.divide %6125, %6126 : tensor<16x1xf32> loc(#loc1771)
    %6128 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %6129 = stablehlo.add %6127, %6128 : tensor<16x1xf32> loc(#loc1772)
    %6130 = stablehlo.rsqrt %6129 : tensor<16x1xf32> loc(#loc1678)
    %6131 = stablehlo.broadcast_in_dim %6130, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %6132 = stablehlo.multiply %6120, %6131 : tensor<16x2048xf32> loc(#loc1773)
    %6133 = stablehlo.convert %6132 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %6134 = stablehlo.broadcast_in_dim %arg317, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %6135 = stablehlo.broadcast_in_dim %6134, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %6136 = stablehlo.multiply %6133, %6135 : tensor<16x2048xbf16> loc(#loc1775)
    %6137 = stablehlo.dot_general %6136, %arg325, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %6138 = stablehlo.reshape %6137 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %6139 = stablehlo.slice %6138 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %6140 = stablehlo.reshape %6139 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %6141 = stablehlo.slice %6138 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %6142 = stablehlo.reshape %6141 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %6143 = stablehlo.slice %6138 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %6144 = stablehlo.reshape %6143 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %6145 = stablehlo.concatenate %6140, %6142, %6144, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %6146 = stablehlo.slice %6145 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %6147 = stablehlo.slice %6145 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %6148 = stablehlo.slice %6145 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %6149 = stablehlo.reshape %6146 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %6150 = stablehlo.convert %6149 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %6151 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %6152 = stablehlo.power %6150, %6151 : tensor<16x32x128xf32> loc(#loc1690)
    %6153 = stablehlo.reduce(%6152 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %6154 = stablehlo.broadcast_in_dim %6153, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %6155 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %6156 = stablehlo.divide %6154, %6155 : tensor<16x32x1xf32> loc(#loc1693)
    %6157 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %6158 = stablehlo.add %6156, %6157 : tensor<16x32x1xf32> loc(#loc1694)
    %6159 = stablehlo.rsqrt %6158 : tensor<16x32x1xf32> loc(#loc1678)
    %6160 = stablehlo.broadcast_in_dim %6159, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %6161 = stablehlo.multiply %6150, %6160 : tensor<16x32x128xf32> loc(#loc1695)
    %6162 = stablehlo.convert %6161 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %6163 = stablehlo.broadcast_in_dim %arg324, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %6164 = stablehlo.broadcast_in_dim %6163, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %6165 = stablehlo.multiply %6162, %6164 : tensor<16x32x128xbf16> loc(#loc1697)
    %6166 = stablehlo.reshape %6165 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %6167 = stablehlo.reshape %6147 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %6168 = stablehlo.convert %6167 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %6169 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %6170 = stablehlo.power %6168, %6169 : tensor<16x4x128xf32> loc(#loc1700)
    %6171 = stablehlo.reduce(%6170 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %6172 = stablehlo.broadcast_in_dim %6171, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %6173 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %6174 = stablehlo.divide %6172, %6173 : tensor<16x4x1xf32> loc(#loc1703)
    %6175 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %6176 = stablehlo.add %6174, %6175 : tensor<16x4x1xf32> loc(#loc1704)
    %6177 = stablehlo.rsqrt %6176 : tensor<16x4x1xf32> loc(#loc1678)
    %6178 = stablehlo.broadcast_in_dim %6177, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %6179 = stablehlo.multiply %6168, %6178 : tensor<16x4x128xf32> loc(#loc1705)
    %6180 = stablehlo.convert %6179 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %6181 = stablehlo.broadcast_in_dim %arg322, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %6182 = stablehlo.broadcast_in_dim %6181, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %6183 = stablehlo.multiply %6180, %6182 : tensor<16x4x128xbf16> loc(#loc1707)
    %6184 = stablehlo.reshape %6183 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %6185 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %6186 = stablehlo.compare  LT, %arg484, %6185,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %6187 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %6188 = stablehlo.add %arg484, %6187 : tensor<16xi32> loc(#loc1710)
    %6189 = stablehlo.select %6186, %6188, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %6190 = stablehlo.broadcast_in_dim %6189, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %6191 = "stablehlo.gather"(%arg10, %6190) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %6192 = stablehlo.slice %6191 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %6193 = stablehlo.slice %6191 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %6194 = stablehlo.reshape %6166 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %6195 = stablehlo.broadcast_in_dim %6192, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %6196 = stablehlo.broadcast_in_dim %6193, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %6197 = stablehlo.slice %6194 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %6198 = stablehlo.slice %6194 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %6199 = stablehlo.broadcast_in_dim %6195, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %6200 = stablehlo.multiply %6197, %6199 : tensor<16x32x64xbf16> loc(#loc1719)
    %6201 = stablehlo.broadcast_in_dim %6196, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %6202 = stablehlo.multiply %6198, %6201 : tensor<16x32x64xbf16> loc(#loc1720)
    %6203 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %6204 = stablehlo.multiply %6202, %6203 : tensor<16x32x64xbf16> loc(#loc1721)
    %6205 = stablehlo.subtract %6200, %6204 : tensor<16x32x64xbf16> loc(#loc1722)
    %6206 = stablehlo.broadcast_in_dim %6195, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %6207 = stablehlo.multiply %6198, %6206 : tensor<16x32x64xbf16> loc(#loc1723)
    %6208 = stablehlo.broadcast_in_dim %6196, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %6209 = stablehlo.multiply %6197, %6208 : tensor<16x32x64xbf16> loc(#loc1724)
    %6210 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %6211 = stablehlo.multiply %6209, %6210 : tensor<16x32x64xbf16> loc(#loc1725)
    %6212 = stablehlo.add %6207, %6211 : tensor<16x32x64xbf16> loc(#loc1726)
    %6213 = stablehlo.concatenate %6205, %6212, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %6214 = stablehlo.slice %6194 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %6215 = stablehlo.concatenate %6213, %6214, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %6216 = stablehlo.reshape %6215 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %6217 = stablehlo.reshape %6184 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %6218 = stablehlo.broadcast_in_dim %6192, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %6219 = stablehlo.broadcast_in_dim %6193, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %6220 = stablehlo.slice %6217 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %6221 = stablehlo.slice %6217 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %6222 = stablehlo.broadcast_in_dim %6218, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %6223 = stablehlo.multiply %6220, %6222 : tensor<16x4x64xbf16> loc(#loc1735)
    %6224 = stablehlo.broadcast_in_dim %6219, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %6225 = stablehlo.multiply %6221, %6224 : tensor<16x4x64xbf16> loc(#loc1736)
    %6226 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %6227 = stablehlo.multiply %6225, %6226 : tensor<16x4x64xbf16> loc(#loc1737)
    %6228 = stablehlo.subtract %6223, %6227 : tensor<16x4x64xbf16> loc(#loc1738)
    %6229 = stablehlo.broadcast_in_dim %6218, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %6230 = stablehlo.multiply %6221, %6229 : tensor<16x4x64xbf16> loc(#loc1739)
    %6231 = stablehlo.broadcast_in_dim %6219, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %6232 = stablehlo.multiply %6220, %6231 : tensor<16x4x64xbf16> loc(#loc1740)
    %6233 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %6234 = stablehlo.multiply %6232, %6233 : tensor<16x4x64xbf16> loc(#loc1741)
    %6235 = stablehlo.add %6230, %6234 : tensor<16x4x64xbf16> loc(#loc1742)
    %6236 = stablehlo.concatenate %6228, %6235, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %6237 = stablehlo.slice %6217 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %6238 = stablehlo.concatenate %6236, %6237, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %6239 = stablehlo.reshape %6238 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %6240:2 = call @_jax_attn_func(%arg475, %6216, %6239, %6148, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %6241 = stablehlo.dot_general %6240#1, %arg323, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %6242 = stablehlo.reshape %6241 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %6243 = stablehlo.reshape %6242 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %6244 = stablehlo.convert %6243 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %6245 = stablehlo.convert %6121 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %6246 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %6247 = stablehlo.multiply %6245, %6246 : tensor<16x2048xf32> loc(#loc1751)
    %6248 = stablehlo.add %6244, %6247 : tensor<16x2048xf32> loc(#loc1752)
    %6249 = stablehlo.convert %6248 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %6250 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %6251 = stablehlo.power %6248, %6250 : tensor<16x2048xf32> loc(#loc1754)
    %6252 = stablehlo.reduce(%6251 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %6253 = stablehlo.broadcast_in_dim %6252, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %6254 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %6255 = stablehlo.divide %6253, %6254 : tensor<16x1xf32> loc(#loc1757)
    %6256 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %6257 = stablehlo.add %6255, %6256 : tensor<16x1xf32> loc(#loc1758)
    %6258 = stablehlo.rsqrt %6257 : tensor<16x1xf32> loc(#loc1678)
    %6259 = stablehlo.broadcast_in_dim %6258, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %6260 = stablehlo.multiply %6248, %6259 : tensor<16x2048xf32> loc(#loc1759)
    %6261 = stablehlo.convert %6260 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %6262 = stablehlo.broadcast_in_dim %arg321, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %6263 = stablehlo.broadcast_in_dim %6262, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %6264 = stablehlo.multiply %6261, %6263 : tensor<16x2048xbf16> loc(#loc1761)
    %6265 = stablehlo.dot_general %6264, %arg320, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %6266 = stablehlo.reshape %6265 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %6267 = stablehlo.reshape %6266 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %6268 = call @jax_fused_moe_func_padded(%6264, %arg318, %arg319, %6267) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %6269 = stablehlo.convert %6268 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %6270 = stablehlo.convert %6249 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %6271 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %6272 = stablehlo.multiply %6270, %6271 : tensor<16x2048xf32> loc(#loc1766)
    %6273 = stablehlo.add %6269, %6272 : tensor<16x2048xf32> loc(#loc1767)
    %6274 = stablehlo.convert %6273 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %6275 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %6276 = stablehlo.power %6273, %6275 : tensor<16x2048xf32> loc(#loc1768)
    %6277 = stablehlo.reduce(%6276 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %6278 = stablehlo.broadcast_in_dim %6277, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %6279 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %6280 = stablehlo.divide %6278, %6279 : tensor<16x1xf32> loc(#loc1771)
    %6281 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %6282 = stablehlo.add %6280, %6281 : tensor<16x1xf32> loc(#loc1772)
    %6283 = stablehlo.rsqrt %6282 : tensor<16x1xf32> loc(#loc1678)
    %6284 = stablehlo.broadcast_in_dim %6283, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %6285 = stablehlo.multiply %6273, %6284 : tensor<16x2048xf32> loc(#loc1773)
    %6286 = stablehlo.convert %6285 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %6287 = stablehlo.broadcast_in_dim %arg326, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %6288 = stablehlo.broadcast_in_dim %6287, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %6289 = stablehlo.multiply %6286, %6288 : tensor<16x2048xbf16> loc(#loc1775)
    %6290 = stablehlo.dot_general %6289, %arg334, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %6291 = stablehlo.reshape %6290 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %6292 = stablehlo.slice %6291 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %6293 = stablehlo.reshape %6292 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %6294 = stablehlo.slice %6291 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %6295 = stablehlo.reshape %6294 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %6296 = stablehlo.slice %6291 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %6297 = stablehlo.reshape %6296 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %6298 = stablehlo.concatenate %6293, %6295, %6297, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %6299 = stablehlo.slice %6298 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %6300 = stablehlo.slice %6298 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %6301 = stablehlo.slice %6298 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %6302 = stablehlo.reshape %6299 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %6303 = stablehlo.convert %6302 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %6304 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %6305 = stablehlo.power %6303, %6304 : tensor<16x32x128xf32> loc(#loc1690)
    %6306 = stablehlo.reduce(%6305 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %6307 = stablehlo.broadcast_in_dim %6306, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %6308 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %6309 = stablehlo.divide %6307, %6308 : tensor<16x32x1xf32> loc(#loc1693)
    %6310 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %6311 = stablehlo.add %6309, %6310 : tensor<16x32x1xf32> loc(#loc1694)
    %6312 = stablehlo.rsqrt %6311 : tensor<16x32x1xf32> loc(#loc1678)
    %6313 = stablehlo.broadcast_in_dim %6312, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %6314 = stablehlo.multiply %6303, %6313 : tensor<16x32x128xf32> loc(#loc1695)
    %6315 = stablehlo.convert %6314 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %6316 = stablehlo.broadcast_in_dim %arg333, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %6317 = stablehlo.broadcast_in_dim %6316, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %6318 = stablehlo.multiply %6315, %6317 : tensor<16x32x128xbf16> loc(#loc1697)
    %6319 = stablehlo.reshape %6318 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %6320 = stablehlo.reshape %6300 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %6321 = stablehlo.convert %6320 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %6322 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %6323 = stablehlo.power %6321, %6322 : tensor<16x4x128xf32> loc(#loc1700)
    %6324 = stablehlo.reduce(%6323 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %6325 = stablehlo.broadcast_in_dim %6324, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %6326 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %6327 = stablehlo.divide %6325, %6326 : tensor<16x4x1xf32> loc(#loc1703)
    %6328 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %6329 = stablehlo.add %6327, %6328 : tensor<16x4x1xf32> loc(#loc1704)
    %6330 = stablehlo.rsqrt %6329 : tensor<16x4x1xf32> loc(#loc1678)
    %6331 = stablehlo.broadcast_in_dim %6330, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %6332 = stablehlo.multiply %6321, %6331 : tensor<16x4x128xf32> loc(#loc1705)
    %6333 = stablehlo.convert %6332 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %6334 = stablehlo.broadcast_in_dim %arg331, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %6335 = stablehlo.broadcast_in_dim %6334, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %6336 = stablehlo.multiply %6333, %6335 : tensor<16x4x128xbf16> loc(#loc1707)
    %6337 = stablehlo.reshape %6336 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %6338 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %6339 = stablehlo.compare  LT, %arg484, %6338,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %6340 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %6341 = stablehlo.add %arg484, %6340 : tensor<16xi32> loc(#loc1710)
    %6342 = stablehlo.select %6339, %6341, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %6343 = stablehlo.broadcast_in_dim %6342, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %6344 = "stablehlo.gather"(%arg10, %6343) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %6345 = stablehlo.slice %6344 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %6346 = stablehlo.slice %6344 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %6347 = stablehlo.reshape %6319 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %6348 = stablehlo.broadcast_in_dim %6345, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %6349 = stablehlo.broadcast_in_dim %6346, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %6350 = stablehlo.slice %6347 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %6351 = stablehlo.slice %6347 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %6352 = stablehlo.broadcast_in_dim %6348, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %6353 = stablehlo.multiply %6350, %6352 : tensor<16x32x64xbf16> loc(#loc1719)
    %6354 = stablehlo.broadcast_in_dim %6349, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %6355 = stablehlo.multiply %6351, %6354 : tensor<16x32x64xbf16> loc(#loc1720)
    %6356 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %6357 = stablehlo.multiply %6355, %6356 : tensor<16x32x64xbf16> loc(#loc1721)
    %6358 = stablehlo.subtract %6353, %6357 : tensor<16x32x64xbf16> loc(#loc1722)
    %6359 = stablehlo.broadcast_in_dim %6348, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %6360 = stablehlo.multiply %6351, %6359 : tensor<16x32x64xbf16> loc(#loc1723)
    %6361 = stablehlo.broadcast_in_dim %6349, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %6362 = stablehlo.multiply %6350, %6361 : tensor<16x32x64xbf16> loc(#loc1724)
    %6363 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %6364 = stablehlo.multiply %6362, %6363 : tensor<16x32x64xbf16> loc(#loc1725)
    %6365 = stablehlo.add %6360, %6364 : tensor<16x32x64xbf16> loc(#loc1726)
    %6366 = stablehlo.concatenate %6358, %6365, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %6367 = stablehlo.slice %6347 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %6368 = stablehlo.concatenate %6366, %6367, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %6369 = stablehlo.reshape %6368 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %6370 = stablehlo.reshape %6337 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %6371 = stablehlo.broadcast_in_dim %6345, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %6372 = stablehlo.broadcast_in_dim %6346, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %6373 = stablehlo.slice %6370 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %6374 = stablehlo.slice %6370 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %6375 = stablehlo.broadcast_in_dim %6371, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %6376 = stablehlo.multiply %6373, %6375 : tensor<16x4x64xbf16> loc(#loc1735)
    %6377 = stablehlo.broadcast_in_dim %6372, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %6378 = stablehlo.multiply %6374, %6377 : tensor<16x4x64xbf16> loc(#loc1736)
    %6379 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %6380 = stablehlo.multiply %6378, %6379 : tensor<16x4x64xbf16> loc(#loc1737)
    %6381 = stablehlo.subtract %6376, %6380 : tensor<16x4x64xbf16> loc(#loc1738)
    %6382 = stablehlo.broadcast_in_dim %6371, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %6383 = stablehlo.multiply %6374, %6382 : tensor<16x4x64xbf16> loc(#loc1739)
    %6384 = stablehlo.broadcast_in_dim %6372, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %6385 = stablehlo.multiply %6373, %6384 : tensor<16x4x64xbf16> loc(#loc1740)
    %6386 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %6387 = stablehlo.multiply %6385, %6386 : tensor<16x4x64xbf16> loc(#loc1741)
    %6388 = stablehlo.add %6383, %6387 : tensor<16x4x64xbf16> loc(#loc1742)
    %6389 = stablehlo.concatenate %6381, %6388, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %6390 = stablehlo.slice %6370 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %6391 = stablehlo.concatenate %6389, %6390, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %6392 = stablehlo.reshape %6391 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %6393:2 = call @_jax_attn_func(%arg476, %6369, %6392, %6301, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %6394 = stablehlo.dot_general %6393#1, %arg332, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %6395 = stablehlo.reshape %6394 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %6396 = stablehlo.reshape %6395 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %6397 = stablehlo.convert %6396 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %6398 = stablehlo.convert %6274 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %6399 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %6400 = stablehlo.multiply %6398, %6399 : tensor<16x2048xf32> loc(#loc1751)
    %6401 = stablehlo.add %6397, %6400 : tensor<16x2048xf32> loc(#loc1752)
    %6402 = stablehlo.convert %6401 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %6403 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %6404 = stablehlo.power %6401, %6403 : tensor<16x2048xf32> loc(#loc1754)
    %6405 = stablehlo.reduce(%6404 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %6406 = stablehlo.broadcast_in_dim %6405, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %6407 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %6408 = stablehlo.divide %6406, %6407 : tensor<16x1xf32> loc(#loc1757)
    %6409 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %6410 = stablehlo.add %6408, %6409 : tensor<16x1xf32> loc(#loc1758)
    %6411 = stablehlo.rsqrt %6410 : tensor<16x1xf32> loc(#loc1678)
    %6412 = stablehlo.broadcast_in_dim %6411, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %6413 = stablehlo.multiply %6401, %6412 : tensor<16x2048xf32> loc(#loc1759)
    %6414 = stablehlo.convert %6413 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %6415 = stablehlo.broadcast_in_dim %arg330, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %6416 = stablehlo.broadcast_in_dim %6415, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %6417 = stablehlo.multiply %6414, %6416 : tensor<16x2048xbf16> loc(#loc1761)
    %6418 = stablehlo.dot_general %6417, %arg329, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %6419 = stablehlo.reshape %6418 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %6420 = stablehlo.reshape %6419 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %6421 = call @jax_fused_moe_func_padded(%6417, %arg327, %arg328, %6420) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %6422 = stablehlo.convert %6421 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %6423 = stablehlo.convert %6402 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %6424 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %6425 = stablehlo.multiply %6423, %6424 : tensor<16x2048xf32> loc(#loc1766)
    %6426 = stablehlo.add %6422, %6425 : tensor<16x2048xf32> loc(#loc1767)
    %6427 = stablehlo.convert %6426 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %6428 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %6429 = stablehlo.power %6426, %6428 : tensor<16x2048xf32> loc(#loc1768)
    %6430 = stablehlo.reduce(%6429 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %6431 = stablehlo.broadcast_in_dim %6430, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %6432 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %6433 = stablehlo.divide %6431, %6432 : tensor<16x1xf32> loc(#loc1771)
    %6434 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %6435 = stablehlo.add %6433, %6434 : tensor<16x1xf32> loc(#loc1772)
    %6436 = stablehlo.rsqrt %6435 : tensor<16x1xf32> loc(#loc1678)
    %6437 = stablehlo.broadcast_in_dim %6436, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %6438 = stablehlo.multiply %6426, %6437 : tensor<16x2048xf32> loc(#loc1773)
    %6439 = stablehlo.convert %6438 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %6440 = stablehlo.broadcast_in_dim %arg335, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %6441 = stablehlo.broadcast_in_dim %6440, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %6442 = stablehlo.multiply %6439, %6441 : tensor<16x2048xbf16> loc(#loc1775)
    %6443 = stablehlo.dot_general %6442, %arg343, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %6444 = stablehlo.reshape %6443 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %6445 = stablehlo.slice %6444 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %6446 = stablehlo.reshape %6445 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %6447 = stablehlo.slice %6444 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %6448 = stablehlo.reshape %6447 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %6449 = stablehlo.slice %6444 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %6450 = stablehlo.reshape %6449 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %6451 = stablehlo.concatenate %6446, %6448, %6450, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %6452 = stablehlo.slice %6451 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %6453 = stablehlo.slice %6451 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %6454 = stablehlo.slice %6451 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %6455 = stablehlo.reshape %6452 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %6456 = stablehlo.convert %6455 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %6457 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %6458 = stablehlo.power %6456, %6457 : tensor<16x32x128xf32> loc(#loc1690)
    %6459 = stablehlo.reduce(%6458 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %6460 = stablehlo.broadcast_in_dim %6459, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %6461 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %6462 = stablehlo.divide %6460, %6461 : tensor<16x32x1xf32> loc(#loc1693)
    %6463 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %6464 = stablehlo.add %6462, %6463 : tensor<16x32x1xf32> loc(#loc1694)
    %6465 = stablehlo.rsqrt %6464 : tensor<16x32x1xf32> loc(#loc1678)
    %6466 = stablehlo.broadcast_in_dim %6465, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %6467 = stablehlo.multiply %6456, %6466 : tensor<16x32x128xf32> loc(#loc1695)
    %6468 = stablehlo.convert %6467 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %6469 = stablehlo.broadcast_in_dim %arg342, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %6470 = stablehlo.broadcast_in_dim %6469, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %6471 = stablehlo.multiply %6468, %6470 : tensor<16x32x128xbf16> loc(#loc1697)
    %6472 = stablehlo.reshape %6471 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %6473 = stablehlo.reshape %6453 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %6474 = stablehlo.convert %6473 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %6475 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %6476 = stablehlo.power %6474, %6475 : tensor<16x4x128xf32> loc(#loc1700)
    %6477 = stablehlo.reduce(%6476 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %6478 = stablehlo.broadcast_in_dim %6477, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %6479 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %6480 = stablehlo.divide %6478, %6479 : tensor<16x4x1xf32> loc(#loc1703)
    %6481 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %6482 = stablehlo.add %6480, %6481 : tensor<16x4x1xf32> loc(#loc1704)
    %6483 = stablehlo.rsqrt %6482 : tensor<16x4x1xf32> loc(#loc1678)
    %6484 = stablehlo.broadcast_in_dim %6483, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %6485 = stablehlo.multiply %6474, %6484 : tensor<16x4x128xf32> loc(#loc1705)
    %6486 = stablehlo.convert %6485 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %6487 = stablehlo.broadcast_in_dim %arg340, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %6488 = stablehlo.broadcast_in_dim %6487, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %6489 = stablehlo.multiply %6486, %6488 : tensor<16x4x128xbf16> loc(#loc1707)
    %6490 = stablehlo.reshape %6489 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %6491 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %6492 = stablehlo.compare  LT, %arg484, %6491,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %6493 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %6494 = stablehlo.add %arg484, %6493 : tensor<16xi32> loc(#loc1710)
    %6495 = stablehlo.select %6492, %6494, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %6496 = stablehlo.broadcast_in_dim %6495, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %6497 = "stablehlo.gather"(%arg10, %6496) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %6498 = stablehlo.slice %6497 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %6499 = stablehlo.slice %6497 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %6500 = stablehlo.reshape %6472 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %6501 = stablehlo.broadcast_in_dim %6498, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %6502 = stablehlo.broadcast_in_dim %6499, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %6503 = stablehlo.slice %6500 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %6504 = stablehlo.slice %6500 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %6505 = stablehlo.broadcast_in_dim %6501, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %6506 = stablehlo.multiply %6503, %6505 : tensor<16x32x64xbf16> loc(#loc1719)
    %6507 = stablehlo.broadcast_in_dim %6502, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %6508 = stablehlo.multiply %6504, %6507 : tensor<16x32x64xbf16> loc(#loc1720)
    %6509 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %6510 = stablehlo.multiply %6508, %6509 : tensor<16x32x64xbf16> loc(#loc1721)
    %6511 = stablehlo.subtract %6506, %6510 : tensor<16x32x64xbf16> loc(#loc1722)
    %6512 = stablehlo.broadcast_in_dim %6501, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %6513 = stablehlo.multiply %6504, %6512 : tensor<16x32x64xbf16> loc(#loc1723)
    %6514 = stablehlo.broadcast_in_dim %6502, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %6515 = stablehlo.multiply %6503, %6514 : tensor<16x32x64xbf16> loc(#loc1724)
    %6516 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %6517 = stablehlo.multiply %6515, %6516 : tensor<16x32x64xbf16> loc(#loc1725)
    %6518 = stablehlo.add %6513, %6517 : tensor<16x32x64xbf16> loc(#loc1726)
    %6519 = stablehlo.concatenate %6511, %6518, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %6520 = stablehlo.slice %6500 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %6521 = stablehlo.concatenate %6519, %6520, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %6522 = stablehlo.reshape %6521 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %6523 = stablehlo.reshape %6490 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %6524 = stablehlo.broadcast_in_dim %6498, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %6525 = stablehlo.broadcast_in_dim %6499, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %6526 = stablehlo.slice %6523 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %6527 = stablehlo.slice %6523 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %6528 = stablehlo.broadcast_in_dim %6524, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %6529 = stablehlo.multiply %6526, %6528 : tensor<16x4x64xbf16> loc(#loc1735)
    %6530 = stablehlo.broadcast_in_dim %6525, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %6531 = stablehlo.multiply %6527, %6530 : tensor<16x4x64xbf16> loc(#loc1736)
    %6532 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %6533 = stablehlo.multiply %6531, %6532 : tensor<16x4x64xbf16> loc(#loc1737)
    %6534 = stablehlo.subtract %6529, %6533 : tensor<16x4x64xbf16> loc(#loc1738)
    %6535 = stablehlo.broadcast_in_dim %6524, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %6536 = stablehlo.multiply %6527, %6535 : tensor<16x4x64xbf16> loc(#loc1739)
    %6537 = stablehlo.broadcast_in_dim %6525, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %6538 = stablehlo.multiply %6526, %6537 : tensor<16x4x64xbf16> loc(#loc1740)
    %6539 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %6540 = stablehlo.multiply %6538, %6539 : tensor<16x4x64xbf16> loc(#loc1741)
    %6541 = stablehlo.add %6536, %6540 : tensor<16x4x64xbf16> loc(#loc1742)
    %6542 = stablehlo.concatenate %6534, %6541, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %6543 = stablehlo.slice %6523 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %6544 = stablehlo.concatenate %6542, %6543, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %6545 = stablehlo.reshape %6544 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %6546:2 = call @_jax_attn_func(%arg477, %6522, %6545, %6454, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %6547 = stablehlo.dot_general %6546#1, %arg341, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %6548 = stablehlo.reshape %6547 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %6549 = stablehlo.reshape %6548 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %6550 = stablehlo.convert %6549 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %6551 = stablehlo.convert %6427 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %6552 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %6553 = stablehlo.multiply %6551, %6552 : tensor<16x2048xf32> loc(#loc1751)
    %6554 = stablehlo.add %6550, %6553 : tensor<16x2048xf32> loc(#loc1752)
    %6555 = stablehlo.convert %6554 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %6556 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %6557 = stablehlo.power %6554, %6556 : tensor<16x2048xf32> loc(#loc1754)
    %6558 = stablehlo.reduce(%6557 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %6559 = stablehlo.broadcast_in_dim %6558, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %6560 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %6561 = stablehlo.divide %6559, %6560 : tensor<16x1xf32> loc(#loc1757)
    %6562 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %6563 = stablehlo.add %6561, %6562 : tensor<16x1xf32> loc(#loc1758)
    %6564 = stablehlo.rsqrt %6563 : tensor<16x1xf32> loc(#loc1678)
    %6565 = stablehlo.broadcast_in_dim %6564, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %6566 = stablehlo.multiply %6554, %6565 : tensor<16x2048xf32> loc(#loc1759)
    %6567 = stablehlo.convert %6566 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %6568 = stablehlo.broadcast_in_dim %arg339, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %6569 = stablehlo.broadcast_in_dim %6568, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %6570 = stablehlo.multiply %6567, %6569 : tensor<16x2048xbf16> loc(#loc1761)
    %6571 = stablehlo.dot_general %6570, %arg338, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %6572 = stablehlo.reshape %6571 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %6573 = stablehlo.reshape %6572 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %6574 = call @jax_fused_moe_func_padded(%6570, %arg336, %arg337, %6573) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %6575 = stablehlo.convert %6574 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %6576 = stablehlo.convert %6555 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %6577 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %6578 = stablehlo.multiply %6576, %6577 : tensor<16x2048xf32> loc(#loc1766)
    %6579 = stablehlo.add %6575, %6578 : tensor<16x2048xf32> loc(#loc1767)
    %6580 = stablehlo.convert %6579 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %6581 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %6582 = stablehlo.power %6579, %6581 : tensor<16x2048xf32> loc(#loc1768)
    %6583 = stablehlo.reduce(%6582 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %6584 = stablehlo.broadcast_in_dim %6583, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %6585 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %6586 = stablehlo.divide %6584, %6585 : tensor<16x1xf32> loc(#loc1771)
    %6587 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %6588 = stablehlo.add %6586, %6587 : tensor<16x1xf32> loc(#loc1772)
    %6589 = stablehlo.rsqrt %6588 : tensor<16x1xf32> loc(#loc1678)
    %6590 = stablehlo.broadcast_in_dim %6589, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %6591 = stablehlo.multiply %6579, %6590 : tensor<16x2048xf32> loc(#loc1773)
    %6592 = stablehlo.convert %6591 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %6593 = stablehlo.broadcast_in_dim %arg344, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %6594 = stablehlo.broadcast_in_dim %6593, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %6595 = stablehlo.multiply %6592, %6594 : tensor<16x2048xbf16> loc(#loc1775)
    %6596 = stablehlo.dot_general %6595, %arg352, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %6597 = stablehlo.reshape %6596 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %6598 = stablehlo.slice %6597 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %6599 = stablehlo.reshape %6598 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %6600 = stablehlo.slice %6597 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %6601 = stablehlo.reshape %6600 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %6602 = stablehlo.slice %6597 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %6603 = stablehlo.reshape %6602 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %6604 = stablehlo.concatenate %6599, %6601, %6603, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %6605 = stablehlo.slice %6604 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %6606 = stablehlo.slice %6604 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %6607 = stablehlo.slice %6604 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %6608 = stablehlo.reshape %6605 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %6609 = stablehlo.convert %6608 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %6610 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %6611 = stablehlo.power %6609, %6610 : tensor<16x32x128xf32> loc(#loc1690)
    %6612 = stablehlo.reduce(%6611 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %6613 = stablehlo.broadcast_in_dim %6612, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %6614 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %6615 = stablehlo.divide %6613, %6614 : tensor<16x32x1xf32> loc(#loc1693)
    %6616 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %6617 = stablehlo.add %6615, %6616 : tensor<16x32x1xf32> loc(#loc1694)
    %6618 = stablehlo.rsqrt %6617 : tensor<16x32x1xf32> loc(#loc1678)
    %6619 = stablehlo.broadcast_in_dim %6618, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %6620 = stablehlo.multiply %6609, %6619 : tensor<16x32x128xf32> loc(#loc1695)
    %6621 = stablehlo.convert %6620 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %6622 = stablehlo.broadcast_in_dim %arg351, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %6623 = stablehlo.broadcast_in_dim %6622, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %6624 = stablehlo.multiply %6621, %6623 : tensor<16x32x128xbf16> loc(#loc1697)
    %6625 = stablehlo.reshape %6624 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %6626 = stablehlo.reshape %6606 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %6627 = stablehlo.convert %6626 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %6628 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %6629 = stablehlo.power %6627, %6628 : tensor<16x4x128xf32> loc(#loc1700)
    %6630 = stablehlo.reduce(%6629 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %6631 = stablehlo.broadcast_in_dim %6630, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %6632 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %6633 = stablehlo.divide %6631, %6632 : tensor<16x4x1xf32> loc(#loc1703)
    %6634 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %6635 = stablehlo.add %6633, %6634 : tensor<16x4x1xf32> loc(#loc1704)
    %6636 = stablehlo.rsqrt %6635 : tensor<16x4x1xf32> loc(#loc1678)
    %6637 = stablehlo.broadcast_in_dim %6636, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %6638 = stablehlo.multiply %6627, %6637 : tensor<16x4x128xf32> loc(#loc1705)
    %6639 = stablehlo.convert %6638 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %6640 = stablehlo.broadcast_in_dim %arg349, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %6641 = stablehlo.broadcast_in_dim %6640, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %6642 = stablehlo.multiply %6639, %6641 : tensor<16x4x128xbf16> loc(#loc1707)
    %6643 = stablehlo.reshape %6642 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %6644 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %6645 = stablehlo.compare  LT, %arg484, %6644,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %6646 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %6647 = stablehlo.add %arg484, %6646 : tensor<16xi32> loc(#loc1710)
    %6648 = stablehlo.select %6645, %6647, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %6649 = stablehlo.broadcast_in_dim %6648, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %6650 = "stablehlo.gather"(%arg10, %6649) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %6651 = stablehlo.slice %6650 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %6652 = stablehlo.slice %6650 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %6653 = stablehlo.reshape %6625 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %6654 = stablehlo.broadcast_in_dim %6651, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %6655 = stablehlo.broadcast_in_dim %6652, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %6656 = stablehlo.slice %6653 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %6657 = stablehlo.slice %6653 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %6658 = stablehlo.broadcast_in_dim %6654, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %6659 = stablehlo.multiply %6656, %6658 : tensor<16x32x64xbf16> loc(#loc1719)
    %6660 = stablehlo.broadcast_in_dim %6655, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %6661 = stablehlo.multiply %6657, %6660 : tensor<16x32x64xbf16> loc(#loc1720)
    %6662 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %6663 = stablehlo.multiply %6661, %6662 : tensor<16x32x64xbf16> loc(#loc1721)
    %6664 = stablehlo.subtract %6659, %6663 : tensor<16x32x64xbf16> loc(#loc1722)
    %6665 = stablehlo.broadcast_in_dim %6654, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %6666 = stablehlo.multiply %6657, %6665 : tensor<16x32x64xbf16> loc(#loc1723)
    %6667 = stablehlo.broadcast_in_dim %6655, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %6668 = stablehlo.multiply %6656, %6667 : tensor<16x32x64xbf16> loc(#loc1724)
    %6669 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %6670 = stablehlo.multiply %6668, %6669 : tensor<16x32x64xbf16> loc(#loc1725)
    %6671 = stablehlo.add %6666, %6670 : tensor<16x32x64xbf16> loc(#loc1726)
    %6672 = stablehlo.concatenate %6664, %6671, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %6673 = stablehlo.slice %6653 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %6674 = stablehlo.concatenate %6672, %6673, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %6675 = stablehlo.reshape %6674 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %6676 = stablehlo.reshape %6643 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %6677 = stablehlo.broadcast_in_dim %6651, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %6678 = stablehlo.broadcast_in_dim %6652, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %6679 = stablehlo.slice %6676 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %6680 = stablehlo.slice %6676 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %6681 = stablehlo.broadcast_in_dim %6677, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %6682 = stablehlo.multiply %6679, %6681 : tensor<16x4x64xbf16> loc(#loc1735)
    %6683 = stablehlo.broadcast_in_dim %6678, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %6684 = stablehlo.multiply %6680, %6683 : tensor<16x4x64xbf16> loc(#loc1736)
    %6685 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %6686 = stablehlo.multiply %6684, %6685 : tensor<16x4x64xbf16> loc(#loc1737)
    %6687 = stablehlo.subtract %6682, %6686 : tensor<16x4x64xbf16> loc(#loc1738)
    %6688 = stablehlo.broadcast_in_dim %6677, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %6689 = stablehlo.multiply %6680, %6688 : tensor<16x4x64xbf16> loc(#loc1739)
    %6690 = stablehlo.broadcast_in_dim %6678, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %6691 = stablehlo.multiply %6679, %6690 : tensor<16x4x64xbf16> loc(#loc1740)
    %6692 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %6693 = stablehlo.multiply %6691, %6692 : tensor<16x4x64xbf16> loc(#loc1741)
    %6694 = stablehlo.add %6689, %6693 : tensor<16x4x64xbf16> loc(#loc1742)
    %6695 = stablehlo.concatenate %6687, %6694, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %6696 = stablehlo.slice %6676 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %6697 = stablehlo.concatenate %6695, %6696, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %6698 = stablehlo.reshape %6697 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %6699:2 = call @_jax_attn_func(%arg478, %6675, %6698, %6607, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %6700 = stablehlo.dot_general %6699#1, %arg350, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %6701 = stablehlo.reshape %6700 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %6702 = stablehlo.reshape %6701 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %6703 = stablehlo.convert %6702 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %6704 = stablehlo.convert %6580 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %6705 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %6706 = stablehlo.multiply %6704, %6705 : tensor<16x2048xf32> loc(#loc1751)
    %6707 = stablehlo.add %6703, %6706 : tensor<16x2048xf32> loc(#loc1752)
    %6708 = stablehlo.convert %6707 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %6709 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %6710 = stablehlo.power %6707, %6709 : tensor<16x2048xf32> loc(#loc1754)
    %6711 = stablehlo.reduce(%6710 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %6712 = stablehlo.broadcast_in_dim %6711, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %6713 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %6714 = stablehlo.divide %6712, %6713 : tensor<16x1xf32> loc(#loc1757)
    %6715 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %6716 = stablehlo.add %6714, %6715 : tensor<16x1xf32> loc(#loc1758)
    %6717 = stablehlo.rsqrt %6716 : tensor<16x1xf32> loc(#loc1678)
    %6718 = stablehlo.broadcast_in_dim %6717, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %6719 = stablehlo.multiply %6707, %6718 : tensor<16x2048xf32> loc(#loc1759)
    %6720 = stablehlo.convert %6719 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %6721 = stablehlo.broadcast_in_dim %arg348, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %6722 = stablehlo.broadcast_in_dim %6721, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %6723 = stablehlo.multiply %6720, %6722 : tensor<16x2048xbf16> loc(#loc1761)
    %6724 = stablehlo.dot_general %6723, %arg347, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %6725 = stablehlo.reshape %6724 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %6726 = stablehlo.reshape %6725 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %6727 = call @jax_fused_moe_func_padded(%6723, %arg345, %arg346, %6726) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %6728 = stablehlo.convert %6727 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %6729 = stablehlo.convert %6708 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %6730 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %6731 = stablehlo.multiply %6729, %6730 : tensor<16x2048xf32> loc(#loc1766)
    %6732 = stablehlo.add %6728, %6731 : tensor<16x2048xf32> loc(#loc1767)
    %6733 = stablehlo.convert %6732 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %6734 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %6735 = stablehlo.power %6732, %6734 : tensor<16x2048xf32> loc(#loc1768)
    %6736 = stablehlo.reduce(%6735 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %6737 = stablehlo.broadcast_in_dim %6736, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %6738 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %6739 = stablehlo.divide %6737, %6738 : tensor<16x1xf32> loc(#loc1771)
    %6740 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %6741 = stablehlo.add %6739, %6740 : tensor<16x1xf32> loc(#loc1772)
    %6742 = stablehlo.rsqrt %6741 : tensor<16x1xf32> loc(#loc1678)
    %6743 = stablehlo.broadcast_in_dim %6742, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %6744 = stablehlo.multiply %6732, %6743 : tensor<16x2048xf32> loc(#loc1773)
    %6745 = stablehlo.convert %6744 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %6746 = stablehlo.broadcast_in_dim %arg353, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %6747 = stablehlo.broadcast_in_dim %6746, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %6748 = stablehlo.multiply %6745, %6747 : tensor<16x2048xbf16> loc(#loc1775)
    %6749 = stablehlo.dot_general %6748, %arg361, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %6750 = stablehlo.reshape %6749 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %6751 = stablehlo.slice %6750 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %6752 = stablehlo.reshape %6751 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %6753 = stablehlo.slice %6750 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %6754 = stablehlo.reshape %6753 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %6755 = stablehlo.slice %6750 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %6756 = stablehlo.reshape %6755 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %6757 = stablehlo.concatenate %6752, %6754, %6756, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %6758 = stablehlo.slice %6757 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %6759 = stablehlo.slice %6757 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %6760 = stablehlo.slice %6757 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %6761 = stablehlo.reshape %6758 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %6762 = stablehlo.convert %6761 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %6763 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %6764 = stablehlo.power %6762, %6763 : tensor<16x32x128xf32> loc(#loc1690)
    %6765 = stablehlo.reduce(%6764 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %6766 = stablehlo.broadcast_in_dim %6765, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %6767 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %6768 = stablehlo.divide %6766, %6767 : tensor<16x32x1xf32> loc(#loc1693)
    %6769 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %6770 = stablehlo.add %6768, %6769 : tensor<16x32x1xf32> loc(#loc1694)
    %6771 = stablehlo.rsqrt %6770 : tensor<16x32x1xf32> loc(#loc1678)
    %6772 = stablehlo.broadcast_in_dim %6771, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %6773 = stablehlo.multiply %6762, %6772 : tensor<16x32x128xf32> loc(#loc1695)
    %6774 = stablehlo.convert %6773 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %6775 = stablehlo.broadcast_in_dim %arg360, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %6776 = stablehlo.broadcast_in_dim %6775, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %6777 = stablehlo.multiply %6774, %6776 : tensor<16x32x128xbf16> loc(#loc1697)
    %6778 = stablehlo.reshape %6777 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %6779 = stablehlo.reshape %6759 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %6780 = stablehlo.convert %6779 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %6781 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %6782 = stablehlo.power %6780, %6781 : tensor<16x4x128xf32> loc(#loc1700)
    %6783 = stablehlo.reduce(%6782 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %6784 = stablehlo.broadcast_in_dim %6783, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %6785 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %6786 = stablehlo.divide %6784, %6785 : tensor<16x4x1xf32> loc(#loc1703)
    %6787 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %6788 = stablehlo.add %6786, %6787 : tensor<16x4x1xf32> loc(#loc1704)
    %6789 = stablehlo.rsqrt %6788 : tensor<16x4x1xf32> loc(#loc1678)
    %6790 = stablehlo.broadcast_in_dim %6789, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %6791 = stablehlo.multiply %6780, %6790 : tensor<16x4x128xf32> loc(#loc1705)
    %6792 = stablehlo.convert %6791 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %6793 = stablehlo.broadcast_in_dim %arg358, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %6794 = stablehlo.broadcast_in_dim %6793, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %6795 = stablehlo.multiply %6792, %6794 : tensor<16x4x128xbf16> loc(#loc1707)
    %6796 = stablehlo.reshape %6795 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %6797 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %6798 = stablehlo.compare  LT, %arg484, %6797,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %6799 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %6800 = stablehlo.add %arg484, %6799 : tensor<16xi32> loc(#loc1710)
    %6801 = stablehlo.select %6798, %6800, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %6802 = stablehlo.broadcast_in_dim %6801, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %6803 = "stablehlo.gather"(%arg10, %6802) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %6804 = stablehlo.slice %6803 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %6805 = stablehlo.slice %6803 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %6806 = stablehlo.reshape %6778 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %6807 = stablehlo.broadcast_in_dim %6804, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %6808 = stablehlo.broadcast_in_dim %6805, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %6809 = stablehlo.slice %6806 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %6810 = stablehlo.slice %6806 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %6811 = stablehlo.broadcast_in_dim %6807, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %6812 = stablehlo.multiply %6809, %6811 : tensor<16x32x64xbf16> loc(#loc1719)
    %6813 = stablehlo.broadcast_in_dim %6808, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %6814 = stablehlo.multiply %6810, %6813 : tensor<16x32x64xbf16> loc(#loc1720)
    %6815 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %6816 = stablehlo.multiply %6814, %6815 : tensor<16x32x64xbf16> loc(#loc1721)
    %6817 = stablehlo.subtract %6812, %6816 : tensor<16x32x64xbf16> loc(#loc1722)
    %6818 = stablehlo.broadcast_in_dim %6807, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %6819 = stablehlo.multiply %6810, %6818 : tensor<16x32x64xbf16> loc(#loc1723)
    %6820 = stablehlo.broadcast_in_dim %6808, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %6821 = stablehlo.multiply %6809, %6820 : tensor<16x32x64xbf16> loc(#loc1724)
    %6822 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %6823 = stablehlo.multiply %6821, %6822 : tensor<16x32x64xbf16> loc(#loc1725)
    %6824 = stablehlo.add %6819, %6823 : tensor<16x32x64xbf16> loc(#loc1726)
    %6825 = stablehlo.concatenate %6817, %6824, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %6826 = stablehlo.slice %6806 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %6827 = stablehlo.concatenate %6825, %6826, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %6828 = stablehlo.reshape %6827 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %6829 = stablehlo.reshape %6796 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %6830 = stablehlo.broadcast_in_dim %6804, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %6831 = stablehlo.broadcast_in_dim %6805, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %6832 = stablehlo.slice %6829 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %6833 = stablehlo.slice %6829 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %6834 = stablehlo.broadcast_in_dim %6830, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %6835 = stablehlo.multiply %6832, %6834 : tensor<16x4x64xbf16> loc(#loc1735)
    %6836 = stablehlo.broadcast_in_dim %6831, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %6837 = stablehlo.multiply %6833, %6836 : tensor<16x4x64xbf16> loc(#loc1736)
    %6838 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %6839 = stablehlo.multiply %6837, %6838 : tensor<16x4x64xbf16> loc(#loc1737)
    %6840 = stablehlo.subtract %6835, %6839 : tensor<16x4x64xbf16> loc(#loc1738)
    %6841 = stablehlo.broadcast_in_dim %6830, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %6842 = stablehlo.multiply %6833, %6841 : tensor<16x4x64xbf16> loc(#loc1739)
    %6843 = stablehlo.broadcast_in_dim %6831, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %6844 = stablehlo.multiply %6832, %6843 : tensor<16x4x64xbf16> loc(#loc1740)
    %6845 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %6846 = stablehlo.multiply %6844, %6845 : tensor<16x4x64xbf16> loc(#loc1741)
    %6847 = stablehlo.add %6842, %6846 : tensor<16x4x64xbf16> loc(#loc1742)
    %6848 = stablehlo.concatenate %6840, %6847, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %6849 = stablehlo.slice %6829 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %6850 = stablehlo.concatenate %6848, %6849, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %6851 = stablehlo.reshape %6850 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %6852:2 = call @_jax_attn_func(%arg479, %6828, %6851, %6760, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %6853 = stablehlo.dot_general %6852#1, %arg359, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %6854 = stablehlo.reshape %6853 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %6855 = stablehlo.reshape %6854 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %6856 = stablehlo.convert %6855 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %6857 = stablehlo.convert %6733 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %6858 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %6859 = stablehlo.multiply %6857, %6858 : tensor<16x2048xf32> loc(#loc1751)
    %6860 = stablehlo.add %6856, %6859 : tensor<16x2048xf32> loc(#loc1752)
    %6861 = stablehlo.convert %6860 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %6862 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %6863 = stablehlo.power %6860, %6862 : tensor<16x2048xf32> loc(#loc1754)
    %6864 = stablehlo.reduce(%6863 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %6865 = stablehlo.broadcast_in_dim %6864, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %6866 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %6867 = stablehlo.divide %6865, %6866 : tensor<16x1xf32> loc(#loc1757)
    %6868 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %6869 = stablehlo.add %6867, %6868 : tensor<16x1xf32> loc(#loc1758)
    %6870 = stablehlo.rsqrt %6869 : tensor<16x1xf32> loc(#loc1678)
    %6871 = stablehlo.broadcast_in_dim %6870, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %6872 = stablehlo.multiply %6860, %6871 : tensor<16x2048xf32> loc(#loc1759)
    %6873 = stablehlo.convert %6872 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %6874 = stablehlo.broadcast_in_dim %arg357, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %6875 = stablehlo.broadcast_in_dim %6874, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %6876 = stablehlo.multiply %6873, %6875 : tensor<16x2048xbf16> loc(#loc1761)
    %6877 = stablehlo.dot_general %6876, %arg356, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %6878 = stablehlo.reshape %6877 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %6879 = stablehlo.reshape %6878 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %6880 = call @jax_fused_moe_func_padded(%6876, %arg354, %arg355, %6879) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %6881 = stablehlo.convert %6880 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %6882 = stablehlo.convert %6861 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %6883 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %6884 = stablehlo.multiply %6882, %6883 : tensor<16x2048xf32> loc(#loc1766)
    %6885 = stablehlo.add %6881, %6884 : tensor<16x2048xf32> loc(#loc1767)
    %6886 = stablehlo.convert %6885 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %6887 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %6888 = stablehlo.power %6885, %6887 : tensor<16x2048xf32> loc(#loc1768)
    %6889 = stablehlo.reduce(%6888 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %6890 = stablehlo.broadcast_in_dim %6889, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %6891 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %6892 = stablehlo.divide %6890, %6891 : tensor<16x1xf32> loc(#loc1771)
    %6893 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %6894 = stablehlo.add %6892, %6893 : tensor<16x1xf32> loc(#loc1772)
    %6895 = stablehlo.rsqrt %6894 : tensor<16x1xf32> loc(#loc1678)
    %6896 = stablehlo.broadcast_in_dim %6895, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %6897 = stablehlo.multiply %6885, %6896 : tensor<16x2048xf32> loc(#loc1773)
    %6898 = stablehlo.convert %6897 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %6899 = stablehlo.broadcast_in_dim %arg362, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %6900 = stablehlo.broadcast_in_dim %6899, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %6901 = stablehlo.multiply %6898, %6900 : tensor<16x2048xbf16> loc(#loc1775)
    %6902 = stablehlo.dot_general %6901, %arg370, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %6903 = stablehlo.reshape %6902 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %6904 = stablehlo.slice %6903 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %6905 = stablehlo.reshape %6904 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %6906 = stablehlo.slice %6903 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %6907 = stablehlo.reshape %6906 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %6908 = stablehlo.slice %6903 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %6909 = stablehlo.reshape %6908 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %6910 = stablehlo.concatenate %6905, %6907, %6909, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %6911 = stablehlo.slice %6910 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %6912 = stablehlo.slice %6910 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %6913 = stablehlo.slice %6910 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %6914 = stablehlo.reshape %6911 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %6915 = stablehlo.convert %6914 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %6916 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %6917 = stablehlo.power %6915, %6916 : tensor<16x32x128xf32> loc(#loc1690)
    %6918 = stablehlo.reduce(%6917 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %6919 = stablehlo.broadcast_in_dim %6918, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %6920 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %6921 = stablehlo.divide %6919, %6920 : tensor<16x32x1xf32> loc(#loc1693)
    %6922 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %6923 = stablehlo.add %6921, %6922 : tensor<16x32x1xf32> loc(#loc1694)
    %6924 = stablehlo.rsqrt %6923 : tensor<16x32x1xf32> loc(#loc1678)
    %6925 = stablehlo.broadcast_in_dim %6924, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %6926 = stablehlo.multiply %6915, %6925 : tensor<16x32x128xf32> loc(#loc1695)
    %6927 = stablehlo.convert %6926 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %6928 = stablehlo.broadcast_in_dim %arg369, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %6929 = stablehlo.broadcast_in_dim %6928, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %6930 = stablehlo.multiply %6927, %6929 : tensor<16x32x128xbf16> loc(#loc1697)
    %6931 = stablehlo.reshape %6930 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %6932 = stablehlo.reshape %6912 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %6933 = stablehlo.convert %6932 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %6934 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %6935 = stablehlo.power %6933, %6934 : tensor<16x4x128xf32> loc(#loc1700)
    %6936 = stablehlo.reduce(%6935 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %6937 = stablehlo.broadcast_in_dim %6936, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %6938 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %6939 = stablehlo.divide %6937, %6938 : tensor<16x4x1xf32> loc(#loc1703)
    %6940 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %6941 = stablehlo.add %6939, %6940 : tensor<16x4x1xf32> loc(#loc1704)
    %6942 = stablehlo.rsqrt %6941 : tensor<16x4x1xf32> loc(#loc1678)
    %6943 = stablehlo.broadcast_in_dim %6942, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %6944 = stablehlo.multiply %6933, %6943 : tensor<16x4x128xf32> loc(#loc1705)
    %6945 = stablehlo.convert %6944 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %6946 = stablehlo.broadcast_in_dim %arg367, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %6947 = stablehlo.broadcast_in_dim %6946, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %6948 = stablehlo.multiply %6945, %6947 : tensor<16x4x128xbf16> loc(#loc1707)
    %6949 = stablehlo.reshape %6948 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %6950 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %6951 = stablehlo.compare  LT, %arg484, %6950,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %6952 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %6953 = stablehlo.add %arg484, %6952 : tensor<16xi32> loc(#loc1710)
    %6954 = stablehlo.select %6951, %6953, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %6955 = stablehlo.broadcast_in_dim %6954, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %6956 = "stablehlo.gather"(%arg10, %6955) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %6957 = stablehlo.slice %6956 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %6958 = stablehlo.slice %6956 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %6959 = stablehlo.reshape %6931 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %6960 = stablehlo.broadcast_in_dim %6957, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %6961 = stablehlo.broadcast_in_dim %6958, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %6962 = stablehlo.slice %6959 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %6963 = stablehlo.slice %6959 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %6964 = stablehlo.broadcast_in_dim %6960, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %6965 = stablehlo.multiply %6962, %6964 : tensor<16x32x64xbf16> loc(#loc1719)
    %6966 = stablehlo.broadcast_in_dim %6961, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %6967 = stablehlo.multiply %6963, %6966 : tensor<16x32x64xbf16> loc(#loc1720)
    %6968 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %6969 = stablehlo.multiply %6967, %6968 : tensor<16x32x64xbf16> loc(#loc1721)
    %6970 = stablehlo.subtract %6965, %6969 : tensor<16x32x64xbf16> loc(#loc1722)
    %6971 = stablehlo.broadcast_in_dim %6960, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %6972 = stablehlo.multiply %6963, %6971 : tensor<16x32x64xbf16> loc(#loc1723)
    %6973 = stablehlo.broadcast_in_dim %6961, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %6974 = stablehlo.multiply %6962, %6973 : tensor<16x32x64xbf16> loc(#loc1724)
    %6975 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %6976 = stablehlo.multiply %6974, %6975 : tensor<16x32x64xbf16> loc(#loc1725)
    %6977 = stablehlo.add %6972, %6976 : tensor<16x32x64xbf16> loc(#loc1726)
    %6978 = stablehlo.concatenate %6970, %6977, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %6979 = stablehlo.slice %6959 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %6980 = stablehlo.concatenate %6978, %6979, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %6981 = stablehlo.reshape %6980 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %6982 = stablehlo.reshape %6949 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %6983 = stablehlo.broadcast_in_dim %6957, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %6984 = stablehlo.broadcast_in_dim %6958, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %6985 = stablehlo.slice %6982 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %6986 = stablehlo.slice %6982 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %6987 = stablehlo.broadcast_in_dim %6983, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %6988 = stablehlo.multiply %6985, %6987 : tensor<16x4x64xbf16> loc(#loc1735)
    %6989 = stablehlo.broadcast_in_dim %6984, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %6990 = stablehlo.multiply %6986, %6989 : tensor<16x4x64xbf16> loc(#loc1736)
    %6991 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %6992 = stablehlo.multiply %6990, %6991 : tensor<16x4x64xbf16> loc(#loc1737)
    %6993 = stablehlo.subtract %6988, %6992 : tensor<16x4x64xbf16> loc(#loc1738)
    %6994 = stablehlo.broadcast_in_dim %6983, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %6995 = stablehlo.multiply %6986, %6994 : tensor<16x4x64xbf16> loc(#loc1739)
    %6996 = stablehlo.broadcast_in_dim %6984, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %6997 = stablehlo.multiply %6985, %6996 : tensor<16x4x64xbf16> loc(#loc1740)
    %6998 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %6999 = stablehlo.multiply %6997, %6998 : tensor<16x4x64xbf16> loc(#loc1741)
    %7000 = stablehlo.add %6995, %6999 : tensor<16x4x64xbf16> loc(#loc1742)
    %7001 = stablehlo.concatenate %6993, %7000, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %7002 = stablehlo.slice %6982 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %7003 = stablehlo.concatenate %7001, %7002, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %7004 = stablehlo.reshape %7003 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %7005:2 = call @_jax_attn_func(%arg480, %6981, %7004, %6913, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %7006 = stablehlo.dot_general %7005#1, %arg368, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %7007 = stablehlo.reshape %7006 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %7008 = stablehlo.reshape %7007 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %7009 = stablehlo.convert %7008 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %7010 = stablehlo.convert %6886 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %7011 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %7012 = stablehlo.multiply %7010, %7011 : tensor<16x2048xf32> loc(#loc1751)
    %7013 = stablehlo.add %7009, %7012 : tensor<16x2048xf32> loc(#loc1752)
    %7014 = stablehlo.convert %7013 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %7015 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %7016 = stablehlo.power %7013, %7015 : tensor<16x2048xf32> loc(#loc1754)
    %7017 = stablehlo.reduce(%7016 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %7018 = stablehlo.broadcast_in_dim %7017, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %7019 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %7020 = stablehlo.divide %7018, %7019 : tensor<16x1xf32> loc(#loc1757)
    %7021 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %7022 = stablehlo.add %7020, %7021 : tensor<16x1xf32> loc(#loc1758)
    %7023 = stablehlo.rsqrt %7022 : tensor<16x1xf32> loc(#loc1678)
    %7024 = stablehlo.broadcast_in_dim %7023, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %7025 = stablehlo.multiply %7013, %7024 : tensor<16x2048xf32> loc(#loc1759)
    %7026 = stablehlo.convert %7025 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %7027 = stablehlo.broadcast_in_dim %arg366, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %7028 = stablehlo.broadcast_in_dim %7027, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %7029 = stablehlo.multiply %7026, %7028 : tensor<16x2048xbf16> loc(#loc1761)
    %7030 = stablehlo.dot_general %7029, %arg365, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %7031 = stablehlo.reshape %7030 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %7032 = stablehlo.reshape %7031 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %7033 = call @jax_fused_moe_func_padded(%7029, %arg363, %arg364, %7032) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %7034 = stablehlo.convert %7033 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %7035 = stablehlo.convert %7014 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %7036 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %7037 = stablehlo.multiply %7035, %7036 : tensor<16x2048xf32> loc(#loc1766)
    %7038 = stablehlo.add %7034, %7037 : tensor<16x2048xf32> loc(#loc1767)
    %7039 = stablehlo.convert %7038 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %7040 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %7041 = stablehlo.power %7038, %7040 : tensor<16x2048xf32> loc(#loc1768)
    %7042 = stablehlo.reduce(%7041 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %7043 = stablehlo.broadcast_in_dim %7042, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %7044 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %7045 = stablehlo.divide %7043, %7044 : tensor<16x1xf32> loc(#loc1771)
    %7046 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %7047 = stablehlo.add %7045, %7046 : tensor<16x1xf32> loc(#loc1772)
    %7048 = stablehlo.rsqrt %7047 : tensor<16x1xf32> loc(#loc1678)
    %7049 = stablehlo.broadcast_in_dim %7048, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %7050 = stablehlo.multiply %7038, %7049 : tensor<16x2048xf32> loc(#loc1773)
    %7051 = stablehlo.convert %7050 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %7052 = stablehlo.broadcast_in_dim %arg371, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %7053 = stablehlo.broadcast_in_dim %7052, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %7054 = stablehlo.multiply %7051, %7053 : tensor<16x2048xbf16> loc(#loc1775)
    %7055 = stablehlo.dot_general %7054, %arg379, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %7056 = stablehlo.reshape %7055 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %7057 = stablehlo.slice %7056 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %7058 = stablehlo.reshape %7057 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %7059 = stablehlo.slice %7056 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %7060 = stablehlo.reshape %7059 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %7061 = stablehlo.slice %7056 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %7062 = stablehlo.reshape %7061 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %7063 = stablehlo.concatenate %7058, %7060, %7062, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %7064 = stablehlo.slice %7063 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %7065 = stablehlo.slice %7063 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %7066 = stablehlo.slice %7063 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %7067 = stablehlo.reshape %7064 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %7068 = stablehlo.convert %7067 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %7069 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %7070 = stablehlo.power %7068, %7069 : tensor<16x32x128xf32> loc(#loc1690)
    %7071 = stablehlo.reduce(%7070 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %7072 = stablehlo.broadcast_in_dim %7071, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %7073 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %7074 = stablehlo.divide %7072, %7073 : tensor<16x32x1xf32> loc(#loc1693)
    %7075 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %7076 = stablehlo.add %7074, %7075 : tensor<16x32x1xf32> loc(#loc1694)
    %7077 = stablehlo.rsqrt %7076 : tensor<16x32x1xf32> loc(#loc1678)
    %7078 = stablehlo.broadcast_in_dim %7077, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %7079 = stablehlo.multiply %7068, %7078 : tensor<16x32x128xf32> loc(#loc1695)
    %7080 = stablehlo.convert %7079 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %7081 = stablehlo.broadcast_in_dim %arg378, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %7082 = stablehlo.broadcast_in_dim %7081, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %7083 = stablehlo.multiply %7080, %7082 : tensor<16x32x128xbf16> loc(#loc1697)
    %7084 = stablehlo.reshape %7083 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %7085 = stablehlo.reshape %7065 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %7086 = stablehlo.convert %7085 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %7087 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %7088 = stablehlo.power %7086, %7087 : tensor<16x4x128xf32> loc(#loc1700)
    %7089 = stablehlo.reduce(%7088 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %7090 = stablehlo.broadcast_in_dim %7089, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %7091 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %7092 = stablehlo.divide %7090, %7091 : tensor<16x4x1xf32> loc(#loc1703)
    %7093 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %7094 = stablehlo.add %7092, %7093 : tensor<16x4x1xf32> loc(#loc1704)
    %7095 = stablehlo.rsqrt %7094 : tensor<16x4x1xf32> loc(#loc1678)
    %7096 = stablehlo.broadcast_in_dim %7095, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %7097 = stablehlo.multiply %7086, %7096 : tensor<16x4x128xf32> loc(#loc1705)
    %7098 = stablehlo.convert %7097 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %7099 = stablehlo.broadcast_in_dim %arg376, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %7100 = stablehlo.broadcast_in_dim %7099, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %7101 = stablehlo.multiply %7098, %7100 : tensor<16x4x128xbf16> loc(#loc1707)
    %7102 = stablehlo.reshape %7101 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %7103 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %7104 = stablehlo.compare  LT, %arg484, %7103,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %7105 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %7106 = stablehlo.add %arg484, %7105 : tensor<16xi32> loc(#loc1710)
    %7107 = stablehlo.select %7104, %7106, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %7108 = stablehlo.broadcast_in_dim %7107, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %7109 = "stablehlo.gather"(%arg10, %7108) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %7110 = stablehlo.slice %7109 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %7111 = stablehlo.slice %7109 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %7112 = stablehlo.reshape %7084 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %7113 = stablehlo.broadcast_in_dim %7110, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %7114 = stablehlo.broadcast_in_dim %7111, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %7115 = stablehlo.slice %7112 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %7116 = stablehlo.slice %7112 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %7117 = stablehlo.broadcast_in_dim %7113, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %7118 = stablehlo.multiply %7115, %7117 : tensor<16x32x64xbf16> loc(#loc1719)
    %7119 = stablehlo.broadcast_in_dim %7114, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %7120 = stablehlo.multiply %7116, %7119 : tensor<16x32x64xbf16> loc(#loc1720)
    %7121 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %7122 = stablehlo.multiply %7120, %7121 : tensor<16x32x64xbf16> loc(#loc1721)
    %7123 = stablehlo.subtract %7118, %7122 : tensor<16x32x64xbf16> loc(#loc1722)
    %7124 = stablehlo.broadcast_in_dim %7113, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %7125 = stablehlo.multiply %7116, %7124 : tensor<16x32x64xbf16> loc(#loc1723)
    %7126 = stablehlo.broadcast_in_dim %7114, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %7127 = stablehlo.multiply %7115, %7126 : tensor<16x32x64xbf16> loc(#loc1724)
    %7128 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %7129 = stablehlo.multiply %7127, %7128 : tensor<16x32x64xbf16> loc(#loc1725)
    %7130 = stablehlo.add %7125, %7129 : tensor<16x32x64xbf16> loc(#loc1726)
    %7131 = stablehlo.concatenate %7123, %7130, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %7132 = stablehlo.slice %7112 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %7133 = stablehlo.concatenate %7131, %7132, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %7134 = stablehlo.reshape %7133 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %7135 = stablehlo.reshape %7102 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %7136 = stablehlo.broadcast_in_dim %7110, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %7137 = stablehlo.broadcast_in_dim %7111, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %7138 = stablehlo.slice %7135 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %7139 = stablehlo.slice %7135 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %7140 = stablehlo.broadcast_in_dim %7136, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %7141 = stablehlo.multiply %7138, %7140 : tensor<16x4x64xbf16> loc(#loc1735)
    %7142 = stablehlo.broadcast_in_dim %7137, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %7143 = stablehlo.multiply %7139, %7142 : tensor<16x4x64xbf16> loc(#loc1736)
    %7144 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %7145 = stablehlo.multiply %7143, %7144 : tensor<16x4x64xbf16> loc(#loc1737)
    %7146 = stablehlo.subtract %7141, %7145 : tensor<16x4x64xbf16> loc(#loc1738)
    %7147 = stablehlo.broadcast_in_dim %7136, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %7148 = stablehlo.multiply %7139, %7147 : tensor<16x4x64xbf16> loc(#loc1739)
    %7149 = stablehlo.broadcast_in_dim %7137, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %7150 = stablehlo.multiply %7138, %7149 : tensor<16x4x64xbf16> loc(#loc1740)
    %7151 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %7152 = stablehlo.multiply %7150, %7151 : tensor<16x4x64xbf16> loc(#loc1741)
    %7153 = stablehlo.add %7148, %7152 : tensor<16x4x64xbf16> loc(#loc1742)
    %7154 = stablehlo.concatenate %7146, %7153, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %7155 = stablehlo.slice %7135 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %7156 = stablehlo.concatenate %7154, %7155, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %7157 = stablehlo.reshape %7156 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %7158:2 = call @_jax_attn_func(%arg481, %7134, %7157, %7066, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %7159 = stablehlo.dot_general %7158#1, %arg377, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %7160 = stablehlo.reshape %7159 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %7161 = stablehlo.reshape %7160 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %7162 = stablehlo.convert %7161 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %7163 = stablehlo.convert %7039 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %7164 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %7165 = stablehlo.multiply %7163, %7164 : tensor<16x2048xf32> loc(#loc1751)
    %7166 = stablehlo.add %7162, %7165 : tensor<16x2048xf32> loc(#loc1752)
    %7167 = stablehlo.convert %7166 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %7168 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %7169 = stablehlo.power %7166, %7168 : tensor<16x2048xf32> loc(#loc1754)
    %7170 = stablehlo.reduce(%7169 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %7171 = stablehlo.broadcast_in_dim %7170, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %7172 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %7173 = stablehlo.divide %7171, %7172 : tensor<16x1xf32> loc(#loc1757)
    %7174 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %7175 = stablehlo.add %7173, %7174 : tensor<16x1xf32> loc(#loc1758)
    %7176 = stablehlo.rsqrt %7175 : tensor<16x1xf32> loc(#loc1678)
    %7177 = stablehlo.broadcast_in_dim %7176, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %7178 = stablehlo.multiply %7166, %7177 : tensor<16x2048xf32> loc(#loc1759)
    %7179 = stablehlo.convert %7178 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %7180 = stablehlo.broadcast_in_dim %arg375, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %7181 = stablehlo.broadcast_in_dim %7180, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %7182 = stablehlo.multiply %7179, %7181 : tensor<16x2048xbf16> loc(#loc1761)
    %7183 = stablehlo.dot_general %7182, %arg374, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %7184 = stablehlo.reshape %7183 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %7185 = stablehlo.reshape %7184 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %7186 = call @jax_fused_moe_func_padded(%7182, %arg372, %arg373, %7185) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %7187 = stablehlo.convert %7186 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %7188 = stablehlo.convert %7167 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %7189 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1766)
    %7190 = stablehlo.multiply %7188, %7189 : tensor<16x2048xf32> loc(#loc1766)
    %7191 = stablehlo.add %7187, %7190 : tensor<16x2048xf32> loc(#loc1767)
    %7192 = stablehlo.convert %7191 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %7193 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1768)
    %7194 = stablehlo.power %7191, %7193 : tensor<16x2048xf32> loc(#loc1768)
    %7195 = stablehlo.reduce(%7194 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1769)
    %7196 = stablehlo.broadcast_in_dim %7195, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1770)
    %7197 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1771)
    %7198 = stablehlo.divide %7196, %7197 : tensor<16x1xf32> loc(#loc1771)
    %7199 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1772)
    %7200 = stablehlo.add %7198, %7199 : tensor<16x1xf32> loc(#loc1772)
    %7201 = stablehlo.rsqrt %7200 : tensor<16x1xf32> loc(#loc1678)
    %7202 = stablehlo.broadcast_in_dim %7201, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1773)
    %7203 = stablehlo.multiply %7191, %7202 : tensor<16x2048xf32> loc(#loc1773)
    %7204 = stablehlo.convert %7203 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %7205 = stablehlo.broadcast_in_dim %arg380, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1774)
    %7206 = stablehlo.broadcast_in_dim %7205, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1775)
    %7207 = stablehlo.multiply %7204, %7206 : tensor<16x2048xbf16> loc(#loc1775)
    %7208 = stablehlo.dot_general %7207, %arg388, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<5120x2048xbf16>) -> tensor<16x5120xbf16> loc(#loc1683)
    %7209 = stablehlo.reshape %7208 : (tensor<16x5120xbf16>) -> tensor<16x4x1280xbf16> loc(#loc1684)
    %7210 = stablehlo.slice %7209 [0:16, 0:4, 0:1024] : (tensor<16x4x1280xbf16>) -> tensor<16x4x1024xbf16> loc(#loc1685)
    %7211 = stablehlo.reshape %7210 : (tensor<16x4x1024xbf16>) -> tensor<16x4096xbf16> loc(#loc1686)
    %7212 = stablehlo.slice %7209 [0:16, 0:4, 1024:1152] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %7213 = stablehlo.reshape %7212 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %7214 = stablehlo.slice %7209 [0:16, 0:4, 1152:1280] : (tensor<16x4x1280xbf16>) -> tensor<16x4x128xbf16> loc(#loc1685)
    %7215 = stablehlo.reshape %7214 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1686)
    %7216 = stablehlo.concatenate %7211, %7213, %7215, dim = 1 : (tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>) -> tensor<16x5120xbf16> loc(#loc1687)
    %7217 = stablehlo.slice %7216 [0:16, 0:4096] : (tensor<16x5120xbf16>) -> tensor<16x4096xbf16> loc(#loc1688)
    %7218 = stablehlo.slice %7216 [0:16, 4096:4608] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %7219 = stablehlo.slice %7216 [0:16, 4608:5120] : (tensor<16x5120xbf16>) -> tensor<16x512xbf16> loc(#loc1688)
    %7220 = stablehlo.reshape %7217 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1689)
    %7221 = stablehlo.convert %7220 : (tensor<16x32x128xbf16>) -> tensor<16x32x128xf32> loc(#loc1672)
    %7222 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x32x128xf32> loc(#loc1690)
    %7223 = stablehlo.power %7221, %7222 : tensor<16x32x128xf32> loc(#loc1690)
    %7224 = stablehlo.reduce(%7223 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x32x128xf32>, tensor<f32>) -> tensor<16x32xf32> loc(#loc1691)
    %7225 = stablehlo.broadcast_in_dim %7224, dims = [0, 1] : (tensor<16x32xf32>) -> tensor<16x32x1xf32> loc(#loc1692)
    %7226 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1693)
    %7227 = stablehlo.divide %7225, %7226 : tensor<16x32x1xf32> loc(#loc1693)
    %7228 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x32x1xf32> loc(#loc1694)
    %7229 = stablehlo.add %7227, %7228 : tensor<16x32x1xf32> loc(#loc1694)
    %7230 = stablehlo.rsqrt %7229 : tensor<16x32x1xf32> loc(#loc1678)
    %7231 = stablehlo.broadcast_in_dim %7230, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x128xf32> loc(#loc1695)
    %7232 = stablehlo.multiply %7221, %7231 : tensor<16x32x128xf32> loc(#loc1695)
    %7233 = stablehlo.convert %7232 : (tensor<16x32x128xf32>) -> tensor<16x32x128xbf16> loc(#loc1680)
    %7234 = stablehlo.broadcast_in_dim %arg387, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1696)
    %7235 = stablehlo.broadcast_in_dim %7234, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x32x128xbf16> loc(#loc1697)
    %7236 = stablehlo.multiply %7233, %7235 : tensor<16x32x128xbf16> loc(#loc1697)
    %7237 = stablehlo.reshape %7236 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1698)
    %7238 = stablehlo.reshape %7218 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1699)
    %7239 = stablehlo.convert %7238 : (tensor<16x4x128xbf16>) -> tensor<16x4x128xf32> loc(#loc1672)
    %7240 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x4x128xf32> loc(#loc1700)
    %7241 = stablehlo.power %7239, %7240 : tensor<16x4x128xf32> loc(#loc1700)
    %7242 = stablehlo.reduce(%7241 init: %cst_5) applies stablehlo.add across dimensions = [2] : (tensor<16x4x128xf32>, tensor<f32>) -> tensor<16x4xf32> loc(#loc1701)
    %7243 = stablehlo.broadcast_in_dim %7242, dims = [0, 1] : (tensor<16x4xf32>) -> tensor<16x4x1xf32> loc(#loc1702)
    %7244 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1703)
    %7245 = stablehlo.divide %7243, %7244 : tensor<16x4x1xf32> loc(#loc1703)
    %7246 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x4x1xf32> loc(#loc1704)
    %7247 = stablehlo.add %7245, %7246 : tensor<16x4x1xf32> loc(#loc1704)
    %7248 = stablehlo.rsqrt %7247 : tensor<16x4x1xf32> loc(#loc1678)
    %7249 = stablehlo.broadcast_in_dim %7248, dims = [0, 1, 2] : (tensor<16x4x1xf32>) -> tensor<16x4x128xf32> loc(#loc1705)
    %7250 = stablehlo.multiply %7239, %7249 : tensor<16x4x128xf32> loc(#loc1705)
    %7251 = stablehlo.convert %7250 : (tensor<16x4x128xf32>) -> tensor<16x4x128xbf16> loc(#loc1680)
    %7252 = stablehlo.broadcast_in_dim %arg385, dims = [2] : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1706)
    %7253 = stablehlo.broadcast_in_dim %7252, dims = [0, 1, 2] : (tensor<1x1x128xbf16>) -> tensor<16x4x128xbf16> loc(#loc1707)
    %7254 = stablehlo.multiply %7251, %7253 : tensor<16x4x128xbf16> loc(#loc1707)
    %7255 = stablehlo.reshape %7254 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1708)
    %7256 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1709)
    %7257 = stablehlo.compare  LT, %arg484, %7256,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1709)
    %7258 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1710)
    %7259 = stablehlo.add %arg484, %7258 : tensor<16xi32> loc(#loc1710)
    %7260 = stablehlo.select %7257, %7259, %arg484 : tensor<16xi1>, tensor<16xi32> loc(#loc1711)
    %7261 = stablehlo.broadcast_in_dim %7260, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1712)
    %7262 = "stablehlo.gather"(%arg10, %7261) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128>}> : (tensor<262144x128xbf16>, tensor<16x1xi32>) -> tensor<16x128xbf16> loc(#loc1713)
    %7263 = stablehlo.slice %7262 [0:16, 0:64] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %7264 = stablehlo.slice %7262 [0:16, 64:128] : (tensor<16x128xbf16>) -> tensor<16x64xbf16> loc(#loc1714)
    %7265 = stablehlo.reshape %7237 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1715)
    %7266 = stablehlo.broadcast_in_dim %7263, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1716)
    %7267 = stablehlo.broadcast_in_dim %7264, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1717)
    %7268 = stablehlo.slice %7265 [0:16, 0:32, 0:64] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %7269 = stablehlo.slice %7265 [0:16, 0:32, 64:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x64xbf16> loc(#loc1718)
    %7270 = stablehlo.broadcast_in_dim %7266, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1719)
    %7271 = stablehlo.multiply %7268, %7270 : tensor<16x32x64xbf16> loc(#loc1719)
    %7272 = stablehlo.broadcast_in_dim %7267, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1720)
    %7273 = stablehlo.multiply %7269, %7272 : tensor<16x32x64xbf16> loc(#loc1720)
    %7274 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1721)
    %7275 = stablehlo.multiply %7273, %7274 : tensor<16x32x64xbf16> loc(#loc1721)
    %7276 = stablehlo.subtract %7271, %7275 : tensor<16x32x64xbf16> loc(#loc1722)
    %7277 = stablehlo.broadcast_in_dim %7266, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1723)
    %7278 = stablehlo.multiply %7269, %7277 : tensor<16x32x64xbf16> loc(#loc1723)
    %7279 = stablehlo.broadcast_in_dim %7267, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x32x64xbf16> loc(#loc1724)
    %7280 = stablehlo.multiply %7268, %7279 : tensor<16x32x64xbf16> loc(#loc1724)
    %7281 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x32x64xbf16> loc(#loc1725)
    %7282 = stablehlo.multiply %7280, %7281 : tensor<16x32x64xbf16> loc(#loc1725)
    %7283 = stablehlo.add %7278, %7282 : tensor<16x32x64xbf16> loc(#loc1726)
    %7284 = stablehlo.concatenate %7276, %7283, dim = 2 : (tensor<16x32x64xbf16>, tensor<16x32x64xbf16>) -> tensor<16x32x128xbf16> loc(#loc1727)
    %7285 = stablehlo.slice %7265 [0:16, 0:32, 128:128] : (tensor<16x32x128xbf16>) -> tensor<16x32x0xbf16> loc(#loc1728)
    %7286 = stablehlo.concatenate %7284, %7285, dim = 2 : (tensor<16x32x128xbf16>, tensor<16x32x0xbf16>) -> tensor<16x32x128xbf16> loc(#loc1729)
    %7287 = stablehlo.reshape %7286 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1730)
    %7288 = stablehlo.reshape %7255 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1731)
    %7289 = stablehlo.broadcast_in_dim %7263, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1732)
    %7290 = stablehlo.broadcast_in_dim %7264, dims = [0, 2] : (tensor<16x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc1733)
    %7291 = stablehlo.slice %7288 [0:16, 0:4, 0:64] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %7292 = stablehlo.slice %7288 [0:16, 0:4, 64:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x64xbf16> loc(#loc1734)
    %7293 = stablehlo.broadcast_in_dim %7289, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1735)
    %7294 = stablehlo.multiply %7291, %7293 : tensor<16x4x64xbf16> loc(#loc1735)
    %7295 = stablehlo.broadcast_in_dim %7290, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1736)
    %7296 = stablehlo.multiply %7292, %7295 : tensor<16x4x64xbf16> loc(#loc1736)
    %7297 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1737)
    %7298 = stablehlo.multiply %7296, %7297 : tensor<16x4x64xbf16> loc(#loc1737)
    %7299 = stablehlo.subtract %7294, %7298 : tensor<16x4x64xbf16> loc(#loc1738)
    %7300 = stablehlo.broadcast_in_dim %7289, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1739)
    %7301 = stablehlo.multiply %7292, %7300 : tensor<16x4x64xbf16> loc(#loc1739)
    %7302 = stablehlo.broadcast_in_dim %7290, dims = [0, 1, 2] : (tensor<16x1x64xbf16>) -> tensor<16x4x64xbf16> loc(#loc1740)
    %7303 = stablehlo.multiply %7291, %7302 : tensor<16x4x64xbf16> loc(#loc1740)
    %7304 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<16x4x64xbf16> loc(#loc1741)
    %7305 = stablehlo.multiply %7303, %7304 : tensor<16x4x64xbf16> loc(#loc1741)
    %7306 = stablehlo.add %7301, %7305 : tensor<16x4x64xbf16> loc(#loc1742)
    %7307 = stablehlo.concatenate %7299, %7306, dim = 2 : (tensor<16x4x64xbf16>, tensor<16x4x64xbf16>) -> tensor<16x4x128xbf16> loc(#loc1743)
    %7308 = stablehlo.slice %7288 [0:16, 0:4, 128:128] : (tensor<16x4x128xbf16>) -> tensor<16x4x0xbf16> loc(#loc1728)
    %7309 = stablehlo.concatenate %7307, %7308, dim = 2 : (tensor<16x4x128xbf16>, tensor<16x4x0xbf16>) -> tensor<16x4x128xbf16> loc(#loc1744)
    %7310 = stablehlo.reshape %7309 : (tensor<16x4x128xbf16>) -> tensor<16x512xbf16> loc(#loc1745)
    %7311:2 = call @_jax_attn_func(%arg482, %7287, %7310, %7219, %arg485, %arg486, %arg487, %arg488) : (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>, tensor<16x512xbf16>, tensor<16x512xbf16>, tensor<131072xi32>, tensor<256xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) loc(#loc1746)
    %7312 = stablehlo.dot_general %7311#1, %arg386, contracting_dims = [1] x [1] : (tensor<16x4096xbf16>, tensor<2048x4096xbf16>) -> tensor<16x2048xbf16> loc(#loc1747)
    %7313 = stablehlo.reshape %7312 : (tensor<16x2048xbf16>) -> tensor<16x1x2048xbf16> loc(#loc1748)
    %7314 = stablehlo.reshape %7313 : (tensor<16x1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1749)
    %7315 = stablehlo.convert %7314 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %7316 = stablehlo.convert %7192 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %7317 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1751)
    %7318 = stablehlo.multiply %7316, %7317 : tensor<16x2048xf32> loc(#loc1751)
    %7319 = stablehlo.add %7315, %7318 : tensor<16x2048xf32> loc(#loc1752)
    %7320 = stablehlo.convert %7319 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1753)
    %7321 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1754)
    %7322 = stablehlo.power %7319, %7321 : tensor<16x2048xf32> loc(#loc1754)
    %7323 = stablehlo.reduce(%7322 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1755)
    %7324 = stablehlo.broadcast_in_dim %7323, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1756)
    %7325 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1757)
    %7326 = stablehlo.divide %7324, %7325 : tensor<16x1xf32> loc(#loc1757)
    %7327 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1758)
    %7328 = stablehlo.add %7326, %7327 : tensor<16x1xf32> loc(#loc1758)
    %7329 = stablehlo.rsqrt %7328 : tensor<16x1xf32> loc(#loc1678)
    %7330 = stablehlo.broadcast_in_dim %7329, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1759)
    %7331 = stablehlo.multiply %7319, %7330 : tensor<16x2048xf32> loc(#loc1759)
    %7332 = stablehlo.convert %7331 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %7333 = stablehlo.broadcast_in_dim %arg384, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1760)
    %7334 = stablehlo.broadcast_in_dim %7333, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1761)
    %7335 = stablehlo.multiply %7332, %7334 : tensor<16x2048xbf16> loc(#loc1761)
    %7336 = stablehlo.dot_general %7335, %arg383, contracting_dims = [1] x [1] : (tensor<16x2048xbf16>, tensor<128x2048xbf16>) -> tensor<16x128xbf16> loc(#loc1762)
    %7337 = stablehlo.reshape %7336 : (tensor<16x128xbf16>) -> tensor<16x1x128xbf16> loc(#loc1763)
    %7338 = stablehlo.reshape %7337 : (tensor<16x1x128xbf16>) -> tensor<16x128xbf16> loc(#loc1764)
    %7339 = call @jax_fused_moe_func_padded(%7335, %arg381, %arg382, %7338) : (tensor<16x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128x2048x768xbf16>, tensor<16x128xbf16>) -> tensor<16x2048xbf16> loc(#loc1765)
    %7340 = stablehlo.convert %7339 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1672)
    %7341 = stablehlo.convert %7320 : (tensor<16x2048xbf16>) -> tensor<16x2048xf32> loc(#loc1750)
    %7342 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1776)
    %7343 = stablehlo.multiply %7341, %7342 : tensor<16x2048xf32> loc(#loc1776)
    %7344 = stablehlo.add %7340, %7343 : tensor<16x2048xf32> loc(#loc1777)
    %7345 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<16x2048xf32> loc(#loc1778)
    %7346 = stablehlo.power %7344, %7345 : tensor<16x2048xf32> loc(#loc1778)
    %7347 = stablehlo.reduce(%7346 init: %cst_5) applies stablehlo.add across dimensions = [1] : (tensor<16x2048xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1779)
    %7348 = stablehlo.broadcast_in_dim %7347, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1780)
    %7349 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1781)
    %7350 = stablehlo.divide %7348, %7349 : tensor<16x1xf32> loc(#loc1781)
    %7351 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc1782)
    %7352 = stablehlo.add %7350, %7351 : tensor<16x1xf32> loc(#loc1782)
    %7353 = stablehlo.rsqrt %7352 : tensor<16x1xf32> loc(#loc1678)
    %7354 = stablehlo.broadcast_in_dim %7353, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x2048xf32> loc(#loc1783)
    %7355 = stablehlo.multiply %7344, %7354 : tensor<16x2048xf32> loc(#loc1783)
    %7356 = stablehlo.convert %7355 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1680)
    %7357 = stablehlo.broadcast_in_dim %arg434, dims = [1] : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc1784)
    %7358 = stablehlo.broadcast_in_dim %7357, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1785)
    %7359 = stablehlo.multiply %7356, %7358 : tensor<16x2048xbf16> loc(#loc1785)
    return %120#0, %273#0, %426#0, %579#0, %732#0, %885#0, %1038#0, %1191#0, %1344#0, %1497#0, %1650#0, %1803#0, %1956#0, %2109#0, %2262#0, %2415#0, %2568#0, %2721#0, %2874#0, %3027#0, %3180#0, %3333#0, %3486#0, %3639#0, %3792#0, %3945#0, %4098#0, %4251#0, %4404#0, %4557#0, %4710#0, %4863#0, %5016#0, %5169#0, %5322#0, %5475#0, %5628#0, %5781#0, %5934#0, %6087#0, %6240#0, %6393#0, %6546#0, %6699#0, %6852#0, %7005#0, %7158#0, %7311#0, %7359 : tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<16x2048xbf16> loc(#loc)
  } loc(#loc)
  func.func private @_take(%arg0: tensor<152064x2048xbf16> loc(unknown), %arg1: tensor<16xi32> loc(unknown)) -> tensor<16x2048xbf16> {
    %cst = stablehlo.constant dense<0x7FC0> : tensor<bf16> loc(#loc1787)
    %c = stablehlo.constant dense<true> : tensor<i1> loc(#loc1788)
    %c_0 = stablehlo.constant dense<152063> : tensor<1xi32> loc(#loc1787)
    %c_1 = stablehlo.constant dense<152064> : tensor<i32> loc(#loc2163)
    %c_2 = stablehlo.constant dense<0> : tensor<i32> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1789)
    %1 = stablehlo.compare  LT, %arg1, %0,  SIGNED : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1> loc(#loc1789)
    %2 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<16xi32> loc(#loc1790)
    %3 = stablehlo.add %arg1, %2 : tensor<16xi32> loc(#loc1790)
    %4 = call @_where(%1, %3, %arg1) : (tensor<16xi1>, tensor<16xi32>, tensor<16xi32>) -> tensor<16xi32> loc(#loc1791)
    %5 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<16xi32>) -> tensor<16x1xi32> loc(#loc1792)
    %6 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<16x1xi32> loc(#loc1793)
    %7 = stablehlo.compare  GE, %5, %6,  SIGNED : (tensor<16x1xi32>, tensor<16x1xi32>) -> tensor<16x1xi1> loc(#loc1793)
    %8 = stablehlo.broadcast_in_dim %c_0, dims = [1] : (tensor<1xi32>) -> tensor<1x1xi32> loc(#loc1792)
    %9 = stablehlo.broadcast_in_dim %8, dims = [0, 1] : (tensor<1x1xi32>) -> tensor<16x1xi32> loc(#loc1794)
    %10 = stablehlo.compare  LE, %5, %9,  SIGNED : (tensor<16x1xi32>, tensor<16x1xi32>) -> tensor<16x1xi1> loc(#loc1794)
    %11 = stablehlo.and %7, %10 : tensor<16x1xi1> loc(#loc1795)
    %12 = stablehlo.reduce(%11 init: %c) applies stablehlo.and across dimensions = [1] : (tensor<16x1xi1>, tensor<i1>) -> tensor<16xi1> loc(#loc1788)
    %13 = "stablehlo.gather"(%arg0, %5) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2048>}> : (tensor<152064x2048xbf16>, tensor<16x1xi32>) -> tensor<16x2048xbf16> loc(#loc1787)
    %14 = stablehlo.broadcast_in_dim %12, dims = [0] : (tensor<16xi1>) -> tensor<16x2048xi1> loc(#loc1792)
    %15 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<16x2048xbf16> loc(#loc1792)
    %16 = stablehlo.select %14, %13, %15 : tensor<16x2048xi1>, tensor<16x2048xbf16> loc(#loc1796)
    return %16 : tensor<16x2048xbf16> loc(#loc2163)
  } loc(#loc2163)
  func.func private @_where(%arg0: tensor<16xi1> loc(unknown), %arg1: tensor<16xi32> loc(unknown), %arg2: tensor<16xi32> loc(unknown)) -> tensor<16xi32> {
    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<16xi1>, tensor<16xi32> loc(#loc1796)
    return %0 : tensor<16xi32> loc(#loc2164)
  } loc(#loc2164)
  func.func private @_jax_attn_func(%arg0: tensor<14813x32x4x2x128xbf16> loc(unknown), %arg1: tensor<16x4096xbf16> loc(unknown), %arg2: tensor<16x512xbf16> loc(unknown), %arg3: tensor<16x512xbf16> loc(unknown), %arg4: tensor<131072xi32> loc(unknown), %arg5: tensor<256xi32> loc(unknown), %arg6: tensor<257xi32> loc(unknown), %arg7: tensor<3xi32> loc(unknown)) -> (tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16>) {
    %0 = stablehlo.reshape %arg1 : (tensor<16x4096xbf16>) -> tensor<16x32x128xbf16> loc(#loc1799)
    %1 = stablehlo.reshape %arg2 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1800)
    %2 = stablehlo.reshape %arg3 : (tensor<16x512xbf16>) -> tensor<16x4x128xbf16> loc(#loc1801)
    %3:2 = call @_ragged_paged_attention(%0, %1, %2, %arg0, %arg5, %arg4, %arg6, %arg7) : (tensor<16x32x128xbf16>, tensor<16x4x128xbf16>, tensor<16x4x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<256xi32>, tensor<131072xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<16x32x128xbf16>, tensor<14813x32x4x2x128xbf16>) loc(#loc1802)
    %4 = stablehlo.reshape %3#0 : (tensor<16x32x128xbf16>) -> tensor<16x4096xbf16> loc(#loc1803)
    return %3#1, %4 : tensor<14813x32x4x2x128xbf16>, tensor<16x4096xbf16> loc(#loc2165)
  } loc(#loc2165)
  func.func private @_ragged_paged_attention(%arg0: tensor<16x32x128xbf16> loc(unknown), %arg1: tensor<16x4x128xbf16> loc(unknown), %arg2: tensor<16x4x128xbf16> loc(unknown), %arg3: tensor<14813x32x4x2x128xbf16> loc(unknown), %arg4: tensor<256xi32> loc(unknown), %arg5: tensor<131072xi32> loc(unknown), %arg6: tensor<257xi32> loc(unknown), %arg7: tensor<3xi32> loc(unknown)) -> (tensor<16x32x128xbf16>, tensor<14813x32x4x2x128xbf16>) {
    %0:2 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7) in_shardings=[<@mesh, [{}, {"model"}, {}]>, <@mesh, [{}, {"model"}, {}]>, <@mesh, [{}, {"model"}, {}]>, <@mesh, [{}, {}, {"model"}, {}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}]>] out_shardings=[<@mesh, [{}, {"model"}, {}]>, <@mesh, [{}, {}, {"model"}, {}, {}]>] manual_axes={"data", "model"} (%arg8: tensor<16x8x128xbf16> loc("shard_map"), %arg9: tensor<16x1x128xbf16> loc("shard_map"), %arg10: tensor<16x1x128xbf16> loc("shard_map"), %arg11: tensor<14813x32x1x2x128xbf16> loc("shard_map"), %arg12: tensor<256xi32> loc("shard_map"), %arg13: tensor<131072xi32> loc("shard_map"), %arg14: tensor<257xi32> loc("shard_map"), %arg15: tensor<3xi32> loc("shard_map")) {
      %1:2 = func.call @ragged_paged_attention(%arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15) : (tensor<16x8x128xbf16>, tensor<16x1x128xbf16>, tensor<16x1x128xbf16>, tensor<14813x32x1x2x128xbf16>, tensor<256xi32>, tensor<131072xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<16x8x128xbf16>, tensor<14813x32x1x2x128xbf16>) loc(#loc1806)
      sdy.return %1#0, %1#1 : tensor<16x8x128xbf16>, tensor<14813x32x1x2x128xbf16> loc(#loc608)
    } : (tensor<16x32x128xbf16>, tensor<16x4x128xbf16>, tensor<16x4x128xbf16>, tensor<14813x32x4x2x128xbf16>, tensor<256xi32>, tensor<131072xi32>, tensor<257xi32>, tensor<3xi32>) -> (tensor<16x32x128xbf16>, tensor<14813x32x4x2x128xbf16>) loc(#loc1805)
    return %0#0, %0#1 : tensor<16x32x128xbf16>, tensor<14813x32x4x2x128xbf16> loc(#loc2166)
  } loc(#loc2166)
  func.func private @ragged_paged_attention(%arg0: tensor<16x8x128xbf16> loc(unknown), %arg1: tensor<16x1x128xbf16> loc(unknown), %arg2: tensor<16x1x128xbf16> loc(unknown), %arg3: tensor<14813x32x1x2x128xbf16> loc(unknown), %arg4: tensor<256xi32> loc(unknown), %arg5: tensor<131072xi32> loc(unknown), %arg6: tensor<257xi32> loc(unknown), %arg7: tensor<3xi32> loc(unknown)) -> (tensor<16x8x128xbf16>, tensor<14813x32x1x2x128xbf16>) {
    %c = stablehlo.constant dense<-1> : tensor<i32> loc(#loc)
    %c_0 = stablehlo.constant dense<0> : tensor<i32> loc(#loc)
    %0 = stablehlo.reshape %arg0 : (tensor<16x8x128xbf16>) -> tensor<16x1x8x128xbf16> loc(#loc1808)
    %1 = call @_pad(%0, %c_0) : (tensor<16x1x8x128xbf16>, tensor<i32>) -> tensor<16x1x8x128xbf16> loc(#loc1809)
    %2 = stablehlo.reshape %1 : (tensor<16x1x8x128xbf16>) -> tensor<16x1x4x2x128xbf16> loc(#loc1810)
    %3 = stablehlo.transpose %2, dims = [1, 0, 2, 3, 4] : (tensor<16x1x4x2x128xbf16>) -> tensor<1x16x4x2x128xbf16> loc(#loc1811)
    %4 = stablehlo.concatenate %arg1, %arg2, dim = 2 : (tensor<16x1x128xbf16>, tensor<16x1x128xbf16>) -> tensor<16x1x256xbf16> loc(#loc1812)
    %5 = stablehlo.reshape %4 : (tensor<16x1x256xbf16>) -> tensor<16x2x128xbf16> loc(#loc1813)
    %6 = call @_pad_67(%5, %c_0) : (tensor<16x2x128xbf16>, tensor<i32>) -> tensor<16x2x128xbf16> loc(#loc1814)
    %7 = stablehlo.reshape %6 : (tensor<16x2x128xbf16>) -> tensor<16x1x2x128xbf16> loc(#loc1815)
    %8 = stablehlo.slice %arg7 [2:3] : (tensor<3xi32>) -> tensor<1xi32> loc(#loc1816)
    %9 = stablehlo.reshape %8 : (tensor<1xi32>) -> tensor<i32> loc(#loc1817)
    %10 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<i32>) -> tensor<3xi32> loc(#loc1818)
    %11 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<4xi32> loc(#loc1819)
    %12 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<6xi32> loc(#loc1820)
    %13:2 = stablehlo.custom_call @tpu_custom_call(%9, %arg4, %arg5, %arg6, %arg7, %10, %11, %12, %3, %7, %arg3) {backend_config = "{\22custom_call_config\22: {\22body\22: \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\22, \22serialization_format\22: 1, \22needs_layout_passes\22: true}, \22scoped_memory_configs\22: [{\22memory_space\22:1, \22offset\22: 0, \22size\22: 104857600}]}", kernel_name = "RPA-bq_16-bkvp_64-p_32", operand_layouts = [dense<> : tensor<0xindex>, dense<0> : tensor<1xindex>, dense<0> : tensor<1xindex>, dense<0> : tensor<1xindex>, dense<0> : tensor<1xindex>, dense<0> : tensor<1xindex>, dense<0> : tensor<1xindex>, dense<0> : tensor<1xindex>, dense<[4, 3, 2, 1, 0]> : tensor<5xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<[4, 3, 2, 1, 0]> : tensor<5xindex>], output_operand_aliases = [#stablehlo.output_operand_alias<output_tuple_indices = [0], operand_index = 8, operand_tuple_indices = []>, #stablehlo.output_operand_alias<output_tuple_indices = [1], operand_index = 10, operand_tuple_indices = []>], result_layouts = [dense<[4, 3, 2, 1, 0]> : tensor<5xindex>, dense<[4, 3, 2, 1, 0]> : tensor<5xindex>]} : (tensor<i32>, tensor<256xi32>, tensor<131072xi32>, tensor<257xi32>, tensor<3xi32>, tensor<3xi32>, tensor<4xi32>, tensor<6xi32>, tensor<1x16x4x2x128xbf16>, tensor<16x1x2x128xbf16>, tensor<14813x32x1x2x128xbf16>) -> (tensor<1x16x4x2x128xbf16>, tensor<14813x32x1x2x128xbf16>) loc(#loc1821)
    %14 = stablehlo.transpose %13#0, dims = [1, 0, 2, 3, 4] : (tensor<1x16x4x2x128xbf16>) -> tensor<16x1x4x2x128xbf16> loc(#loc1822)
    %15 = stablehlo.reshape %14 : (tensor<16x1x4x2x128xbf16>) -> tensor<16x1x8x128xbf16> loc(#loc1823)
    %16 = stablehlo.reshape %15 : (tensor<16x1x8x128xbf16>) -> tensor<16x8x128xbf16> loc(#loc1824)
    return %16, %13#1 : tensor<16x8x128xbf16>, tensor<14813x32x1x2x128xbf16> loc(#loc2167)
  } loc(#loc2167)
  func.func private @_pad(%arg0: tensor<16x1x8x128xbf16> loc(unknown), %arg1: tensor<i32> loc(unknown)) -> tensor<16x1x8x128xbf16> {
    %0 = stablehlo.convert %arg1 : (tensor<i32>) -> tensor<bf16> loc(#loc1826)
    %1 = stablehlo.pad %arg0, %0, low = [0, 0, 0, 0], high = [0, 0, 0, 0], interior = [0, 0, 0, 0] : (tensor<16x1x8x128xbf16>, tensor<bf16>) -> tensor<16x1x8x128xbf16> loc(#loc1827)
    return %1 : tensor<16x1x8x128xbf16> loc(#loc2168)
  } loc(#loc2168)
  func.func private @_pad_67(%arg0: tensor<16x2x128xbf16> loc(unknown), %arg1: tensor<i32> loc(unknown)) -> tensor<16x2x128xbf16> {
    %0 = stablehlo.convert %arg1 : (tensor<i32>) -> tensor<bf16> loc(#loc1829)
    %1 = stablehlo.pad %arg0, %0, low = [0, 0, 0], high = [0, 0, 0], interior = [0, 0, 0] : (tensor<16x2x128xbf16>, tensor<bf16>) -> tensor<16x2x128xbf16> loc(#loc1830)
    return %1 : tensor<16x2x128xbf16> loc(#loc2169)
  } loc(#loc2169)
  func.func private @jax_fused_moe_func_padded(%arg0: tensor<16x2048xbf16> loc(unknown), %arg1: tensor<128x1536x2048xbf16> loc(unknown), %arg2: tensor<128x2048x768xbf16> loc(unknown), %arg3: tensor<16x128xbf16> loc(unknown)) -> tensor<16x2048xbf16> {
    %c = stablehlo.constant dense<16> : tensor<i32> loc(#loc2170)
    %c_0 = stablehlo.constant dense<1> : tensor<i32> loc(#loc2170)
    %c_1 = stablehlo.constant dense<128> : tensor<i32> loc(#loc)
    %c_2 = stablehlo.constant dense<0> : tensor<i32> loc(#loc)
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %cst_3 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg3 : (tensor<16x128xbf16>) -> tensor<16x128xf32> loc(#loc1832)
    %1 = stablehlo.reduce(%0 init: %cst_3) applies stablehlo.maximum across dimensions = [1] : (tensor<16x128xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1833)
    %2 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16xf32> loc(#loc1834)
    %3 = stablehlo.maximum %2, %1 : tensor<16xf32> loc(#loc1834)
    %4 = stablehlo.broadcast_in_dim %3, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1835)
    %5 = stablehlo.broadcast_in_dim %4, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x128xf32> loc(#loc1836)
    %6 = stablehlo.subtract %0, %5 : tensor<16x128xf32> loc(#loc1836)
    %7 = stablehlo.exponential %6 : tensor<16x128xf32> loc(#loc1837)
    %8 = stablehlo.reduce(%7 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<16x128xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1838)
    %9 = stablehlo.broadcast_in_dim %8, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1835)
    %10 = stablehlo.broadcast_in_dim %9, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x128xf32> loc(#loc1839)
    %11 = stablehlo.divide %7, %10 : tensor<16x128xf32> loc(#loc1839)
    %12:2 = stablehlo.custom_call @mhlo.topk(%11) {mhlo.attributes = {k = 8 : i64}, mhlo.version = 1 : i64} : (tensor<16x128xf32>) -> (tensor<16x8xf32>, tensor<16x8xi32>) loc(#loc1840)
    %13 = stablehlo.reduce(%12#0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<16x8xf32>, tensor<f32>) -> tensor<16xf32> loc(#loc1841)
    %14 = stablehlo.broadcast_in_dim %13, dims = [0] : (tensor<16xf32>) -> tensor<16x1xf32> loc(#loc1842)
    %15 = stablehlo.broadcast_in_dim %14, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x8xf32> loc(#loc1843)
    %16 = stablehlo.divide %12#0, %15 : tensor<16x8xf32> loc(#loc1843)
    %17 = stablehlo.convert %16 : (tensor<16x8xf32>) -> tensor<16x8xbf16> loc(#loc1844)
    %18 = stablehlo.reshape %12#1 : (tensor<16x8xi32>) -> tensor<128xi32> loc(#loc1845)
    %19 = call @argsort(%18) : (tensor<128xi32>) -> tensor<128xi32> loc(#loc1846)
    %20 = call @argsort_93(%19) : (tensor<128xi32>) -> tensor<128xi32> loc(#loc1847)
    %21 = stablehlo.iota dim = 0 : tensor<16xi32> loc(#loc1848)
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<16xi32>) -> tensor<16x8xi32> loc(#loc1849)
    %23 = stablehlo.reshape %22 : (tensor<16x8xi32>) -> tensor<128xi32> loc(#loc1850)
    %24 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1851)
    %25 = stablehlo.compare  LT, %19, %24,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc1851)
    %26 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1852)
    %27 = stablehlo.add %19, %26 : tensor<128xi32> loc(#loc1852)
    %28 = stablehlo.select %25, %27, %19 : tensor<128xi1>, tensor<128xi32> loc(#loc1853)
    %29 = stablehlo.broadcast_in_dim %28, dims = [0] : (tensor<128xi32>) -> tensor<128x1xi32> loc(#loc1854)
    %30 = "stablehlo.gather"(%23, %29) <{dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>}> : (tensor<128xi32>, tensor<128x1xi32>) -> tensor<128xi32> loc(#loc1855)
    %31 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1856)
    %32 = call @clip(%18, %c_2) : (tensor<128xi32>, tensor<i32>) -> tensor<128xi32> loc(#loc1857)
    %33 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1858)
    %34 = stablehlo.compare  LT, %32, %33,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc1858)
    %35 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1859)
    %36 = stablehlo.add %32, %35 : tensor<128xi32> loc(#loc1859)
    %37 = stablehlo.select %34, %36, %32 : tensor<128xi1>, tensor<128xi32> loc(#loc1860)
    %38 = stablehlo.broadcast_in_dim %37, dims = [0] : (tensor<128xi32>) -> tensor<128x1xi32> loc(#loc1856)
    %39 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1856)
    %40 = "stablehlo.scatter"(%31, %38, %39) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>}> ({
    ^bb0(%arg4: tensor<i32> loc("scatter-add"), %arg5: tensor<i32> loc("scatter-add")):
      %73 = stablehlo.add %arg4, %arg5 : tensor<i32> loc(#loc1859)
      stablehlo.return %73 : tensor<i32> loc(#loc1861)
    }) : (tensor<128xi32>, tensor<128x1xi32>, tensor<128xi32>) -> tensor<128xi32> loc(#loc1861)
    %41 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1862)
    %42 = stablehlo.compare  LT, %30, %41,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc1862)
    %43 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1863)
    %44 = stablehlo.add %30, %43 : tensor<128xi32> loc(#loc1863)
    %45 = stablehlo.select %42, %44, %30 : tensor<128xi1>, tensor<128xi32> loc(#loc1864)
    %46 = stablehlo.broadcast_in_dim %45, dims = [0] : (tensor<128xi32>) -> tensor<128x1xi32> loc(#loc1865)
    %47 = "stablehlo.gather"(%arg0, %46) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2048>}> : (tensor<16x2048xbf16>, tensor<128x1xi32>) -> tensor<128x2048xbf16> loc(#loc1866)
    %48 = sdy.manual_computation(%c_2, %47, %arg1, %40) in_shardings=[<@mesh, []>, <@mesh, [{}, {}]>, <@mesh, [{}, {"model"}, {}]>, <@mesh, [{}]>] out_shardings=[<@mesh, [{}, {"model"}]>] manual_axes={"data", "model"} (%arg4: tensor<i32> loc("shard_map"), %arg5: tensor<128x2048xbf16> loc("shard_map"), %arg6: tensor<128x384x2048xbf16> loc("shard_map"), %arg7: tensor<128xi32> loc("shard_map")) {
      %73 = func.call @gmm(%arg5, %arg6, %arg7, %arg4) : (tensor<128x2048xbf16>, tensor<128x384x2048xbf16>, tensor<128xi32>, tensor<i32>) -> tensor<128x384xbf16> loc(#loc1868)
      sdy.return %73 : tensor<128x384xbf16> loc(#loc608)
    } : (tensor<i32>, tensor<128x2048xbf16>, tensor<128x1536x2048xbf16>, tensor<128xi32>) -> tensor<128x1536xbf16> loc(#loc1867)
    %49 = stablehlo.reshape %48 : (tensor<128x1536xbf16>) -> tensor<128x4x384xbf16> loc(#loc1869)
    %50 = stablehlo.slice %49 [0:128, 0:4, 0:192] : (tensor<128x4x384xbf16>) -> tensor<128x4x192xbf16> loc(#loc1870)
    %51 = stablehlo.reshape %50 : (tensor<128x4x192xbf16>) -> tensor<128x768xbf16> loc(#loc1871)
    %52 = stablehlo.slice %49 [0:128, 0:4, 192:384] : (tensor<128x4x384xbf16>) -> tensor<128x4x192xbf16> loc(#loc1870)
    %53 = stablehlo.reshape %52 : (tensor<128x4x192xbf16>) -> tensor<128x768xbf16> loc(#loc1871)
    %54 = call @silu(%51) : (tensor<128x768xbf16>) -> tensor<128x768xbf16> loc(#loc1872)
    %55 = stablehlo.multiply %54, %53 : tensor<128x768xbf16> loc(#loc1873)
    %56 = sdy.sharding_constraint %55 <@mesh, [{}, {"model"}]> : tensor<128x768xbf16> loc(#loc1874)
    %57 = sdy.manual_computation(%c_2, %56, %arg2, %40) in_shardings=[<@mesh, []>, <@mesh, [{}, {"model"}]>, <@mesh, [{}, {}, {"model"}]>, <@mesh, [{}]>] out_shardings=[<@mesh, [{}, {}]>] manual_axes={"data", "model"} (%arg4: tensor<i32> loc("shard_map"), %arg5: tensor<128x192xbf16> loc("shard_map"), %arg6: tensor<128x2048x192xbf16> loc("shard_map"), %arg7: tensor<128xi32> loc("shard_map")) {
      %73 = func.call @gmm_254(%arg5, %arg6, %arg7, %arg4) : (tensor<128x192xbf16>, tensor<128x2048x192xbf16>, tensor<128xi32>, tensor<i32>) -> tensor<128x2048xbf16> loc(#loc1876)
      %74 = "stablehlo.all_reduce"(%73) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>, replica_groups = dense<[[0, 1, 2, 3]]> : tensor<1x4xi64>, use_global_device_ids}> ({
      ^bb0(%arg8: tensor<bf16> loc("psum"), %arg9: tensor<bf16> loc("psum")):
        %75 = stablehlo.add %arg8, %arg9 : tensor<bf16> loc(#loc1878)
        stablehlo.return %75 : tensor<bf16> loc(#loc1877)
      }) : (tensor<128x2048xbf16>) -> tensor<128x2048xbf16> loc(#loc1877)
      sdy.return %74 : tensor<128x2048xbf16> loc(#loc608)
    } : (tensor<i32>, tensor<128x768xbf16>, tensor<128x2048x768xbf16>, tensor<128xi32>) -> tensor<128x2048xbf16> loc(#loc1875)
    %58 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1879)
    %59 = stablehlo.compare  LT, %20, %58,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc1879)
    %60 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1880)
    %61 = stablehlo.add %20, %60 : tensor<128xi32> loc(#loc1880)
    %62 = stablehlo.select %59, %61, %20 : tensor<128xi1>, tensor<128xi32> loc(#loc1881)
    %63 = stablehlo.broadcast_in_dim %62, dims = [0] : (tensor<128xi32>) -> tensor<128x1xi32> loc(#loc1882)
    %64 = "stablehlo.gather"(%57, %63) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2048>}> : (tensor<128x2048xbf16>, tensor<128x1xi32>) -> tensor<128x2048xbf16> loc(#loc1883)
    %65 = stablehlo.reshape %64 : (tensor<128x2048xbf16>) -> tensor<16x8x2048xbf16> loc(#loc1884)
    %66 = stablehlo.broadcast_in_dim %17, dims = [0, 1] : (tensor<16x8xbf16>) -> tensor<16x8x1xbf16> loc(#loc1885)
    %67 = stablehlo.broadcast_in_dim %66, dims = [0, 1, 2] : (tensor<16x8x1xbf16>) -> tensor<16x8x2048xbf16> loc(#loc1886)
    %68 = stablehlo.multiply %65, %67 : tensor<16x8x2048xbf16> loc(#loc1886)
    %69 = stablehlo.convert %68 : (tensor<16x8x2048xbf16>) -> tensor<16x8x2048xf32> loc(#loc1887)
    %70 = stablehlo.reduce(%69 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<16x8x2048xf32>, tensor<f32>) -> tensor<16x2048xf32> loc(#loc1888)
    %71 = stablehlo.convert %70 : (tensor<16x2048xf32>) -> tensor<16x2048xbf16> loc(#loc1887)
    %72 = sdy.sharding_constraint %71 <@mesh, [{}, {}]> : tensor<16x2048xbf16> loc(#loc1889)
    return %72 : tensor<16x2048xbf16> loc(#loc2170)
  } loc(#loc2170)
  func.func private @argsort(%arg0: tensor<128xi32> loc(unknown)) -> tensor<128xi32> {
    %0 = stablehlo.iota dim = 0 : tensor<128xi32> loc(#loc1891)
    %1:2 = "stablehlo.sort"(%arg0, %0) <{dimension = 0 : i64, is_stable = true}> ({
    ^bb0(%arg1: tensor<i32> loc("sort"), %arg2: tensor<i32> loc("sort"), %arg3: tensor<i32> loc("sort"), %arg4: tensor<i32> loc("sort")):
      %2 = stablehlo.compare  LT, %arg1, %arg2,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc1893)
      stablehlo.return %2 : tensor<i1> loc(#loc1892)
    }) : (tensor<128xi32>, tensor<128xi32>) -> (tensor<128xi32>, tensor<128xi32>) loc(#loc1892)
    return %1#1 : tensor<128xi32> loc(#loc2171)
  } loc(#loc2171)
  func.func private @argsort_93(%arg0: tensor<128xi32> loc(unknown)) -> tensor<128xi32> {
    %0 = stablehlo.iota dim = 0 : tensor<128xi32> loc(#loc1895)
    %1:2 = "stablehlo.sort"(%arg0, %0) <{dimension = 0 : i64, is_stable = true}> ({
    ^bb0(%arg1: tensor<i32> loc("sort"), %arg2: tensor<i32> loc("sort"), %arg3: tensor<i32> loc("sort"), %arg4: tensor<i32> loc("sort")):
      %2 = stablehlo.compare  LT, %arg1, %arg2,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc1897)
      stablehlo.return %2 : tensor<i1> loc(#loc1896)
    }) : (tensor<128xi32>, tensor<128xi32>) -> (tensor<128xi32>, tensor<128xi32>) loc(#loc1896)
    return %1#1 : tensor<128xi32> loc(#loc2172)
  } loc(#loc2172)
  func.func private @clip(%arg0: tensor<128xi32> loc(unknown), %arg1: tensor<i32> loc(unknown)) -> tensor<128xi32> {
    %0 = stablehlo.convert %arg1 : tensor<i32> loc(#loc1899)
    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1900)
    %2 = stablehlo.maximum %1, %arg0 : tensor<128xi32> loc(#loc1900)
    return %2 : tensor<128xi32> loc(#loc2173)
  } loc(#loc2173)
  func.func private @gmm(%arg0: tensor<128x2048xbf16> loc(unknown), %arg1: tensor<128x384x2048xbf16> loc(unknown), %arg2: tensor<128xi32> loc(unknown), %arg3: tensor<i32> loc(unknown)) -> tensor<128x384xbf16> {
    %c = stablehlo.constant dense<2> : tensor<i32> loc(#loc2174)
    %cst = stablehlo.constant dense<5.000000e-01> : tensor<f32> loc(#loc)
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<f32> loc(#loc)
    %c_2 = stablehlo.constant dense<0> : tensor<i32> loc(#loc)
    %c_3 = stablehlo.constant dense<1> : tensor<i32> loc(#loc)
    %c_4 = stablehlo.constant dense<128> : tensor<i32> loc(#loc)
    %cst_5 = stablehlo.constant dense<0.000000e+00> : tensor<2xf32> loc(#loc2174)
    %0 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<i32>) -> tensor<1xi32> loc(#loc1902)
    %1 = stablehlo.slice %0 [0:1] : (tensor<1xi32>) -> tensor<1xi32> loc(#loc1903)
    %2 = stablehlo.reshape %1 : (tensor<1xi32>) -> tensor<i32> loc(#loc1904)
    %3 = stablehlo.add %2, %c_4 : tensor<i32> loc(#loc1905)
    %4 = stablehlo.subtract %3, %c_3 : tensor<i32> loc(#loc1906)
    %5 = call @cumsum(%arg2) : (tensor<128xi32>) -> tensor<128xi32> loc(#loc1907)
    %6 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<1xi32> loc(#loc1908)
    %7 = stablehlo.concatenate %6, %5, dim = 0 : (tensor<1xi32>, tensor<128xi32>) -> tensor<129xi32> loc(#loc1909)
    %8 = stablehlo.broadcast_in_dim %c_4, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1910)
    %9 = stablehlo.add %5, %8 : tensor<128xi32> loc(#loc1910)
    %10 = stablehlo.broadcast_in_dim %c_3, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1911)
    %11 = stablehlo.subtract %9, %10 : tensor<128xi32> loc(#loc1911)
    %12 = call @floor_divide(%11, %c_4) : (tensor<128xi32>, tensor<i32>) -> tensor<128xi32> loc(#loc1912)
    %13 = stablehlo.broadcast_in_dim %c_4, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1913)
    %14 = stablehlo.multiply %12, %13 : tensor<128xi32> loc(#loc1913)
    %15 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<1xi32> loc(#loc1914)
    %16 = stablehlo.slice %5 [0:127] : (tensor<128xi32>) -> tensor<127xi32> loc(#loc1915)
    %17 = stablehlo.concatenate %15, %16, dim = 0 : (tensor<1xi32>, tensor<127xi32>) -> tensor<128xi32> loc(#loc1916)
    %18 = call @floor_divide(%17, %c_4) : (tensor<128xi32>, tensor<i32>) -> tensor<128xi32> loc(#loc1917)
    %19 = stablehlo.broadcast_in_dim %c_4, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1918)
    %20 = stablehlo.multiply %18, %19 : tensor<128xi32> loc(#loc1918)
    %21 = stablehlo.subtract %14, %20 : tensor<128xi32> loc(#loc1919)
    %22 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1920)
    %23 = stablehlo.compare  EQ, %arg2, %22,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc1920)
    %24 = call @_where_135(%23, %c_2, %21) : (tensor<128xi1>, tensor<i32>, tensor<128xi32>) -> tensor<128xi32> loc(#loc1921)
    %25 = call @floor_divide(%24, %c_4) : (tensor<128xi32>, tensor<i32>) -> tensor<128xi32> loc(#loc1922)
    %26 = stablehlo.iota dim = 0 : tensor<128xi32> loc(#loc1923)
    %27 = call @_roll_static(%25) : (tensor<128xi32>) -> tensor<128xi32> loc(#loc1924)
    %28 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<1xi32> loc(#loc1925)
    %29 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<i32> loc(#loc1925)
    %30 = "stablehlo.scatter"(%27, %28, %29) <{indices_are_sorted = true, scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0], scatter_dims_to_operand_dims = [0]>, unique_indices = true}> ({
    ^bb0(%arg4: tensor<i32> loc("scatter"), %arg5: tensor<i32> loc("scatter")):
      stablehlo.return %arg5 : tensor<i32> loc(#loc1926)
    }) : (tensor<128xi32>, tensor<1xi32>, tensor<i32>) -> tensor<128xi32> loc(#loc1926)
    %31 = call @cumsum(%30) : (tensor<128xi32>) -> tensor<128xi32> loc(#loc1927)
    %32 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1925)
    %33 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1928)
    %34 = stablehlo.compare  LT, %31, %33,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc1928)
    %35 = stablehlo.broadcast_in_dim %c_4, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1929)
    %36 = stablehlo.add %31, %35 : tensor<128xi32> loc(#loc1929)
    %37 = stablehlo.select %34, %36, %31 : tensor<128xi1>, tensor<128xi32> loc(#loc1930)
    %38 = stablehlo.broadcast_in_dim %37, dims = [0] : (tensor<128xi32>) -> tensor<128x1xi32> loc(#loc1925)
    %39 = stablehlo.broadcast_in_dim %c_3, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1925)
    %40 = "stablehlo.scatter"(%32, %38, %39) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>}> ({
    ^bb0(%arg4: tensor<i32> loc("scatter-add"), %arg5: tensor<i32> loc("scatter-add")):
      %131 = stablehlo.add %arg4, %arg5 : tensor<i32> loc(#loc1929)
      stablehlo.return %131 : tensor<i32> loc(#loc1931)
    }) : (tensor<128xi32>, tensor<128x1xi32>, tensor<128xi32>) -> tensor<128xi32> loc(#loc1931)
    %41 = call @cumsum(%40) : (tensor<128xi32>) -> tensor<128xi32> loc(#loc1927)
    %42 = stablehlo.broadcast_in_dim %c_3, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1932)
    %43 = stablehlo.subtract %41, %42 : tensor<128xi32> loc(#loc1932)
    %44 = call @_take_147(%26, %43) : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi32> loc(#loc1933)
    %45 = stablehlo.slice %7 [0:128] : (tensor<129xi32>) -> tensor<128xi32> loc(#loc1934)
    %46 = call @remainder(%45, %c_4) : (tensor<128xi32>, tensor<i32>) -> tensor<128xi32> loc(#loc1935)
    %47 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1936)
    %48 = stablehlo.compare  EQ, %46, %47,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc1936)
    %49 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1937)
    %50 = stablehlo.compare  EQ, %arg2, %49,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc1937)
    %51 = stablehlo.or %48, %50 : tensor<128xi1> loc(#loc1938)
    %52 = stablehlo.slice %7 [0:128] : (tensor<129xi32>) -> tensor<128xi32> loc(#loc1939)
    %53 = call @floor_divide(%52, %c_4) : (tensor<128xi32>, tensor<i32>) -> tensor<128xi32> loc(#loc1940)
    %54 = call @_where_135(%51, %c_3, %53) : (tensor<128xi1>, tensor<i32>, tensor<128xi32>) -> tensor<128xi32> loc(#loc1941)
    %55 = stablehlo.convert %54 : (tensor<128xi32>) -> tensor<128xf32> loc(#loc1942)
    %56 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32> loc(#loc1943)
    %57 = call @_ptp(%cst_5) : (tensor<2xf32>) -> tensor<f32> loc(#loc1944)
    %58 = stablehlo.compare  EQ, %57, %cst_0,  FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1> loc(#loc1945)
    %59 = stablehlo.slice %cst_5 [0:1] : (tensor<2xf32>) -> tensor<1xf32> loc(#loc1946)
    %60 = stablehlo.reshape %59 : (tensor<1xf32>) -> tensor<f32> loc(#loc1947)
    %61 = stablehlo.subtract %60, %cst : tensor<f32> loc(#loc1948)
    %62 = stablehlo.slice %cst_5 [0:1] : (tensor<2xf32>) -> tensor<1xf32> loc(#loc1946)
    %63 = stablehlo.reshape %62 : (tensor<1xf32>) -> tensor<f32> loc(#loc1947)
    %64 = call @_where_171(%58, %61, %63) : (tensor<i1>, tensor<f32>, tensor<f32>) -> tensor<f32> loc(#loc1949)
    %65 = call @_ptp(%cst_5) : (tensor<2xf32>) -> tensor<f32> loc(#loc1944)
    %66 = stablehlo.compare  EQ, %65, %cst_0,  FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1> loc(#loc1945)
    %67 = stablehlo.slice %cst_5 [1:2] : (tensor<2xf32>) -> tensor<1xf32> loc(#loc1946)
    %68 = stablehlo.reshape %67 : (tensor<1xf32>) -> tensor<f32> loc(#loc1947)
    %69 = stablehlo.add %68, %cst : tensor<f32> loc(#loc1950)
    %70 = stablehlo.slice %cst_5 [1:2] : (tensor<2xf32>) -> tensor<1xf32> loc(#loc1946)
    %71 = stablehlo.reshape %70 : (tensor<1xf32>) -> tensor<f32> loc(#loc1947)
    %72 = call @_where_171(%66, %69, %71) : (tensor<i1>, tensor<f32>, tensor<f32>) -> tensor<f32> loc(#loc1949)
    %73 = call @_linspace(%64, %72) : (tensor<f32>, tensor<f32>) -> tensor<2xf32> loc(#loc1951)
    %74 = call @searchsorted(%73, %55) : (tensor<2xf32>, tensor<128xf32>) -> tensor<128xi32> loc(#loc1952)
    %75 = stablehlo.dynamic_slice %73, %c_3, sizes = [1] : (tensor<2xf32>, tensor<i32>) -> tensor<1xf32> loc(#loc1953)
    %76 = stablehlo.reshape %75 : (tensor<1xf32>) -> tensor<f32> loc(#loc1947)
    %77 = stablehlo.broadcast_in_dim %76, dims = [] : (tensor<f32>) -> tensor<128xf32> loc(#loc1945)
    %78 = stablehlo.compare  EQ, %55, %77,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1> loc(#loc1945)
    %79 = call @_where_135(%78, %c_3, %74) : (tensor<128xi1>, tensor<i32>, tensor<128xi32>) -> tensor<128xi32> loc(#loc1949)
    %80 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<2xf32> loc(#loc1943)
    %81 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1954)
    %82 = stablehlo.compare  LT, %79, %81,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc1954)
    %83 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1950)
    %84 = stablehlo.add %79, %83 : tensor<128xi32> loc(#loc1950)
    %85 = stablehlo.select %82, %84, %79 : tensor<128xi1>, tensor<128xi32> loc(#loc1955)
    %86 = stablehlo.broadcast_in_dim %85, dims = [0] : (tensor<128xi32>) -> tensor<128x1xi32> loc(#loc1943)
    %87 = "stablehlo.scatter"(%80, %86, %56) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>}> ({
    ^bb0(%arg4: tensor<f32> loc("scatter-add"), %arg5: tensor<f32> loc("scatter-add")):
      %131 = stablehlo.add %arg4, %arg5 : tensor<f32> loc(#loc1950)
      stablehlo.return %131 : tensor<f32> loc(#loc1956)
    }) : (tensor<2xf32>, tensor<128x1xi32>, tensor<128xf32>) -> tensor<2xf32> loc(#loc1956)
    %88 = stablehlo.slice %87 [1:2] : (tensor<2xf32>) -> tensor<1xf32> loc(#loc1946)
    %89 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1xf32> loc(#loc1957)
    %90 = stablehlo.add %88, %89 : tensor<1xf32> loc(#loc1957)
    %91 = stablehlo.iota dim = 0 : tensor<1xi32> loc(#loc1958)
    %92 = stablehlo.convert %90 : (tensor<1xf32>) -> tensor<1xi32> loc(#loc1959)
    %93 = call @_roll_static_208(%92) : (tensor<1xi32>) -> tensor<1xi32> loc(#loc1960)
    %94 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<1xi32> loc(#loc1961)
    %95 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<i32> loc(#loc1961)
    %96 = "stablehlo.scatter"(%93, %94, %95) <{indices_are_sorted = true, scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0], scatter_dims_to_operand_dims = [0]>, unique_indices = true}> ({
    ^bb0(%arg4: tensor<i32> loc("scatter"), %arg5: tensor<i32> loc("scatter")):
      stablehlo.return %arg5 : tensor<i32> loc(#loc1962)
    }) : (tensor<1xi32>, tensor<1xi32>, tensor<i32>) -> tensor<1xi32> loc(#loc1962)
    %97 = call @cumsum_213(%96) : (tensor<1xi32>) -> tensor<1xi32> loc(#loc1963)
    %98 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1961)
    %99 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<1xi32> loc(#loc1964)
    %100 = stablehlo.compare  LT, %97, %99,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1> loc(#loc1964)
    %101 = stablehlo.broadcast_in_dim %c_4, dims = [] : (tensor<i32>) -> tensor<1xi32> loc(#loc1965)
    %102 = stablehlo.add %97, %101 : tensor<1xi32> loc(#loc1965)
    %103 = stablehlo.select %100, %102, %97 : tensor<1xi1>, tensor<1xi32> loc(#loc1966)
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32> loc(#loc1961)
    %105 = stablehlo.broadcast_in_dim %c_3, dims = [] : (tensor<i32>) -> tensor<1xi32> loc(#loc1961)
    %106 = "stablehlo.scatter"(%98, %104, %105) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>}> ({
    ^bb0(%arg4: tensor<i32> loc("scatter-add"), %arg5: tensor<i32> loc("scatter-add")):
      %131 = stablehlo.add %arg4, %arg5 : tensor<i32> loc(#loc1965)
      stablehlo.return %131 : tensor<i32> loc(#loc1967)
    }) : (tensor<128xi32>, tensor<1x1xi32>, tensor<1xi32>) -> tensor<128xi32> loc(#loc1967)
    %107 = call @cumsum(%106) : (tensor<128xi32>) -> tensor<128xi32> loc(#loc1963)
    %108 = stablehlo.broadcast_in_dim %c_3, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1968)
    %109 = stablehlo.subtract %107, %108 : tensor<128xi32> loc(#loc1968)
    %110 = call @_take_221(%91, %109) : (tensor<1xi32>, tensor<128xi32>) -> tensor<128xi32> loc(#loc1969)
    %111 = stablehlo.convert %2 : tensor<i32> loc(#loc1970)
    %112 = stablehlo.broadcast_in_dim %111, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1971)
    %113 = stablehlo.compare  LT, %44, %112,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc1971)
    %114 = stablehlo.convert %113 : (tensor<128xi1>) -> tensor<128xi32> loc(#loc1972)
    %115 = stablehlo.reduce(%114 init: %c_2) applies stablehlo.add across dimensions = [0] : (tensor<128xi32>, tensor<i32>) -> tensor<i32> loc(#loc1973)
    %116 = stablehlo.negate %115 : tensor<i32> loc(#loc1974)
    %117 = call @_roll_dynamic(%44, %116) : (tensor<128xi32>, tensor<i32>) -> tensor<128xi32> loc(#loc1975)
    %118 = stablehlo.negate %115 : tensor<i32> loc(#loc1976)
    %119 = call @_roll_dynamic(%110, %118) : (tensor<128xi32>, tensor<i32>) -> tensor<128xi32> loc(#loc1977)
    %120 = stablehlo.iota dim = 0 : tensor<128xi32> loc(#loc1978)
    %121 = stablehlo.convert %4 : tensor<i32> loc(#loc1979)
    %122 = stablehlo.broadcast_in_dim %121, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1980)
    %123 = stablehlo.compare  LE, %120, %122,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc1980)
    %124 = stablehlo.convert %2 : tensor<i32> loc(#loc1981)
    %125 = stablehlo.broadcast_in_dim %124, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1982)
    %126 = stablehlo.compare  GE, %120, %125,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc1982)
    %127 = stablehlo.and %123, %126 : tensor<128xi1> loc(#loc1983)
    %128 = call @_where_242(%127, %25, %c_2) : (tensor<128xi1>, tensor<128xi32>, tensor<i32>) -> tensor<128xi32> loc(#loc1984)
    %129 = stablehlo.reduce(%128 init: %c_2) applies stablehlo.add across dimensions = [0] : (tensor<128xi32>, tensor<i32>) -> tensor<i32> loc(#loc1985)
    %130 = stablehlo.custom_call @tpu_custom_call(%129, %7, %117, %119, %0, %arg0, %arg1) {backend_config = "{\22custom_call_config\22: {\22body\22: \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\22, \22cost_estimate\22: {\22bytes_accessed\22: 201949184, \22flops\22: 201326592, \22transcendentals\22: 0}, \22serialization_format\22: 1, \22needs_layout_passes\22: true}}", kernel_name = "kernel", operand_layouts = [dense<> : tensor<0xindex>, dense<0> : tensor<1xindex>, dense<0> : tensor<1xindex>, dense<0> : tensor<1xindex>, dense<0> : tensor<1xindex>, dense<[1, 0]> : tensor<2xindex>, dense<[2, 1, 0]> : tensor<3xindex>], result_layouts = [dense<[1, 0]> : tensor<2xindex>]} : (tensor<i32>, tensor<129xi32>, tensor<128xi32>, tensor<128xi32>, tensor<1xi32>, tensor<128x2048xbf16>, tensor<128x384x2048xbf16>) -> tensor<128x384xbf16> loc(#loc1986)
    return %130 : tensor<128x384xbf16> loc(#loc2174)
  } loc(#loc2174)
  func.func private @cumsum(%arg0: tensor<128xi32> loc(unknown)) -> tensor<128xi32> {
    %0 = call @cumsum_119(%arg0) : (tensor<128xi32>) -> tensor<128xi32> loc(#loc1579)
    return %0 : tensor<128xi32> loc(#loc2175)
  } loc(#loc2175)
  func.func private @cumsum_119(%arg0: tensor<128xi32> loc(callsite(#loc888 at #loc1427))) -> tensor<128xi32> {
    %c = stablehlo.constant dense<0> : tensor<i32> loc(#loc1988)
    %0 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<i32> loc(#loc1988)
    %1 = "stablehlo.reduce_window"(%arg0, %0) <{padding = dense<[[127, 0]]> : tensor<1x2xi64>, window_dimensions = array<i64: 128>}> ({
    ^bb0(%arg1: tensor<i32> loc("reduce_window_sum"), %arg2: tensor<i32> loc("reduce_window_sum")):
      %2 = stablehlo.add %arg1, %arg2 : tensor<i32> loc(#loc1988)
      stablehlo.return %2 : tensor<i32> loc(#loc1988)
    }) : (tensor<128xi32>, tensor<i32>) -> tensor<128xi32> loc(#loc1988)
    return %1 : tensor<128xi32> loc(#loc1579)
  } loc(#loc1579)
  func.func private @floor_divide(%arg0: tensor<128xi32> loc(unknown), %arg1: tensor<i32> loc(unknown)) -> tensor<128xi32> {
    %c = stablehlo.constant dense<1> : tensor<i32> loc(#loc2176)
    %c_0 = stablehlo.constant dense<0> : tensor<i32> loc(#loc2176)
    %0 = stablehlo.convert %arg1 : tensor<i32> loc(#loc1990)
    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1991)
    %2 = stablehlo.divide %arg0, %1 : tensor<128xi32> loc(#loc1991)
    %3 = stablehlo.sign %arg0 : tensor<128xi32> loc(#loc1992)
    %4 = stablehlo.sign %0 : tensor<i32> loc(#loc1992)
    %5 = stablehlo.broadcast_in_dim %4, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1993)
    %6 = stablehlo.compare  NE, %3, %5,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc1993)
    %7 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1994)
    %8 = stablehlo.remainder %arg0, %7 : tensor<128xi32> loc(#loc1994)
    %9 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1993)
    %10 = stablehlo.compare  NE, %8, %9,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc1993)
    %11 = stablehlo.and %6, %10 : tensor<128xi1> loc(#loc1995)
    %12 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1996)
    %13 = stablehlo.subtract %2, %12 : tensor<128xi32> loc(#loc1996)
    %14 = call @_where_129(%11, %13, %2) : (tensor<128xi1>, tensor<128xi32>, tensor<128xi32>) -> tensor<128xi32> loc(#loc1997)
    return %14 : tensor<128xi32> loc(#loc2176)
  } loc(#loc2176)
  func.func private @_where_129(%arg0: tensor<128xi1> loc(unknown), %arg1: tensor<128xi32> loc(unknown), %arg2: tensor<128xi32> loc(unknown)) -> tensor<128xi32> {
    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xi32> loc(#loc1998)
    return %0 : tensor<128xi32> loc(#loc2176)
  } loc(#loc2176)
  func.func private @_where_135(%arg0: tensor<128xi1> loc(unknown), %arg1: tensor<i32> loc(unknown), %arg2: tensor<128xi32> loc(unknown)) -> tensor<128xi32> {
    %0 = stablehlo.convert %arg1 : tensor<i32> loc(#loc2000)
    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2001)
    %2 = stablehlo.select %arg0, %1, %arg2 : tensor<128xi1>, tensor<128xi32> loc(#loc2002)
    return %2 : tensor<128xi32> loc(#loc2177)
  } loc(#loc2177)
  func.func private @_roll_static(%arg0: tensor<128xi32> loc(unknown)) -> tensor<128xi32> {
    %0 = stablehlo.slice %arg0 [127:128] : (tensor<128xi32>) -> tensor<1xi32> loc(#loc2004)
    %1 = stablehlo.slice %arg0 [0:127] : (tensor<128xi32>) -> tensor<127xi32> loc(#loc2004)
    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1xi32>, tensor<127xi32>) -> tensor<128xi32> loc(#loc2005)
    return %2 : tensor<128xi32> loc(#loc2178)
  } loc(#loc2178)
  func.func private @_take_147(%arg0: tensor<128xi32> loc(unknown), %arg1: tensor<128xi32> loc(unknown)) -> tensor<128xi32> {
    %c = stablehlo.constant dense<-2147483648> : tensor<i32> loc(#loc2006)
    %c_0 = stablehlo.constant dense<true> : tensor<i1> loc(#loc2007)
    %c_1 = stablehlo.constant dense<127> : tensor<1xi32> loc(#loc2006)
    %c_2 = stablehlo.constant dense<128> : tensor<i32> loc(#loc2178)
    %c_3 = stablehlo.constant dense<0> : tensor<i32> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %c_3, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1928)
    %1 = stablehlo.compare  LT, %arg1, %0,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc1928)
    %2 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1929)
    %3 = stablehlo.add %arg1, %2 : tensor<128xi32> loc(#loc1929)
    %4 = call @_where_129(%1, %3, %arg1) : (tensor<128xi1>, tensor<128xi32>, tensor<128xi32>) -> tensor<128xi32> loc(#loc2008)
    %5 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<128xi32>) -> tensor<128x1xi32> loc(#loc1925)
    %6 = stablehlo.broadcast_in_dim %c_3, dims = [] : (tensor<i32>) -> tensor<128x1xi32> loc(#loc2009)
    %7 = stablehlo.compare  GE, %5, %6,  SIGNED : (tensor<128x1xi32>, tensor<128x1xi32>) -> tensor<128x1xi1> loc(#loc2009)
    %8 = stablehlo.broadcast_in_dim %c_1, dims = [1] : (tensor<1xi32>) -> tensor<1x1xi32> loc(#loc1925)
    %9 = stablehlo.broadcast_in_dim %8, dims = [0, 1] : (tensor<1x1xi32>) -> tensor<128x1xi32> loc(#loc2010)
    %10 = stablehlo.compare  LE, %5, %9,  SIGNED : (tensor<128x1xi32>, tensor<128x1xi32>) -> tensor<128x1xi1> loc(#loc2010)
    %11 = stablehlo.and %7, %10 : tensor<128x1xi1> loc(#loc2011)
    %12 = stablehlo.reduce(%11 init: %c_0) applies stablehlo.and across dimensions = [1] : (tensor<128x1xi1>, tensor<i1>) -> tensor<128xi1> loc(#loc2007)
    %13 = "stablehlo.gather"(%arg0, %5) <{dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>}> : (tensor<128xi32>, tensor<128x1xi32>) -> tensor<128xi32> loc(#loc2006)
    %14 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1925)
    %15 = stablehlo.select %12, %13, %14 : tensor<128xi1>, tensor<128xi32> loc(#loc1930)
    return %15 : tensor<128xi32> loc(#loc2178)
  } loc(#loc2178)
  func.func private @remainder(%arg0: tensor<128xi32> loc(unknown), %arg1: tensor<i32> loc(unknown)) -> tensor<128xi32> {
    %c = stablehlo.constant dense<1> : tensor<i32> loc(#loc2179)
    %c_0 = stablehlo.constant dense<0> : tensor<i32> loc(#loc)
    %0 = stablehlo.convert %arg1 : tensor<i32> loc(#loc2013)
    %1 = stablehlo.compare  EQ, %0, %c_0,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc2014)
    %2 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<i32> loc(#loc2015)
    %3 = call @_where_158(%1, %2, %0) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32> loc(#loc2016)
    %4 = stablehlo.broadcast_in_dim %3, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2017)
    %5 = stablehlo.remainder %arg0, %4 : tensor<128xi32> loc(#loc2017)
    %6 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2018)
    %7 = stablehlo.compare  NE, %5, %6,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc2018)
    %8 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2019)
    %9 = stablehlo.compare  LT, %5, %8,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc2019)
    %10 = stablehlo.compare  LT, %3, %c_0,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc2019)
    %11 = stablehlo.broadcast_in_dim %10, dims = [] : (tensor<i1>) -> tensor<128xi1> loc(#loc2018)
    %12 = stablehlo.compare  NE, %9, %11,  UNSIGNED : (tensor<128xi1>, tensor<128xi1>) -> tensor<128xi1> loc(#loc2018)
    %13 = stablehlo.and %12, %7 : tensor<128xi1> loc(#loc2020)
    %14 = stablehlo.broadcast_in_dim %3, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2021)
    %15 = stablehlo.add %5, %14 : tensor<128xi32> loc(#loc2021)
    %16 = stablehlo.select %13, %15, %5 : tensor<128xi1>, tensor<128xi32> loc(#loc2022)
    return %16 : tensor<128xi32> loc(#loc2179)
  } loc(#loc2179)
  func.func private @_where_158(%arg0: tensor<i1> loc(unknown), %arg1: tensor<i32> loc(unknown), %arg2: tensor<i32> loc(unknown)) -> tensor<i32> {
    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<i1>, tensor<i32> loc(#loc2022)
    return %0 : tensor<i32> loc(#loc2179)
  } loc(#loc2179)
  func.func private @_ptp(%arg0: tensor<2xf32> loc(unknown)) -> tensor<f32> {
    %cst = stablehlo.constant dense<0x7F800000> : tensor<f32> loc(#loc2024)
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc2025)
    %0 = stablehlo.reduce(%arg0 init: %cst_0) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32> loc(#loc2025)
    %1 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.minimum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32> loc(#loc2024)
    %2 = stablehlo.subtract %0, %1 : tensor<f32> loc(#loc1948)
    return %2 : tensor<f32> loc(#loc2180)
  } loc(#loc2180)
  func.func private @_where_171(%arg0: tensor<i1> loc(unknown), %arg1: tensor<f32> loc(unknown), %arg2: tensor<f32> loc(unknown)) -> tensor<f32> {
    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<i1>, tensor<f32> loc(#loc1955)
    return %0 : tensor<f32> loc(#loc2180)
  } loc(#loc2180)
  func.func private @_linspace(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<2xf32> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.iota dim = 0 : tensor<1xf32> loc(#loc2026)
    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1xf32> loc(#loc2027)
    %2 = stablehlo.divide %0, %1 : tensor<1xf32> loc(#loc2027)
    %3 = stablehlo.reshape %arg0 : (tensor<f32>) -> tensor<1xf32> loc(#loc2028)
    %4 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1xf32> loc(#loc1948)
    %5 = stablehlo.subtract %4, %2 : tensor<1xf32> loc(#loc1948)
    %6 = stablehlo.multiply %3, %5 : tensor<1xf32> loc(#loc2029)
    %7 = stablehlo.reshape %arg1 : (tensor<f32>) -> tensor<1xf32> loc(#loc2028)
    %8 = stablehlo.multiply %7, %2 : tensor<1xf32> loc(#loc2029)
    %9 = stablehlo.add %6, %8 : tensor<1xf32> loc(#loc1950)
    %10 = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f32>) -> tensor<1xf32> loc(#loc1943)
    %11 = stablehlo.concatenate %9, %10, dim = 0 : (tensor<1xf32>, tensor<1xf32>) -> tensor<2xf32> loc(#loc2030)
    return %11 : tensor<2xf32> loc(#loc2180)
  } loc(#loc2180)
  func.func private @searchsorted(%arg0: tensor<2xf32> loc(unknown), %arg1: tensor<128xf32> loc(unknown)) -> tensor<128xi32> {
    %c = stablehlo.constant dense<1> : tensor<i32> loc(#loc)
    %c_0 = stablehlo.constant dense<2> : tensor<i32> loc(#loc)
    %c_1 = stablehlo.constant dense<0> : tensor<i32> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2031)
    %1 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2031)
    %2:5 = stablehlo.while(%iterArg = %arg0, %iterArg_2 = %arg1, %iterArg_3 = %c_1, %iterArg_4 = %0, %iterArg_5 = %1) : tensor<2xf32>, tensor<128xf32>, tensor<i32>, tensor<128xi32>, tensor<128xi32>
    cond {
      %3 = stablehlo.compare  LT, %iterArg_3, %c_0,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc2033)
      stablehlo.return %3 : tensor<i1> loc(#loc2032)
    } do {
      %3:2 = func.call @closed_call(%iterArg, %iterArg_2, %iterArg_4, %iterArg_5) : (tensor<2xf32>, tensor<128xf32>, tensor<128xi32>, tensor<128xi32>) -> (tensor<128xi32>, tensor<128xi32>) loc(#loc2034)
      %4 = stablehlo.add %iterArg_3, %c : tensor<i32> loc(#loc2035)
      stablehlo.return %iterArg, %iterArg_2, %4, %3#0, %3#1 : tensor<2xf32>, tensor<128xf32>, tensor<i32>, tensor<128xi32>, tensor<128xi32> loc(#loc2032)
    } loc(#loc2032)
    return %2#4 : tensor<128xi32> loc(#loc2180)
  } loc(#loc2180)
  func.func private @closed_call(%arg0: tensor<2xf32> loc(unknown), %arg1: tensor<128xf32> loc(unknown), %arg2: tensor<128xi32> loc(unknown), %arg3: tensor<128xi32> loc(unknown)) -> (tensor<128xi32>, tensor<128xi32>) {
    %cst = stablehlo.constant dense<0x7FC00000> : tensor<f32> loc(#loc)
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %c = stablehlo.constant dense<2> : tensor<i32> loc(#loc2181)
    %c_1 = stablehlo.constant dense<0> : tensor<i32> loc(#loc2181)
    %c_2 = stablehlo.constant dense<2> : tensor<ui32> loc(#loc2181)
    %0 = stablehlo.convert %arg2 : (tensor<128xi32>) -> tensor<128xui32> loc(#loc1942)
    %1 = stablehlo.convert %arg3 : (tensor<128xi32>) -> tensor<128xui32> loc(#loc1942)
    %2 = stablehlo.add %0, %1 : tensor<128xui32> loc(#loc1950)
    %3 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<ui32>) -> tensor<128xui32> loc(#loc2027)
    %4 = stablehlo.divide %2, %3 : tensor<128xui32> loc(#loc2027)
    %5 = stablehlo.convert %4 : (tensor<128xui32>) -> tensor<128xi32> loc(#loc1942)
    %6 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1954)
    %7 = stablehlo.compare  LT, %5, %6,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc1954)
    %8 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1950)
    %9 = stablehlo.add %5, %8 : tensor<128xi32> loc(#loc1950)
    %10 = stablehlo.select %7, %9, %5 : tensor<128xi1>, tensor<128xi32> loc(#loc1955)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0] : (tensor<128xi32>) -> tensor<128x1xi32> loc(#loc1943)
    %12 = "stablehlo.gather"(%arg0, %11) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>}> : (tensor<2xf32>, tensor<128x1xi32>) -> tensor<128x1xf32> loc(#loc2037)
    %13 = stablehlo.reshape %12 : (tensor<128x1xf32>) -> tensor<128xf32> loc(#loc1947)
    %14 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<f32> loc(#loc1943)
    %15 = stablehlo.broadcast_in_dim %14, dims = [] : (tensor<f32>) -> tensor<128xf32> loc(#loc1945)
    %16 = stablehlo.compare  EQ, %arg1, %15,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1> loc(#loc1945)
    %17 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<f32> loc(#loc1943)
    %18 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<128xf32> loc(#loc1943)
    %19 = stablehlo.select %16, %18, %arg1 : tensor<128xi1>, tensor<128xf32> loc(#loc1955)
    %20 = stablehlo.compare  NE, %arg1, %arg1,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1> loc(#loc2038)
    %21 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<f32> loc(#loc1943)
    %22 = stablehlo.broadcast_in_dim %21, dims = [] : (tensor<f32>) -> tensor<128xf32> loc(#loc1943)
    %23 = stablehlo.select %20, %22, %19 : tensor<128xi1>, tensor<128xf32> loc(#loc1955)
    %24 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<f32> loc(#loc1943)
    %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<f32>) -> tensor<128xf32> loc(#loc1945)
    %26 = stablehlo.compare  EQ, %13, %25,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1> loc(#loc1945)
    %27 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<f32> loc(#loc1943)
    %28 = stablehlo.broadcast_in_dim %27, dims = [] : (tensor<f32>) -> tensor<128xf32> loc(#loc1943)
    %29 = stablehlo.select %26, %28, %13 : tensor<128xi1>, tensor<128xf32> loc(#loc1955)
    %30 = stablehlo.compare  NE, %13, %13,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1> loc(#loc2038)
    %31 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<f32> loc(#loc1943)
    %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<f32>) -> tensor<128xf32> loc(#loc1943)
    %33 = stablehlo.select %30, %32, %29 : tensor<128xi1>, tensor<128xf32> loc(#loc1955)
    %34 = stablehlo.compare  LT, %23, %33,  TOTALORDER : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1> loc(#loc2039)
    %35 = call @_where_196(%34, %arg2, %5) : (tensor<128xi1>, tensor<128xi32>, tensor<128xi32>) -> tensor<128xi32> loc(#loc1949)
    %36 = call @_where_198(%34, %5, %arg3) : (tensor<128xi1>, tensor<128xi32>, tensor<128xi32>) -> tensor<128xi32> loc(#loc1949)
    return %35, %36 : tensor<128xi32>, tensor<128xi32> loc(#loc2181)
  } loc(#loc2181)
  func.func private @_where_196(%arg0: tensor<128xi1> loc(unknown), %arg1: tensor<128xi32> loc(unknown), %arg2: tensor<128xi32> loc(unknown)) -> tensor<128xi32> {
    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xi32> loc(#loc1955)
    return %0 : tensor<128xi32> loc(#loc2180)
  } loc(#loc2180)
  func.func private @_where_198(%arg0: tensor<128xi1> loc(unknown), %arg1: tensor<128xi32> loc(unknown), %arg2: tensor<128xi32> loc(unknown)) -> tensor<128xi32> {
    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xi32> loc(#loc1955)
    return %0 : tensor<128xi32> loc(#loc2180)
  } loc(#loc2180)
  func.func private @_roll_static_208(%arg0: tensor<1xi32> loc(unknown)) -> tensor<1xi32> {
    %0 = stablehlo.slice %arg0 [0:1] : (tensor<1xi32>) -> tensor<1xi32> loc(#loc2041)
    %1 = stablehlo.slice %arg0 [0:0] : (tensor<1xi32>) -> tensor<0xi32> loc(#loc2041)
    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1xi32>, tensor<0xi32>) -> tensor<1xi32> loc(#loc2042)
    return %2 : tensor<1xi32> loc(#loc2182)
  } loc(#loc2182)
  func.func private @cumsum_213(%arg0: tensor<1xi32> loc(unknown)) -> tensor<1xi32> {
    %0 = call @cumsum_214(%arg0) : (tensor<1xi32>) -> tensor<1xi32> loc(#loc1609)
    return %0 : tensor<1xi32> loc(#loc2182)
  } loc(#loc2182)
  func.func private @cumsum_214(%arg0: tensor<1xi32> loc(callsite(#loc918 at #loc1427))) -> tensor<1xi32> {
    %c = stablehlo.constant dense<0> : tensor<i32> loc(#loc2043)
    %0 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<i32> loc(#loc2043)
    %1 = "stablehlo.reduce_window"(%arg0, %0) <{window_dimensions = array<i64: 1>}> ({
    ^bb0(%arg1: tensor<i32> loc("reduce_window_sum"), %arg2: tensor<i32> loc("reduce_window_sum")):
      %2 = stablehlo.add %arg1, %arg2 : tensor<i32> loc(#loc2043)
      stablehlo.return %2 : tensor<i32> loc(#loc2043)
    }) : (tensor<1xi32>, tensor<i32>) -> tensor<1xi32> loc(#loc2043)
    return %1 : tensor<1xi32> loc(#loc1609)
  } loc(#loc1609)
  func.func private @_take_221(%arg0: tensor<1xi32> loc(unknown), %arg1: tensor<128xi32> loc(unknown)) -> tensor<128xi32> {
    %c = stablehlo.constant dense<-2147483648> : tensor<i32> loc(#loc2044)
    %c_0 = stablehlo.constant dense<true> : tensor<i1> loc(#loc2045)
    %c_1 = stablehlo.constant dense<0> : tensor<1xi32> loc(#loc2044)
    %c_2 = stablehlo.constant dense<1> : tensor<i32> loc(#loc2182)
    %c_3 = stablehlo.constant dense<0> : tensor<i32> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %c_3, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1964)
    %1 = stablehlo.compare  LT, %arg1, %0,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc1964)
    %2 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1965)
    %3 = stablehlo.add %arg1, %2 : tensor<128xi32> loc(#loc1965)
    %4 = call @_where_129(%1, %3, %arg1) : (tensor<128xi1>, tensor<128xi32>, tensor<128xi32>) -> tensor<128xi32> loc(#loc2046)
    %5 = stablehlo.broadcast_in_dim %4, dims = [0] : (tensor<128xi32>) -> tensor<128x1xi32> loc(#loc1961)
    %6 = stablehlo.broadcast_in_dim %c_3, dims = [] : (tensor<i32>) -> tensor<128x1xi32> loc(#loc2047)
    %7 = stablehlo.compare  GE, %5, %6,  SIGNED : (tensor<128x1xi32>, tensor<128x1xi32>) -> tensor<128x1xi1> loc(#loc2047)
    %8 = stablehlo.broadcast_in_dim %c_1, dims = [1] : (tensor<1xi32>) -> tensor<1x1xi32> loc(#loc1961)
    %9 = stablehlo.broadcast_in_dim %8, dims = [0, 1] : (tensor<1x1xi32>) -> tensor<128x1xi32> loc(#loc2048)
    %10 = stablehlo.compare  LE, %5, %9,  SIGNED : (tensor<128x1xi32>, tensor<128x1xi32>) -> tensor<128x1xi1> loc(#loc2048)
    %11 = stablehlo.and %7, %10 : tensor<128x1xi1> loc(#loc2049)
    %12 = stablehlo.reduce(%11 init: %c_0) applies stablehlo.and across dimensions = [1] : (tensor<128x1xi1>, tensor<i1>) -> tensor<128xi1> loc(#loc2045)
    %13 = "stablehlo.gather"(%arg0, %5) <{dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>}> : (tensor<1xi32>, tensor<128x1xi32>) -> tensor<128xi32> loc(#loc2044)
    %14 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc1961)
    %15 = stablehlo.select %12, %13, %14 : tensor<128xi1>, tensor<128xi32> loc(#loc1966)
    return %15 : tensor<128xi32> loc(#loc2182)
  } loc(#loc2182)
  func.func private @_roll_dynamic(%arg0: tensor<128xi32> loc(unknown), %arg1: tensor<i32> loc(unknown)) -> tensor<128xi32> {
    %c = stablehlo.constant dense<256> : tensor<i32> loc(#loc2183)
    %c_0 = stablehlo.constant dense<0> : tensor<i32> loc(#loc2183)
    %c_1 = stablehlo.constant dense<1> : tensor<i32> loc(#loc2183)
    %c_2 = stablehlo.constant dense<128> : tensor<i32> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<i32>) -> tensor<1xi32> loc(#loc2051)
    %1 = stablehlo.slice %0 [0:1] : (tensor<1xi32>) -> tensor<1xi32> loc(#loc2052)
    %2 = stablehlo.reshape %1 : (tensor<1xi32>) -> tensor<i32> loc(#loc2053)
    %3 = stablehlo.maximum %c_2, %c_1 : tensor<i32> loc(#loc2054)
    %4 = call @remainder_230(%2, %3) : (tensor<i32>, tensor<i32>) -> tensor<i32> loc(#loc2055)
    %5 = stablehlo.concatenate %arg0, %arg0, dim = 0 : (tensor<128xi32>, tensor<128xi32>) -> tensor<256xi32> loc(#loc2056)
    %6 = stablehlo.subtract %c_2, %4 : tensor<i32> loc(#loc2057)
    %7 = stablehlo.compare  LT, %6, %c_0,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc2058)
    %8 = stablehlo.add %6, %c : tensor<i32> loc(#loc2059)
    %9 = stablehlo.select %7, %8, %6 : tensor<i1>, tensor<i32> loc(#loc2060)
    %10 = stablehlo.dynamic_slice %5, %9, sizes = [128] : (tensor<256xi32>, tensor<i32>) -> tensor<128xi32> loc(#loc2061)
    return %10 : tensor<128xi32> loc(#loc2183)
  } loc(#loc2183)
  func.func private @remainder_230(%arg0: tensor<i32> loc(unknown), %arg1: tensor<i32> loc(unknown)) -> tensor<i32> {
    %c = stablehlo.constant dense<1> : tensor<i32> loc(#loc2183)
    %c_0 = stablehlo.constant dense<0> : tensor<i32> loc(#loc)
    %0 = stablehlo.compare  EQ, %arg1, %c_0,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc2062)
    %1 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<i32> loc(#loc2051)
    %2 = call @_where_158(%0, %1, %arg1) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32> loc(#loc2063)
    %3 = stablehlo.remainder %arg0, %2 : tensor<i32> loc(#loc2064)
    %4 = stablehlo.compare  NE, %3, %c_0,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc2065)
    %5 = stablehlo.compare  LT, %3, %c_0,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc2058)
    %6 = stablehlo.compare  LT, %2, %c_0,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc2058)
    %7 = stablehlo.compare  NE, %5, %6,  UNSIGNED : (tensor<i1>, tensor<i1>) -> tensor<i1> loc(#loc2065)
    %8 = stablehlo.and %7, %4 : tensor<i1> loc(#loc2066)
    %9 = stablehlo.add %3, %2 : tensor<i32> loc(#loc2059)
    %10 = stablehlo.select %8, %9, %3 : tensor<i1>, tensor<i32> loc(#loc2060)
    return %10 : tensor<i32> loc(#loc2183)
  } loc(#loc2183)
  func.func private @_where_242(%arg0: tensor<128xi1> loc(unknown), %arg1: tensor<128xi32> loc(unknown), %arg2: tensor<i32> loc(unknown)) -> tensor<128xi32> {
    %0 = stablehlo.convert %arg2 : tensor<i32> loc(#loc2068)
    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2069)
    %2 = stablehlo.select %arg0, %arg1, %1 : tensor<128xi1>, tensor<128xi32> loc(#loc2070)
    return %2 : tensor<128xi32> loc(#loc2184)
  } loc(#loc2184)
  func.func private @silu(%arg0: tensor<128x768xbf16> loc(unknown)) -> tensor<128x768xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.negate %arg0 : tensor<128x768xbf16> loc(#loc2072)
    %1 = stablehlo.exponential %0 : tensor<128x768xbf16> loc(#loc2073)
    %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x768xbf16> loc(#loc2074)
    %3 = stablehlo.add %2, %1 : tensor<128x768xbf16> loc(#loc2074)
    %4 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<128x768xbf16> loc(#loc2075)
    %5 = stablehlo.divide %4, %3 : tensor<128x768xbf16> loc(#loc2075)
    %6 = stablehlo.multiply %arg0, %5 : tensor<128x768xbf16> loc(#loc2076)
    return %6 : tensor<128x768xbf16> loc(#loc2185)
  } loc(#loc2185)
  func.func private @gmm_254(%arg0: tensor<128x192xbf16> loc(unknown), %arg1: tensor<128x2048x192xbf16> loc(unknown), %arg2: tensor<128xi32> loc(unknown), %arg3: tensor<i32> loc(unknown)) -> tensor<128x2048xbf16> {
    %c = stablehlo.constant dense<2> : tensor<i32> loc(#loc2186)
    %cst = stablehlo.constant dense<5.000000e-01> : tensor<f32> loc(#loc)
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<f32> loc(#loc)
    %c_2 = stablehlo.constant dense<0> : tensor<i32> loc(#loc)
    %c_3 = stablehlo.constant dense<1> : tensor<i32> loc(#loc)
    %c_4 = stablehlo.constant dense<128> : tensor<i32> loc(#loc)
    %cst_5 = stablehlo.constant dense<0.000000e+00> : tensor<2xf32> loc(#loc2186)
    %0 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<i32>) -> tensor<1xi32> loc(#loc2078)
    %1 = stablehlo.slice %0 [0:1] : (tensor<1xi32>) -> tensor<1xi32> loc(#loc2079)
    %2 = stablehlo.reshape %1 : (tensor<1xi32>) -> tensor<i32> loc(#loc2080)
    %3 = stablehlo.add %2, %c_4 : tensor<i32> loc(#loc2081)
    %4 = stablehlo.subtract %3, %c_3 : tensor<i32> loc(#loc2082)
    %5 = call @cumsum(%arg2) : (tensor<128xi32>) -> tensor<128xi32> loc(#loc2083)
    %6 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<1xi32> loc(#loc2084)
    %7 = stablehlo.concatenate %6, %5, dim = 0 : (tensor<1xi32>, tensor<128xi32>) -> tensor<129xi32> loc(#loc2085)
    %8 = stablehlo.broadcast_in_dim %c_4, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2086)
    %9 = stablehlo.add %5, %8 : tensor<128xi32> loc(#loc2086)
    %10 = stablehlo.broadcast_in_dim %c_3, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2087)
    %11 = stablehlo.subtract %9, %10 : tensor<128xi32> loc(#loc2087)
    %12 = call @floor_divide(%11, %c_4) : (tensor<128xi32>, tensor<i32>) -> tensor<128xi32> loc(#loc2088)
    %13 = stablehlo.broadcast_in_dim %c_4, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2089)
    %14 = stablehlo.multiply %12, %13 : tensor<128xi32> loc(#loc2089)
    %15 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<1xi32> loc(#loc2090)
    %16 = stablehlo.slice %5 [0:127] : (tensor<128xi32>) -> tensor<127xi32> loc(#loc2091)
    %17 = stablehlo.concatenate %15, %16, dim = 0 : (tensor<1xi32>, tensor<127xi32>) -> tensor<128xi32> loc(#loc2092)
    %18 = call @floor_divide(%17, %c_4) : (tensor<128xi32>, tensor<i32>) -> tensor<128xi32> loc(#loc2093)
    %19 = stablehlo.broadcast_in_dim %c_4, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2094)
    %20 = stablehlo.multiply %18, %19 : tensor<128xi32> loc(#loc2094)
    %21 = stablehlo.subtract %14, %20 : tensor<128xi32> loc(#loc2095)
    %22 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2096)
    %23 = stablehlo.compare  EQ, %arg2, %22,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc2096)
    %24 = call @_where_135(%23, %c_2, %21) : (tensor<128xi1>, tensor<i32>, tensor<128xi32>) -> tensor<128xi32> loc(#loc2097)
    %25 = call @floor_divide(%24, %c_4) : (tensor<128xi32>, tensor<i32>) -> tensor<128xi32> loc(#loc2098)
    %26 = stablehlo.iota dim = 0 : tensor<128xi32> loc(#loc2099)
    %27 = call @_roll_static(%25) : (tensor<128xi32>) -> tensor<128xi32> loc(#loc2100)
    %28 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<1xi32> loc(#loc2101)
    %29 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<i32> loc(#loc2101)
    %30 = "stablehlo.scatter"(%27, %28, %29) <{indices_are_sorted = true, scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0], scatter_dims_to_operand_dims = [0]>, unique_indices = true}> ({
    ^bb0(%arg4: tensor<i32> loc("scatter"), %arg5: tensor<i32> loc("scatter")):
      stablehlo.return %arg5 : tensor<i32> loc(#loc2102)
    }) : (tensor<128xi32>, tensor<1xi32>, tensor<i32>) -> tensor<128xi32> loc(#loc2102)
    %31 = call @cumsum(%30) : (tensor<128xi32>) -> tensor<128xi32> loc(#loc2103)
    %32 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2101)
    %33 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2104)
    %34 = stablehlo.compare  LT, %31, %33,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc2104)
    %35 = stablehlo.broadcast_in_dim %c_4, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2105)
    %36 = stablehlo.add %31, %35 : tensor<128xi32> loc(#loc2105)
    %37 = stablehlo.select %34, %36, %31 : tensor<128xi1>, tensor<128xi32> loc(#loc2106)
    %38 = stablehlo.broadcast_in_dim %37, dims = [0] : (tensor<128xi32>) -> tensor<128x1xi32> loc(#loc2101)
    %39 = stablehlo.broadcast_in_dim %c_3, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2101)
    %40 = "stablehlo.scatter"(%32, %38, %39) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>}> ({
    ^bb0(%arg4: tensor<i32> loc("scatter-add"), %arg5: tensor<i32> loc("scatter-add")):
      %131 = stablehlo.add %arg4, %arg5 : tensor<i32> loc(#loc2105)
      stablehlo.return %131 : tensor<i32> loc(#loc2107)
    }) : (tensor<128xi32>, tensor<128x1xi32>, tensor<128xi32>) -> tensor<128xi32> loc(#loc2107)
    %41 = call @cumsum(%40) : (tensor<128xi32>) -> tensor<128xi32> loc(#loc2103)
    %42 = stablehlo.broadcast_in_dim %c_3, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2108)
    %43 = stablehlo.subtract %41, %42 : tensor<128xi32> loc(#loc2108)
    %44 = call @_take_147(%26, %43) : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi32> loc(#loc2109)
    %45 = stablehlo.slice %7 [0:128] : (tensor<129xi32>) -> tensor<128xi32> loc(#loc2110)
    %46 = call @remainder(%45, %c_4) : (tensor<128xi32>, tensor<i32>) -> tensor<128xi32> loc(#loc2111)
    %47 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2112)
    %48 = stablehlo.compare  EQ, %46, %47,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc2112)
    %49 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2113)
    %50 = stablehlo.compare  EQ, %arg2, %49,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc2113)
    %51 = stablehlo.or %48, %50 : tensor<128xi1> loc(#loc2114)
    %52 = stablehlo.slice %7 [0:128] : (tensor<129xi32>) -> tensor<128xi32> loc(#loc2115)
    %53 = call @floor_divide(%52, %c_4) : (tensor<128xi32>, tensor<i32>) -> tensor<128xi32> loc(#loc2116)
    %54 = call @_where_135(%51, %c_3, %53) : (tensor<128xi1>, tensor<i32>, tensor<128xi32>) -> tensor<128xi32> loc(#loc2117)
    %55 = stablehlo.convert %54 : (tensor<128xi32>) -> tensor<128xf32> loc(#loc2118)
    %56 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32> loc(#loc2119)
    %57 = call @_ptp(%cst_5) : (tensor<2xf32>) -> tensor<f32> loc(#loc2120)
    %58 = stablehlo.compare  EQ, %57, %cst_0,  FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1> loc(#loc2121)
    %59 = stablehlo.slice %cst_5 [0:1] : (tensor<2xf32>) -> tensor<1xf32> loc(#loc2122)
    %60 = stablehlo.reshape %59 : (tensor<1xf32>) -> tensor<f32> loc(#loc2123)
    %61 = stablehlo.subtract %60, %cst : tensor<f32> loc(#loc2124)
    %62 = stablehlo.slice %cst_5 [0:1] : (tensor<2xf32>) -> tensor<1xf32> loc(#loc2122)
    %63 = stablehlo.reshape %62 : (tensor<1xf32>) -> tensor<f32> loc(#loc2123)
    %64 = call @_where_171(%58, %61, %63) : (tensor<i1>, tensor<f32>, tensor<f32>) -> tensor<f32> loc(#loc2125)
    %65 = call @_ptp(%cst_5) : (tensor<2xf32>) -> tensor<f32> loc(#loc2120)
    %66 = stablehlo.compare  EQ, %65, %cst_0,  FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1> loc(#loc2121)
    %67 = stablehlo.slice %cst_5 [1:2] : (tensor<2xf32>) -> tensor<1xf32> loc(#loc2122)
    %68 = stablehlo.reshape %67 : (tensor<1xf32>) -> tensor<f32> loc(#loc2123)
    %69 = stablehlo.add %68, %cst : tensor<f32> loc(#loc2126)
    %70 = stablehlo.slice %cst_5 [1:2] : (tensor<2xf32>) -> tensor<1xf32> loc(#loc2122)
    %71 = stablehlo.reshape %70 : (tensor<1xf32>) -> tensor<f32> loc(#loc2123)
    %72 = call @_where_171(%66, %69, %71) : (tensor<i1>, tensor<f32>, tensor<f32>) -> tensor<f32> loc(#loc2125)
    %73 = call @_linspace(%64, %72) : (tensor<f32>, tensor<f32>) -> tensor<2xf32> loc(#loc2127)
    %74 = call @searchsorted(%73, %55) : (tensor<2xf32>, tensor<128xf32>) -> tensor<128xi32> loc(#loc2128)
    %75 = stablehlo.dynamic_slice %73, %c_3, sizes = [1] : (tensor<2xf32>, tensor<i32>) -> tensor<1xf32> loc(#loc2129)
    %76 = stablehlo.reshape %75 : (tensor<1xf32>) -> tensor<f32> loc(#loc2123)
    %77 = stablehlo.broadcast_in_dim %76, dims = [] : (tensor<f32>) -> tensor<128xf32> loc(#loc2121)
    %78 = stablehlo.compare  EQ, %55, %77,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1> loc(#loc2121)
    %79 = call @_where_135(%78, %c_3, %74) : (tensor<128xi1>, tensor<i32>, tensor<128xi32>) -> tensor<128xi32> loc(#loc2125)
    %80 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<2xf32> loc(#loc2119)
    %81 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2130)
    %82 = stablehlo.compare  LT, %79, %81,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc2130)
    %83 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2126)
    %84 = stablehlo.add %79, %83 : tensor<128xi32> loc(#loc2126)
    %85 = stablehlo.select %82, %84, %79 : tensor<128xi1>, tensor<128xi32> loc(#loc2131)
    %86 = stablehlo.broadcast_in_dim %85, dims = [0] : (tensor<128xi32>) -> tensor<128x1xi32> loc(#loc2119)
    %87 = "stablehlo.scatter"(%80, %86, %56) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>}> ({
    ^bb0(%arg4: tensor<f32> loc("scatter-add"), %arg5: tensor<f32> loc("scatter-add")):
      %131 = stablehlo.add %arg4, %arg5 : tensor<f32> loc(#loc2126)
      stablehlo.return %131 : tensor<f32> loc(#loc2132)
    }) : (tensor<2xf32>, tensor<128x1xi32>, tensor<128xf32>) -> tensor<2xf32> loc(#loc2132)
    %88 = stablehlo.slice %87 [1:2] : (tensor<2xf32>) -> tensor<1xf32> loc(#loc2122)
    %89 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1xf32> loc(#loc2133)
    %90 = stablehlo.add %88, %89 : tensor<1xf32> loc(#loc2133)
    %91 = stablehlo.iota dim = 0 : tensor<1xi32> loc(#loc2134)
    %92 = stablehlo.convert %90 : (tensor<1xf32>) -> tensor<1xi32> loc(#loc2135)
    %93 = call @_roll_static_208(%92) : (tensor<1xi32>) -> tensor<1xi32> loc(#loc2136)
    %94 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<1xi32> loc(#loc2137)
    %95 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<i32> loc(#loc2137)
    %96 = "stablehlo.scatter"(%93, %94, %95) <{indices_are_sorted = true, scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0], scatter_dims_to_operand_dims = [0]>, unique_indices = true}> ({
    ^bb0(%arg4: tensor<i32> loc("scatter"), %arg5: tensor<i32> loc("scatter")):
      stablehlo.return %arg5 : tensor<i32> loc(#loc2138)
    }) : (tensor<1xi32>, tensor<1xi32>, tensor<i32>) -> tensor<1xi32> loc(#loc2138)
    %97 = call @cumsum_213(%96) : (tensor<1xi32>) -> tensor<1xi32> loc(#loc2139)
    %98 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2137)
    %99 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i32>) -> tensor<1xi32> loc(#loc2140)
    %100 = stablehlo.compare  LT, %97, %99,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1> loc(#loc2140)
    %101 = stablehlo.broadcast_in_dim %c_4, dims = [] : (tensor<i32>) -> tensor<1xi32> loc(#loc2141)
    %102 = stablehlo.add %97, %101 : tensor<1xi32> loc(#loc2141)
    %103 = stablehlo.select %100, %102, %97 : tensor<1xi1>, tensor<1xi32> loc(#loc2142)
    %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32> loc(#loc2137)
    %105 = stablehlo.broadcast_in_dim %c_3, dims = [] : (tensor<i32>) -> tensor<1xi32> loc(#loc2137)
    %106 = "stablehlo.scatter"(%98, %104, %105) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>}> ({
    ^bb0(%arg4: tensor<i32> loc("scatter-add"), %arg5: tensor<i32> loc("scatter-add")):
      %131 = stablehlo.add %arg4, %arg5 : tensor<i32> loc(#loc2141)
      stablehlo.return %131 : tensor<i32> loc(#loc2143)
    }) : (tensor<128xi32>, tensor<1x1xi32>, tensor<1xi32>) -> tensor<128xi32> loc(#loc2143)
    %107 = call @cumsum(%106) : (tensor<128xi32>) -> tensor<128xi32> loc(#loc2139)
    %108 = stablehlo.broadcast_in_dim %c_3, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2144)
    %109 = stablehlo.subtract %107, %108 : tensor<128xi32> loc(#loc2144)
    %110 = call @_take_221(%91, %109) : (tensor<1xi32>, tensor<128xi32>) -> tensor<128xi32> loc(#loc2145)
    %111 = stablehlo.convert %2 : tensor<i32> loc(#loc2146)
    %112 = stablehlo.broadcast_in_dim %111, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2147)
    %113 = stablehlo.compare  LT, %44, %112,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc2147)
    %114 = stablehlo.convert %113 : (tensor<128xi1>) -> tensor<128xi32> loc(#loc2148)
    %115 = stablehlo.reduce(%114 init: %c_2) applies stablehlo.add across dimensions = [0] : (tensor<128xi32>, tensor<i32>) -> tensor<i32> loc(#loc2149)
    %116 = stablehlo.negate %115 : tensor<i32> loc(#loc2150)
    %117 = call @_roll_dynamic(%44, %116) : (tensor<128xi32>, tensor<i32>) -> tensor<128xi32> loc(#loc2151)
    %118 = stablehlo.negate %115 : tensor<i32> loc(#loc2152)
    %119 = call @_roll_dynamic(%110, %118) : (tensor<128xi32>, tensor<i32>) -> tensor<128xi32> loc(#loc2153)
    %120 = stablehlo.iota dim = 0 : tensor<128xi32> loc(#loc2154)
    %121 = stablehlo.convert %4 : tensor<i32> loc(#loc2155)
    %122 = stablehlo.broadcast_in_dim %121, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2156)
    %123 = stablehlo.compare  LE, %120, %122,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc2156)
    %124 = stablehlo.convert %2 : tensor<i32> loc(#loc2157)
    %125 = stablehlo.broadcast_in_dim %124, dims = [] : (tensor<i32>) -> tensor<128xi32> loc(#loc2158)
    %126 = stablehlo.compare  GE, %120, %125,  SIGNED : (tensor<128xi32>, tensor<128xi32>) -> tensor<128xi1> loc(#loc2158)
    %127 = stablehlo.and %123, %126 : tensor<128xi1> loc(#loc2159)
    %128 = call @_where_242(%127, %25, %c_2) : (tensor<128xi1>, tensor<128xi32>, tensor<i32>) -> tensor<128xi32> loc(#loc2160)
    %129 = stablehlo.reduce(%128 init: %c_2) applies stablehlo.add across dimensions = [0] : (tensor<128xi32>, tensor<i32>) -> tensor<i32> loc(#loc2161)
    %130 = stablehlo.custom_call @tpu_custom_call(%129, %7, %117, %119, %0, %arg0, %arg1) {backend_config = "{\22custom_call_config\22: {\22body\22: \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\22, \22cost_estimate\22: {\22bytes_accessed\22: 101236736, \22flops\22: 100663296, \22transcendentals\22: 0}, \22serialization_format\22: 1, \22needs_layout_passes\22: true}}", kernel_name = "kernel", operand_layouts = [dense<> : tensor<0xindex>, dense<0> : tensor<1xindex>, dense<0> : tensor<1xindex>, dense<0> : tensor<1xindex>, dense<0> : tensor<1xindex>, dense<[1, 0]> : tensor<2xindex>, dense<[2, 1, 0]> : tensor<3xindex>], result_layouts = [dense<[1, 0]> : tensor<2xindex>]} : (tensor<i32>, tensor<129xi32>, tensor<128xi32>, tensor<128xi32>, tensor<1xi32>, tensor<128x192xbf16>, tensor<128x2048x192xbf16>) -> tensor<128x2048xbf16> loc(#loc2162)
    return %130 : tensor<128x2048xbf16> loc(#loc2186)
  } loc(#loc2186)
} loc(#loc)
#loc490 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/ops/jaten.py":552:9 to :31)
#loc491 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/tensor.py":584:14 to :38)
#loc492 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/tensor.py":267:13 to :57)
#loc493 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torch/nn/functional.py":2546:11 to :82)
#loc494 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/tensor.py":243:13 to :42)
#loc495 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torch/overrides.py":1725:21 to :77)
#loc496 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torch/nn/functional.py":2511:15 to 2521:9)
#loc497 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py":72:15 to :48)
#loc498 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py":475:26 to :80)
#loc500 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/tensor.py":154:10 to :51)
#loc501 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/tensor.py":474:14 to :70)
#loc502 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/tensor.py":521:11 to :51)
#loc503 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/tensor.py":533:13 to :48)
#loc504 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/tensor.py":235:15 to :59)
#loc505 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py":195:12 to :31)
#loc507 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py":1784:19 to :48)
#loc508 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py":1773:19 to :51)
#loc509 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/ops/jaten.py":487:8 to :23)
#loc510 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py":218:19 to :31)
#loc511 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/models/qwen3_moe.py":382:28 to :63)
#loc512 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/ops/jaten.py":265:9 to :43)
#loc513 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py":218:19 to :58)
#loc514 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/ops/jaten.py":65:8 to :21)
#loc515 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py":220:28 to :60)
#loc516 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/ops/jaten.py":656:9 to :25)
#loc517 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/ops/op_base.py":118:11 to :32)
#loc518 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py":220:16 to :61)
#loc519 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/ops/jaten.py":325:8 to :13)
#loc520 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py":220:12 to :61)
#loc521 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py":221:12 to :28)
#loc522 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py":223:16 to :31)
#loc523 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/quantization/unquantized.py":129:15 to :57)
#loc524 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/quantization/unquantized.py":113:22 to :55)
#loc525 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py":582:26 to :69)
#loc526 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/models/qwen3_moe.py":302:17 to :45)
#loc527 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/models/qwen3_moe.py":385:24 to 388:9)
#loc528 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/linear_common.py":97:21 to :54)
#loc529 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/quantization/unquantized.py":133:15 to 134:73)
#loc530 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/linear_common.py":107:23 to :67)
#loc531 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/linear_common.py":108:29 to :74)
#loc532 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/quantization/unquantized.py":135:14 to :44)
#loc533 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/ops/jaten.py":784:6 to :42)
#loc534 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/ops/jaten.py":783:9 to 786:3)
#loc535 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torch/_tensor.py":1052:19 to :68)
#loc536 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torch/_tensor.py":1040:19 to 1042:13)
#loc537 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/models/qwen3_moe.py":303:18 to :78)
#loc538 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/ops/jaten.py":55:9 to :30)
#loc539 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/models/qwen3_moe.py":305:20 to :86)
#loc540 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/models/qwen3_moe.py":306:20 to :42)
#loc541 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/models/qwen3_moe.py":307:12 to :35)
#loc542 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/models/qwen3_moe.py":309:20 to :86)
#loc543 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/models/qwen3_moe.py":310:20 to :42)
#loc544 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/models/qwen3_moe.py":311:12 to :35)
#loc545 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/ops/jtorch.py":291:31 to :50)
#loc546 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/tensor.py":589:16 to :40)
#loc547 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/mrope.py":284:18 to :47)
#loc548 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/models/qwen3_moe.py":312:15 to :47)
#loc549 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/mrope.py":285:19 to :43)
#loc550 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/mrope.py":302:16 to :58)
#loc551 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/ops/jaten.py":801:9 to :35)
#loc552 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py":41:10 to :27)
#loc553 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py":70:15 to :65)
#loc554 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/mrope.py":305:20 to :86)
#loc555 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py":42:10 to :27)
#loc556 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py":44:17 to :42)
#loc557 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py":48:9 to :17)
#loc558 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py":48:20 to :28)
#loc559 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/ops/jaten.py":299:13 to :22)
#loc560 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py":48:9 to :28)
#loc561 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/ops/jaten.py":299:9 to :22)
#loc562 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py":49:9 to :17)
#loc563 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py":49:20 to :28)
#loc564 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/ops/jaten.py":65:12 to :21)
#loc565 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py":49:9 to :28)
#loc566 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/ops/jaten.py":1211:11 to :50)
#loc567 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py":51:15 to :42)
#loc568 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/view.py":106:13 to :35)
#loc569 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/view.py":355:15 to :49)
#loc570 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/view.py":285:18 to :28)
#loc571 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/tensor.py":662:15 to :24)
#loc572 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torch/utils/_pytree.py":1498:23 to :30)
#loc573 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torch/utils/_pytree.py":1197:21 to :33)
#loc574 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torch/utils/_pytree.py":1380:11 to :52)
#loc575 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torch/utils/_pytree.py":1568:11 to :81)
#loc576 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/tensor.py":665:10 to :60)
#loc577 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/torchax/tensor.py":563:25 to :53)
#loc578 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/mrope.py":306:16 to :58)
#loc579 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/mrope.py":306:16 to :79)
#loc580 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/mrope.py":309:14 to :54)
#loc581 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/mrope.py":312:18 to :82)
#loc582 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/mrope.py":313:14 to :52)
#loc583 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/mrope.py":313:14 to :71)
#loc584 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/attention.py":118:32 to 122:64)
#loc585 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/attention/layer.py":381:23 to 383:17)
#loc586 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/models/qwen3_moe.py":313:22 to :40)
#loc587 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py":1422:26 to :78)
#loc588 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/models/qwen3_moe.py":314:20 to :44)
#loc589 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py":197:20 to :46)
#loc590 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py":197:16 to :46)
#loc591 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/models/qwen3_moe.py":391:34 to :88)
#loc592 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py":198:23 to :39)
#loc593 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py":416:17 to :55)
#loc594 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/models/qwen3_moe.py":196:27 to :51)
#loc595 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/models/qwen3_moe.py":392:24 to :47)
#loc599 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/models/qwen3_moe.py":197:30 to 199:9)
#loc600 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/models/qwen3_moe.py":384:38 to :83)
#loc601 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/vllm/model_executor/models/qwen3_omni_moe_thinker.py":635:27 to :61)
#loc602 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/attention.py":161:8 to :46)
#loc603 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/attention.py":163:8 to :49)
#loc604 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/attention.py":164:8 to :49)
#loc605 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/jax/attention_interface.py":343:23 to 354:9)
#loc606 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/attention.py":166:28 to 176:5)
#loc607 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/attention.py":182:14 to :51)
#loc609 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/jax/attention_interface.py":293:15 to 300:9)
#loc610 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py":920:12 to 925:13)
#loc611 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py":1305:12 to :35)
#loc612 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py":919:8 to 933:9)
#loc613 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py":933:10 to 939:9)
#loc614 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py":941:9 to :23)
#loc615 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py":885:8 to 886:27)
#loc616 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py":943:9 to :23)
#loc617 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py":886:28 to 887:52)
#loc618 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py":884:9 to 894:5)
#loc619 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py":894:6 to 899:5)
#loc620 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py":1338:12 to :27)
#loc621 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py":1393:8 to :35)
#loc622 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py":1395:8 to :38)
#loc623 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py":1397:8 to :38)
#loc624 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py":1442:31 to :74)
#loc625 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py":960:12 to :30)
#loc626 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py":1444:8 to 1445:40)
#loc627 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py":960:12 to 965:5)
#loc628 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py":965:63 to 966:60)
#loc629 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":309:34 to :67)
#loc631 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":309:19 to :77)
#loc632 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":310:33 to :68)
#loc633 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":312:38 to :78)
#loc634 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":312:23 to :78)
#loc635 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":313:19 to :45)
#loc636 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":315:24 to :46)
#loc637 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":316:27 to :57)
#loc638 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":317:34 to :67)
#loc639 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":318:20 to :59)
#loc640 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":318:20 to :72)
#loc641 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":319:27 to :62)
#loc642 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":320:18 to :76)
#loc644 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":322:8 to :43)
#loc647 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":107:11 to 108:59)
#loc648 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":342:8 to :23)
#loc649 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":342:8 to :28)
#loc650 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":353:12 to 354:53)
#loc651 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":135:11 to 141:28)
#loc652 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":355:12 to 359:54)
#loc653 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":132:12 to :39)
#loc654 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":133:15 to :49)
#loc656 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":361:8 to :38)
#loc657 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":361:8 to :69)
#loc658 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":362:12 to :50)
#loc659 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":362:8 to :50)
#loc660 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":363:8 to :22)
#loc661 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/tpu_inference/layers/vllm/fused_moe.py":367:12 to :73)
#loc663 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":358:19 to :37)
#loc664 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":388:18 to :33)
#loc665 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":116:14 to :46)
#loc667 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":116:14 to :50)
#loc669 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":127:35 to :64)
#loc670 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":127:18 to :78)
#loc671 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":140:25 to :40)
#loc672 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":140:25 to :44)
#loc673 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":140:24 to :51)
#loc674 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":140:24 to :56)
#loc675 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":144:7 to :36)
#loc676 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":144:38 to :53)
#loc677 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":143:17 to 145:3)
#loc678 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":146:25 to :43)
#loc679 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":146:25 to :48)
#loc680 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":153:24 to :65)
#loc681 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":154:34 to :50)
#loc682 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":154:24 to :75)
#loc683 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":174:16 to :41)
#loc684 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":188:6 to :45)
#loc685 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":187:14 to 191:3)
#loc687 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":213:7 to :25)
#loc688 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":213:7 to :30)
#loc689 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":213:6 to :36)
#loc690 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":213:38 to :54)
#loc691 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":212:22 to 214:3)
#loc692 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":222:34 to :52)
#loc693 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":222:34 to :58)
#loc694 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":221:21 to 223:3)
#loc695 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":226:6 to :75)
#loc696 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":226:6 to 227:9)
#loc697 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":233:6 to :42)
#loc698 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":234:6 to :35)
#loc700 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":244:25 to :48)
#loc701 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":244:24 to :55)
#loc702 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":245:40 to :60)
#loc703 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":245:14 to :69)
#loc704 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":246:42 to :62)
#loc705 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":246:15 to :71)
#loc706 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":251:9 to :48)
#loc707 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":252:38 to :55)
#loc708 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":252:57 to :76)
#loc709 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":252:22 to :77)
#loc710 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":253:16 to :60)
#loc711 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":254:14 to :31)
#loc712 = loc("/home/eporat/benchmarking_qwen_omni_tpu/.venv/lib/python3.11/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py":547:8 to 553:3)
#loc714 = loc("_aten_embedding"(#loc490))
#loc715 = loc("Environment.dispatch"(#loc491))
#loc716 = loc("XLADispatchMode.__torch_dispatch__"(#loc492))
#loc717 = loc("embedding"(#loc493))
#loc718 = loc("XLAFunctionMode.__torch_function__"(#loc494))
#loc719 = loc("handle_torch_function"(#loc495))
#loc720 = loc("embedding"(#loc496))
#loc721 = loc("UnquantizedEmbeddingMethod.embedding"(#loc497))
#loc722 = loc("VocabParallelEmbedding.forward_native"(#loc498))
#loc724 = loc("Tensor.apply_jax"(#loc500))
#loc725 = loc("Environment._to_copy"(#loc501))
#loc726 = loc("Environment._torch_Tensor_to"(#loc502))
#loc727 = loc("Environment.dispatch"(#loc503))
#loc728 = loc("XLAFunctionMode.__torch_function__"(#loc504))
#loc729 = loc("RMSNorm.forward_native"(#loc505))
#loc731 = loc("Module._call_impl"(#loc507))
#loc732 = loc("Module._wrapped_call_impl"(#loc508))
#loc733 = loc("_aten_pow"(#loc509))
#loc734 = loc("RMSNorm.forward_native"(#loc510))
#loc735 = loc("Qwen3MoeDecoderLayer.forward"(#loc511))
#loc736 = loc("_aten_mean"(#loc512))
#loc737 = loc("RMSNorm.forward_native"(#loc513))
#loc738 = loc("_aten_add"(#loc514))
#loc739 = loc("RMSNorm.forward_native"(#loc515))
#loc740 = loc("_aten_rsqrt"(#loc516))
#loc741 = loc("promote_int_input.<locals>.wrapper"(#loc517))
#loc742 = loc("RMSNorm.forward_native"(#loc518))
#loc743 = loc("_aten_mul"(#loc519))
#loc744 = loc("RMSNorm.forward_native"(#loc520))
#loc745 = loc("RMSNorm.forward_native"(#loc521))
#loc746 = loc("RMSNorm.forward_native"(#loc522))
#loc747 = loc("VllmUnquantizedLinearMethod._apply_fused"(#loc523))
#loc748 = loc("VllmUnquantizedLinearMethod.apply"(#loc524))
#loc749 = loc("ColumnParallelLinear.forward"(#loc525))
#loc750 = loc("Qwen3MoeAttention.forward"(#loc526))
#loc751 = loc("Qwen3MoeDecoderLayer.forward"(#loc527))
#loc752 = loc("slice_sharded_tensor_for_concatenation"(#loc528))
#loc753 = loc("VllmUnquantizedLinearMethod._apply_fused"(#loc529))
#loc754 = loc("slice_sharded_tensor_for_concatenation"(#loc530))
#loc755 = loc("slice_sharded_tensor_for_concatenation"(#loc531))
#loc756 = loc("VllmUnquantizedLinearMethod._apply_fused"(#loc532))
#loc757 = loc("split_with_sizes.<locals>.<listcomp>"(#loc533))
#loc758 = loc("split_with_sizes"(#loc534))
#loc759 = loc("Tensor.split"(#loc535))
#loc760 = loc("Tensor.split"(#loc536))
#loc761 = loc("Qwen3MoeAttention.forward"(#loc537))
#loc762 = loc("_aten_unsafe_view"(#loc538))
#loc763 = loc("Qwen3MoeAttention.forward"(#loc539))
#loc764 = loc("Qwen3MoeAttention.forward"(#loc540))
#loc765 = loc("Qwen3MoeAttention.forward"(#loc541))
#loc766 = loc("Qwen3MoeAttention.forward"(#loc542))
#loc767 = loc("Qwen3MoeAttention.forward"(#loc543))
#loc768 = loc("Qwen3MoeAttention.forward"(#loc544))
#loc769 = loc("getitem"(#loc545))
#loc770 = loc("Environment.dispatch"(#loc546))
#loc771 = loc("MRotaryEmbedding.forward_native"(#loc547))
#loc772 = loc("Qwen3MoeAttention.forward"(#loc548))
#loc773 = loc("MRotaryEmbedding.forward_native"(#loc549))
#loc774 = loc("MRotaryEmbedding.forward_native"(#loc550))
#loc775 = loc("_aten_unsqueeze"(#loc551))
#loc776 = loc("apply_rotary_emb_torch"(#loc552))
#loc777 = loc("apply_rotary_emb_dispatch"(#loc553))
#loc778 = loc("MRotaryEmbedding.forward_native"(#loc554))
#loc779 = loc("apply_rotary_emb_torch"(#loc555))
#loc780 = loc("apply_rotary_emb_torch"(#loc556))
#loc781 = loc("apply_rotary_emb_torch"(#loc557))
#loc782 = loc("apply_rotary_emb_torch"(#loc558))
#loc783 = loc("_aten_sub"(#loc559))
#loc784 = loc("apply_rotary_emb_torch"(#loc560))
#loc785 = loc("_aten_sub"(#loc561))
#loc786 = loc("apply_rotary_emb_torch"(#loc562))
#loc787 = loc("apply_rotary_emb_torch"(#loc563))
#loc788 = loc("_aten_add"(#loc564))
#loc789 = loc("apply_rotary_emb_torch"(#loc565))
#loc790 = loc("_aten_cat"(#loc566))
#loc791 = loc("apply_rotary_emb_torch"(#loc567))
#loc792 = loc("NarrowInfo.transform_tensor"(#loc568))
#loc793 = loc("View.jax"(#loc569))
#loc794 = loc("View.torch"(#loc570))
#loc795 = loc("Environment.v2t_iso.<locals>.to_tensor"(#loc571))
#loc796 = loc("map_only.<locals>.wrapper.<locals>.wrapped"(#loc572))
#loc797 = loc("TreeSpec.unflatten"(#loc573))
#loc798 = loc("tree_map"(#loc574))
#loc799 = loc("tree_map_only"(#loc575))
#loc800 = loc("Environment.v2t_iso"(#loc576))
#loc801 = loc("Environment.dispatch"(#loc577))
#loc802 = loc("MRotaryEmbedding.forward_native"(#loc578))
#loc803 = loc("MRotaryEmbedding.forward_native"(#loc579))
#loc804 = loc("MRotaryEmbedding.forward_native"(#loc580))
#loc805 = loc("MRotaryEmbedding.forward_native"(#loc581))
#loc806 = loc("MRotaryEmbedding.forward_native"(#loc582))
#loc807 = loc("MRotaryEmbedding.forward_native"(#loc583))
#loc808 = loc("PallasAttentionBackendImpl.forward"(#loc584))
#loc809 = loc("Attention.forward"(#loc585))
#loc810 = loc("Qwen3MoeAttention.forward"(#loc586))
#loc811 = loc("RowParallelLinear.forward"(#loc587))
#loc812 = loc("Qwen3MoeAttention.forward"(#loc588))
#loc813 = loc("RMSNorm.forward_native"(#loc589))
#loc814 = loc("RMSNorm.forward_native"(#loc590))
#loc815 = loc("Qwen3MoeDecoderLayer.forward"(#loc591))
#loc816 = loc("RMSNorm.forward_native"(#loc592))
#loc817 = loc("ReplicatedLinear.forward"(#loc593))
#loc818 = loc("Qwen3MoeSparseMoeBlock.forward"(#loc594))
#loc819 = loc("Qwen3MoeDecoderLayer.forward"(#loc595))
#loc823 = loc("Qwen3MoeSparseMoeBlock.forward"(#loc599))
#loc824 = loc("Qwen3MoeDecoderLayer.forward"(#loc600))
#loc825 = loc("Qwen3MoeLLMModel.forward"(#loc601))
#loc826 = loc("_jax_attn_func"(#loc602))
#loc827 = loc("_jax_attn_func"(#loc603))
#loc828 = loc("_jax_attn_func"(#loc604))
#loc829 = loc("attention"(#loc605))
#loc830 = loc("_jax_attn_func"(#loc606))
#loc831 = loc("_jax_attn_func"(#loc607))
#loc832 = loc("sharded_ragged_paged_attention.<locals>._ragged_paged_attention"(#loc609))
#loc833 = loc("prepare_inputs"(#loc610))
#loc834 = loc("ragged_paged_attention"(#loc611))
#loc835 = loc("prepare_inputs"(#loc612))
#loc836 = loc("prepare_inputs"(#loc613))
#loc837 = loc("prepare_inputs"(#loc614))
#loc838 = loc("merge_kv"(#loc615))
#loc839 = loc("prepare_inputs"(#loc616))
#loc840 = loc("merge_kv"(#loc617))
#loc841 = loc("merge_kv"(#loc618))
#loc842 = loc("merge_kv"(#loc619))
#loc843 = loc("ragged_paged_attention"(#loc620))
#loc844 = loc("ragged_paged_attention"(#loc621))
#loc845 = loc("ragged_paged_attention"(#loc622))
#loc846 = loc("ragged_paged_attention"(#loc623))
#loc847 = loc("ragged_paged_attention"(#loc624))
#loc848 = loc("prepare_outputs"(#loc625))
#loc849 = loc("ragged_paged_attention"(#loc626))
#loc850 = loc("prepare_outputs"(#loc627))
#loc851 = loc("prepare_outputs"(#loc628))
#loc852 = loc("jax_fused_moe_func"(#loc629))
#loc854 = loc("jax_fused_moe_func"(#loc631))
#loc855 = loc("jax_fused_moe_func"(#loc632))
#loc856 = loc("jax_fused_moe_func"(#loc633))
#loc857 = loc("jax_fused_moe_func"(#loc634))
#loc858 = loc("jax_fused_moe_func"(#loc635))
#loc859 = loc("jax_fused_moe_func"(#loc636))
#loc860 = loc("jax_fused_moe_func"(#loc637))
#loc861 = loc("jax_fused_moe_func"(#loc638))
#loc862 = loc("jax_fused_moe_func"(#loc639))
#loc863 = loc("jax_fused_moe_func"(#loc640))
#loc864 = loc("jax_fused_moe_func"(#loc641))
#loc865 = loc("jax_fused_moe_func"(#loc642))
#loc866 = loc("jax_fused_moe_func"(#loc644))
#loc869 = loc("tensor_sharded_gmm_merged_column_parallel"(#loc647))
#loc870 = loc("jax_fused_moe_func"(#loc648))
#loc871 = loc("jax_fused_moe_func"(#loc649))
#loc872 = loc("jax_fused_moe_func"(#loc650))
#loc873 = loc("tensor_sharded_gmm_row_parallel"(#loc651))
#loc874 = loc("jax_fused_moe_func"(#loc652))
#loc875 = loc("tensor_sharded_gmm_row_parallel.<locals>._gmm_all_reduce"(#loc653))
#loc876 = loc("tensor_sharded_gmm_row_parallel.<locals>._gmm_all_reduce"(#loc654))
#loc877 = loc("jax_fused_moe_func"(#loc656))
#loc878 = loc("jax_fused_moe_func"(#loc657))
#loc879 = loc("jax_fused_moe_func"(#loc658))
#loc880 = loc("jax_fused_moe_func"(#loc659))
#loc881 = loc("jax_fused_moe_func"(#loc660))
#loc882 = loc("jax_fused_moe_func"(#loc661))
#loc883 = loc("gmm"(#loc663))
#loc884 = loc("gmm"(#loc664))
#loc885 = loc("make_group_metadata"(#loc665))
#loc887 = loc("make_group_metadata"(#loc667))
#loc889 = loc("make_group_metadata"(#loc669))
#loc890 = loc("make_group_metadata"(#loc670))
#loc891 = loc("make_group_metadata"(#loc671))
#loc892 = loc("make_group_metadata"(#loc672))
#loc893 = loc("make_group_metadata"(#loc673))
#loc894 = loc("make_group_metadata"(#loc674))
#loc895 = loc("make_group_metadata"(#loc675))
#loc896 = loc("make_group_metadata"(#loc676))
#loc897 = loc("make_group_metadata"(#loc677))
#loc898 = loc("make_group_metadata"(#loc678))
#loc899 = loc("make_group_metadata"(#loc679))
#loc900 = loc("make_group_metadata"(#loc680))
#loc901 = loc("make_group_metadata"(#loc681))
#loc902 = loc("make_group_metadata"(#loc682))
#loc903 = loc("make_group_metadata"(#loc683))
#loc904 = loc("make_group_metadata"(#loc684))
#loc905 = loc("make_group_metadata"(#loc685))
#loc906 = loc("make_group_metadata"(#loc687))
#loc907 = loc("make_group_metadata"(#loc688))
#loc908 = loc("make_group_metadata"(#loc689))
#loc909 = loc("make_group_metadata"(#loc690))
#loc910 = loc("make_group_metadata"(#loc691))
#loc911 = loc("make_group_metadata"(#loc692))
#loc912 = loc("make_group_metadata"(#loc693))
#loc913 = loc("make_group_metadata"(#loc694))
#loc914 = loc("make_group_metadata"(#loc695))
#loc915 = loc("make_group_metadata"(#loc696))
#loc916 = loc("make_group_metadata"(#loc697))
#loc917 = loc("make_group_metadata"(#loc698))
#loc919 = loc("make_group_metadata"(#loc700))
#loc920 = loc("make_group_metadata"(#loc701))
#loc921 = loc("make_group_metadata"(#loc702))
#loc922 = loc("make_group_metadata"(#loc703))
#loc923 = loc("make_group_metadata"(#loc704))
#loc924 = loc("make_group_metadata"(#loc705))
#loc925 = loc("make_group_metadata"(#loc706))
#loc926 = loc("make_group_metadata"(#loc707))
#loc927 = loc("make_group_metadata"(#loc708))
#loc928 = loc("make_group_metadata"(#loc709))
#loc929 = loc("make_group_metadata"(#loc710))
#loc930 = loc("make_group_metadata"(#loc711))
#loc931 = loc("gmm"(#loc712))
#loc932 = loc(callsite(#loc722 at #loc723))
#loc933 = loc(callsite(#loc731 at #loc732))
#loc934 = loc(callsite(#loc732 at #loc735))
#loc935 = loc(callsite(#loc751 at #loc731))
#loc936 = loc(callsite(#loc732 at #loc751))
#loc937 = loc(callsite(#loc761 at #loc731))
#loc938 = loc(callsite(#loc732 at #loc764))
#loc939 = loc(callsite(#loc732 at #loc767))
#loc940 = loc(callsite(#loc772 at #loc731))
#loc941 = loc(callsite(#loc732 at #loc772))
#loc942 = loc(callsite(#loc730 at #loc731))
#loc944 = loc(callsite(#loc800 at #loc801))
#loc945 = loc(callsite(#loc732 at #loc815))
#loc946 = loc(callsite(#loc819 at #loc731))
#loc947 = loc(callsite(#loc732 at #loc819))
#loc948 = loc(callsite(#loc732 at #loc824))
#loc949 = loc(callsite(#loc732 at #loc825))
#loc950 = loc(callsite(#loc732 at #loc810))
#loc951 = loc(callsite(#loc810 at #loc731))
#loc952 = loc(callsite(#loc732 at #loc823))
#loc953 = loc(callsite(#loc822 at #loc723))
#loc954 = loc(callsite(#loc721 at #loc932))
#loc955 = loc(callsite(#loc730 at #loc933))
#loc956 = loc(callsite(#loc731 at #loc934))
#loc957 = loc(callsite(#loc732 at #loc935))
#loc958 = loc(callsite(#loc731 at #loc936))
#loc959 = loc(callsite(#loc760 at #loc937))
#loc960 = loc(callsite(#loc751 at #loc933))
#loc961 = loc(callsite(#loc731 at #loc938))
#loc962 = loc(callsite(#loc731 at #loc939))
#loc963 = loc(callsite(#loc732 at #loc940))
#loc964 = loc(callsite(#loc731 at #loc941))
#loc965 = loc(callsite(#loc723 at #loc942))
#loc966 = loc(callsite(#loc778 at #loc943))
#loc967 = loc(callsite(#loc799 at #loc944))
#loc968 = loc(callsite(#loc805 at #loc943))
#loc969 = loc(callsite(#loc731 at #loc945))
#loc970 = loc(callsite(#loc732 at #loc946))
#loc971 = loc(callsite(#loc731 at #loc947))
#loc972 = loc(callsite(#loc823 at #loc933))
#loc973 = loc(callsite(#loc731 at #loc948))
#loc974 = loc(callsite(#loc731 at #loc949))
#loc975 = loc(callsite(#loc810 at #loc933))
#loc976 = loc(callsite(#loc731 at #loc950))
#loc977 = loc(callsite(#loc809 at #loc933))
#loc978 = loc(callsite(#loc732 at #loc951))
#loc979 = loc(callsite(#loc731 at #loc952))
#loc981 = loc(callsite(#loc821 at #loc953))
#loc982 = loc(callsite(#loc720 at #loc954))
#loc983 = loc(callsite(#loc723 at #loc955))
#loc984 = loc(callsite(#loc730 at #loc956))
#loc985 = loc(callsite(#loc731 at #loc957))
#loc986 = loc(callsite(#loc750 at #loc958))
#loc987 = loc(callsite(#loc719 at #loc959))
#loc988 = loc(callsite(#loc732 at #loc960))
#loc989 = loc(callsite(#loc730 at #loc961))
#loc990 = loc(callsite(#loc730 at #loc962))
#loc991 = loc(callsite(#loc731 at #loc963))
#loc992 = loc(callsite(#loc730 at #loc964))
#loc993 = loc(callsite(#loc778 at #loc965))
#loc994 = loc(callsite(#loc777 at #loc966))
#loc995 = loc(callsite(#loc798 at #loc967))
#loc996 = loc(callsite(#loc805 at #loc965))
#loc997 = loc(callsite(#loc777 at #loc968))
#loc998 = loc(callsite(#loc812 at #loc958))
#loc999 = loc(callsite(#loc730 at #loc969))
#loc1000 = loc(callsite(#loc731 at #loc970))
#loc1001 = loc(callsite(#loc818 at #loc971))
#loc1002 = loc(callsite(#loc732 at #loc972))
#loc1003 = loc(callsite(#loc730 at #loc973))
#loc1004 = loc(callsite(#loc730 at #loc974))
#loc1005 = loc(callsite(#loc810 at #loc958))
#loc1006 = loc(callsite(#loc732 at #loc975))
#loc1007 = loc(callsite(#loc809 at #loc976))
#loc1008 = loc(callsite(#loc808 at #loc977))
#loc1009 = loc(callsite(#loc731 at #loc978))
#loc1010 = loc(callsite(#loc730 at #loc979))
#loc1011 = loc(callsite(#loc822 at #loc965))
#loc1013 = loc(callsite(#loc820 at #loc981))
#loc1014 = loc(callsite(#loc719 at #loc982))
#loc1015 = loc(callsite(#loc729 at #loc983))
#loc1016 = loc(callsite(#loc723 at #loc984))
#loc1017 = loc(callsite(#loc742 at #loc983))
#loc1018 = loc(callsite(#loc745 at #loc983))
#loc1019 = loc(callsite(#loc750 at #loc985))
#loc1020 = loc(callsite(#loc732 at #loc986))
#loc1021 = loc(callsite(#loc718 at #loc987))
#loc1022 = loc(callsite(#loc731 at #loc988))
#loc1023 = loc(callsite(#loc723 at #loc989))
#loc1024 = loc(callsite(#loc723 at #loc990))
#loc1025 = loc(callsite(#loc730 at #loc991))
#loc1026 = loc(callsite(#loc773 at #loc983))
#loc1027 = loc(callsite(#loc723 at #loc992))
#loc1028 = loc(callsite(#loc777 at #loc993))
#loc1029 = loc(callsite(#loc780 at #loc994))
#loc1030 = loc(callsite(#loc797 at #loc995))
#loc1031 = loc(callsite(#loc777 at #loc996))
#loc1032 = loc(callsite(#loc780 at #loc997))
#loc1033 = loc(callsite(#loc812 at #loc985))
#loc1034 = loc(callsite(#loc732 at #loc998))
#loc1035 = loc(callsite(#loc813 at #loc983))
#loc1036 = loc(callsite(#loc723 at #loc999))
#loc1037 = loc(callsite(#loc816 at #loc983))
#loc1038 = loc(callsite(#loc818 at #loc1000))
#loc1039 = loc(callsite(#loc732 at #loc1001))
#loc1040 = loc(callsite(#loc731 at #loc1002))
#loc1041 = loc(callsite(#loc723 at #loc1003))
#loc1042 = loc(callsite(#loc723 at #loc1004))
#loc1043 = loc(callsite(#loc810 at #loc985))
#loc1044 = loc(callsite(#loc732 at #loc1005))
#loc1045 = loc(callsite(#loc731 at #loc1006))
#loc1046 = loc(callsite(#loc808 at #loc1007))
#loc1047 = loc(callsite(#loc830 at #loc1008))
#loc1048 = loc(callsite(#loc809 at #loc1009))
#loc1049 = loc(callsite(#loc723 at #loc1010))
#loc1050 = loc(callsite(#loc822 at #loc983))
#loc1051 = loc(callsite(#loc821 at #loc1011))
#loc1053 = loc(callsite(#loc853 at #loc1013))
#loc1054 = loc(callsite(#loc718 at #loc1014))
#loc1055 = loc(callsite(#loc728 at #loc1015))
#loc1056 = loc(callsite(#loc734 at #loc1016))
#loc1057 = loc(callsite(#loc737 at #loc1016))
#loc1058 = loc(callsite(#loc739 at #loc1016))
#loc1059 = loc(callsite(#loc718 at #loc1017))
#loc1060 = loc(callsite(#loc744 at #loc1016))
#loc1061 = loc(callsite(#loc728 at #loc1018))
#loc1062 = loc(callsite(#loc746 at #loc1016))
#loc1063 = loc(callsite(#loc732 at #loc1019))
#loc1064 = loc(callsite(#loc731 at #loc1020))
#loc1065 = loc(callsite(#loc759 at #loc1021))
#loc1066 = loc(callsite(#loc763 at #loc1022))
#loc1067 = loc(callsite(#loc734 at #loc1023))
#loc1068 = loc(callsite(#loc737 at #loc1023))
#loc1069 = loc(callsite(#loc739 at #loc1023))
#loc1070 = loc(callsite(#loc744 at #loc1023))
#loc1071 = loc(callsite(#loc746 at #loc1023))
#loc1072 = loc(callsite(#loc765 at #loc1022))
#loc1073 = loc(callsite(#loc766 at #loc1022))
#loc1074 = loc(callsite(#loc734 at #loc1024))
#loc1075 = loc(callsite(#loc737 at #loc1024))
#loc1076 = loc(callsite(#loc739 at #loc1024))
#loc1077 = loc(callsite(#loc744 at #loc1024))
#loc1078 = loc(callsite(#loc746 at #loc1024))
#loc1079 = loc(callsite(#loc768 at #loc1022))
#loc1080 = loc(callsite(#loc723 at #loc1025))
#loc1081 = loc(callsite(#loc718 at #loc1026))
#loc1082 = loc(callsite(#loc774 at #loc1027))
#loc1083 = loc(callsite(#loc776 at #loc1028))
#loc1084 = loc(callsite(#loc779 at #loc1028))
#loc1085 = loc(callsite(#loc718 at #loc1029))
#loc1086 = loc(callsite(#loc781 at #loc1028))
#loc1087 = loc(callsite(#loc782 at #loc1028))
#loc1088 = loc(callsite(#loc784 at #loc1028))
#loc1089 = loc(callsite(#loc786 at #loc1028))
#loc1090 = loc(callsite(#loc787 at #loc1028))
#loc1091 = loc(callsite(#loc789 at #loc1028))
#loc1092 = loc(callsite(#loc791 at #loc1028))
#loc1093 = loc(callsite(#loc796 at #loc1030))
#loc1094 = loc(callsite(#loc802 at #loc1027))
#loc1095 = loc(callsite(#loc803 at #loc1027))
#loc1096 = loc(callsite(#loc804 at #loc1027))
#loc1097 = loc(callsite(#loc776 at #loc1031))
#loc1098 = loc(callsite(#loc779 at #loc1031))
#loc1099 = loc(callsite(#loc718 at #loc1032))
#loc1100 = loc(callsite(#loc781 at #loc1031))
#loc1101 = loc(callsite(#loc782 at #loc1031))
#loc1102 = loc(callsite(#loc784 at #loc1031))
#loc1103 = loc(callsite(#loc786 at #loc1031))
#loc1104 = loc(callsite(#loc787 at #loc1031))
#loc1105 = loc(callsite(#loc789 at #loc1031))
#loc1106 = loc(callsite(#loc791 at #loc1031))
#loc1107 = loc(callsite(#loc806 at #loc1027))
#loc1108 = loc(callsite(#loc807 at #loc1027))
#loc1109 = loc(callsite(#loc810 at #loc1022))
#loc1110 = loc(callsite(#loc732 at #loc1033))
#loc1111 = loc(callsite(#loc731 at #loc1034))
#loc1112 = loc(callsite(#loc728 at #loc1035))
#loc1113 = loc(callsite(#loc814 at #loc1036))
#loc1114 = loc(callsite(#loc728 at #loc1037))
#loc1115 = loc(callsite(#loc734 at #loc1036))
#loc1116 = loc(callsite(#loc737 at #loc1036))
#loc1117 = loc(callsite(#loc739 at #loc1036))
#loc1118 = loc(callsite(#loc744 at #loc1036))
#loc1119 = loc(callsite(#loc746 at #loc1036))
#loc1120 = loc(callsite(#loc732 at #loc1038))
#loc1121 = loc(callsite(#loc731 at #loc1039))
#loc1122 = loc(callsite(#loc730 at #loc1040))
#loc1123 = loc(callsite(#loc814 at #loc1041))
#loc1124 = loc(callsite(#loc734 at #loc1041))
#loc1125 = loc(callsite(#loc737 at #loc1041))
#loc1126 = loc(callsite(#loc739 at #loc1041))
#loc1127 = loc(callsite(#loc744 at #loc1041))
#loc1128 = loc(callsite(#loc746 at #loc1041))
#loc1129 = loc(callsite(#loc814 at #loc1042))
#loc1130 = loc(callsite(#loc734 at #loc1042))
#loc1131 = loc(callsite(#loc737 at #loc1042))
#loc1132 = loc(callsite(#loc739 at #loc1042))
#loc1133 = loc(callsite(#loc744 at #loc1042))
#loc1134 = loc(callsite(#loc746 at #loc1042))
#loc1135 = loc(callsite(#loc732 at #loc1043))
#loc1136 = loc(callsite(#loc731 at #loc1044))
#loc1137 = loc(callsite(#loc809 at #loc1045))
#loc1138 = loc(callsite(#loc830 at #loc1046))
#loc1139 = loc(callsite(#loc829 at #loc1047))
#loc1140 = loc(callsite(#loc808 at #loc1048))
#loc1141 = loc(callsite(#loc822 at #loc1049))
#loc1142 = loc(callsite(#loc821 at #loc1050))
#loc1143 = loc(callsite(#loc820 at #loc1051))
#loc1145 = loc(callsite(#loc874 at #loc1053))
#loc1146 = loc(callsite(#loc717 at #loc1054))
#loc1147 = loc(callsite(#loc727 at #loc1055))
#loc1148 = loc(callsite(#loc718 at #loc1056))
#loc1149 = loc(callsite(#loc718 at #loc1057))
#loc1150 = loc(callsite(#loc718 at #loc1058))
#loc1151 = loc(callsite(#loc716 at #loc1059))
#loc1152 = loc(callsite(#loc718 at #loc1060))
#loc1153 = loc(callsite(#loc727 at #loc1061))
#loc1154 = loc(callsite(#loc718 at #loc1062))
#loc1155 = loc(callsite(#loc731 at #loc1063))
#loc1156 = loc(callsite(#loc749 at #loc1064))
#loc1157 = loc(callsite(#loc716 at #loc1065))
#loc1158 = loc(callsite(#loc718 at #loc1066))
#loc1159 = loc(callsite(#loc718 at #loc1067))
#loc1160 = loc(callsite(#loc718 at #loc1068))
#loc1161 = loc(callsite(#loc718 at #loc1069))
#loc1162 = loc(callsite(#loc718 at #loc1070))
#loc1163 = loc(callsite(#loc718 at #loc1071))
#loc1164 = loc(callsite(#loc718 at #loc1072))
#loc1165 = loc(callsite(#loc718 at #loc1073))
#loc1166 = loc(callsite(#loc718 at #loc1074))
#loc1167 = loc(callsite(#loc718 at #loc1075))
#loc1168 = loc(callsite(#loc718 at #loc1076))
#loc1169 = loc(callsite(#loc718 at #loc1077))
#loc1170 = loc(callsite(#loc718 at #loc1078))
#loc1171 = loc(callsite(#loc718 at #loc1079))
#loc1172 = loc(callsite(#loc771 at #loc1080))
#loc1173 = loc(callsite(#loc716 at #loc1081))
#loc1174 = loc(callsite(#loc718 at #loc1082))
#loc1175 = loc(callsite(#loc718 at #loc1083))
#loc1176 = loc(callsite(#loc718 at #loc1084))
#loc1177 = loc(callsite(#loc716 at #loc1085))
#loc1178 = loc(callsite(#loc718 at #loc1086))
#loc1179 = loc(callsite(#loc718 at #loc1087))
#loc1180 = loc(callsite(#loc718 at #loc1088))
#loc1181 = loc(callsite(#loc718 at #loc1089))
#loc1182 = loc(callsite(#loc718 at #loc1090))
#loc1183 = loc(callsite(#loc718 at #loc1091))
#loc1184 = loc(callsite(#loc718 at #loc1092))
#loc1185 = loc(callsite(#loc795 at #loc1093))
#loc1186 = loc(callsite(#loc718 at #loc1094))
#loc1187 = loc(callsite(#loc718 at #loc1095))
#loc1188 = loc(callsite(#loc718 at #loc1096))
#loc1189 = loc(callsite(#loc718 at #loc1097))
#loc1190 = loc(callsite(#loc718 at #loc1098))
#loc1191 = loc(callsite(#loc716 at #loc1099))
#loc1192 = loc(callsite(#loc718 at #loc1100))
#loc1193 = loc(callsite(#loc718 at #loc1101))
#loc1194 = loc(callsite(#loc718 at #loc1102))
#loc1195 = loc(callsite(#loc718 at #loc1103))
#loc1196 = loc(callsite(#loc718 at #loc1104))
#loc1197 = loc(callsite(#loc718 at #loc1105))
#loc1198 = loc(callsite(#loc718 at #loc1106))
#loc1199 = loc(callsite(#loc718 at #loc1107))
#loc1200 = loc(callsite(#loc718 at #loc1108))
#loc1201 = loc(callsite(#loc732 at #loc1109))
#loc1202 = loc(callsite(#loc731 at #loc1110))
#loc1203 = loc(callsite(#loc811 at #loc1111))
#loc1204 = loc(callsite(#loc727 at #loc1112))
#loc1205 = loc(callsite(#loc718 at #loc1113))
#loc1206 = loc(callsite(#loc727 at #loc1114))
#loc1207 = loc(callsite(#loc718 at #loc1115))
#loc1208 = loc(callsite(#loc718 at #loc1116))
#loc1209 = loc(callsite(#loc718 at #loc1117))
#loc1210 = loc(callsite(#loc718 at #loc1118))
#loc1211 = loc(callsite(#loc718 at #loc1119))
#loc1212 = loc(callsite(#loc731 at #loc1120))
#loc1213 = loc(callsite(#loc817 at #loc1121))
#loc1214 = loc(callsite(#loc723 at #loc1122))
#loc1215 = loc(callsite(#loc718 at #loc1123))
#loc1216 = loc(callsite(#loc718 at #loc1124))
#loc1217 = loc(callsite(#loc718 at #loc1125))
#loc1218 = loc(callsite(#loc718 at #loc1126))
#loc1219 = loc(callsite(#loc718 at #loc1127))
#loc1220 = loc(callsite(#loc718 at #loc1128))
#loc1221 = loc(callsite(#loc718 at #loc1129))
#loc1222 = loc(callsite(#loc718 at #loc1130))
#loc1223 = loc(callsite(#loc718 at #loc1131))
#loc1224 = loc(callsite(#loc718 at #loc1132))
#loc1225 = loc(callsite(#loc718 at #loc1133))
#loc1226 = loc(callsite(#loc718 at #loc1134))
#loc1227 = loc(callsite(#loc731 at #loc1135))
#loc1228 = loc(callsite(#loc809 at #loc1136))
#loc1229 = loc(callsite(#loc808 at #loc1137))
#loc1230 = loc(callsite(#loc829 at #loc1138))
#loc1231 = loc(callsite(#loc832 at #loc1139))
#loc1232 = loc(callsite(#loc830 at #loc1140))
#loc1233 = loc(callsite(#loc821 at #loc1141))
#loc1234 = loc(callsite(#loc820 at #loc1142))
#loc1235 = loc(callsite(#loc853 at #loc1143))
#loc1237 = loc(callsite(#loc874 at #loc1144))
#loc1238 = loc(callsite(#loc873 at #loc1145))
#loc1239 = loc(callsite(#loc716 at #loc1146))
#loc1240 = loc(callsite(#loc726 at #loc1147))
#loc1241 = loc(callsite(#loc716 at #loc1148))
#loc1242 = loc(callsite(#loc716 at #loc1149))
#loc1243 = loc(callsite(#loc716 at #loc1150))
#loc1244 = loc(callsite(#loc715 at #loc1151))
#loc1245 = loc(callsite(#loc716 at #loc1152))
#loc1246 = loc(callsite(#loc726 at #loc1153))
#loc1247 = loc(callsite(#loc716 at #loc1154))
#loc1248 = loc(callsite(#loc749 at #loc1155))
#loc1249 = loc(callsite(#loc748 at #loc1156))
#loc1250 = loc(callsite(#loc715 at #loc1157))
#loc1251 = loc(callsite(#loc716 at #loc1158))
#loc1252 = loc(callsite(#loc716 at #loc1159))
#loc1253 = loc(callsite(#loc716 at #loc1160))
#loc1254 = loc(callsite(#loc716 at #loc1161))
#loc1255 = loc(callsite(#loc716 at #loc1162))
#loc1256 = loc(callsite(#loc716 at #loc1163))
#loc1257 = loc(callsite(#loc716 at #loc1164))
#loc1258 = loc(callsite(#loc716 at #loc1165))
#loc1259 = loc(callsite(#loc716 at #loc1166))
#loc1260 = loc(callsite(#loc716 at #loc1167))
#loc1261 = loc(callsite(#loc716 at #loc1168))
#loc1262 = loc(callsite(#loc716 at #loc1169))
#loc1263 = loc(callsite(#loc716 at #loc1170))
#loc1264 = loc(callsite(#loc716 at #loc1171))
#loc1265 = loc(callsite(#loc728 at #loc1172))
#loc1266 = loc(callsite(#loc715 at #loc1173))
#loc1267 = loc(callsite(#loc716 at #loc1174))
#loc1268 = loc(callsite(#loc716 at #loc1175))
#loc1269 = loc(callsite(#loc716 at #loc1176))
#loc1270 = loc(callsite(#loc715 at #loc1177))
#loc1271 = loc(callsite(#loc716 at #loc1178))
#loc1272 = loc(callsite(#loc716 at #loc1179))
#loc1273 = loc(callsite(#loc716 at #loc1180))
#loc1274 = loc(callsite(#loc716 at #loc1181))
#loc1275 = loc(callsite(#loc716 at #loc1182))
#loc1276 = loc(callsite(#loc716 at #loc1183))
#loc1277 = loc(callsite(#loc716 at #loc1184))
#loc1278 = loc(callsite(#loc794 at #loc1185))
#loc1279 = loc(callsite(#loc716 at #loc1186))
#loc1280 = loc(callsite(#loc716 at #loc1187))
#loc1281 = loc(callsite(#loc716 at #loc1188))
#loc1282 = loc(callsite(#loc716 at #loc1189))
#loc1283 = loc(callsite(#loc716 at #loc1190))
#loc1284 = loc(callsite(#loc715 at #loc1191))
#loc1285 = loc(callsite(#loc716 at #loc1192))
#loc1286 = loc(callsite(#loc716 at #loc1193))
#loc1287 = loc(callsite(#loc716 at #loc1194))
#loc1288 = loc(callsite(#loc716 at #loc1195))
#loc1289 = loc(callsite(#loc716 at #loc1196))
#loc1290 = loc(callsite(#loc716 at #loc1197))
#loc1291 = loc(callsite(#loc716 at #loc1198))
#loc1292 = loc(callsite(#loc716 at #loc1199))
#loc1293 = loc(callsite(#loc716 at #loc1200))
#loc1294 = loc(callsite(#loc731 at #loc1201))
#loc1295 = loc(callsite(#loc811 at #loc1202))
#loc1296 = loc(callsite(#loc748 at #loc1203))
#loc1297 = loc(callsite(#loc726 at #loc1204))
#loc1298 = loc(callsite(#loc716 at #loc1205))
#loc1299 = loc(callsite(#loc726 at #loc1206))
#loc1300 = loc(callsite(#loc716 at #loc1207))
#loc1301 = loc(callsite(#loc716 at #loc1208))
#loc1302 = loc(callsite(#loc716 at #loc1209))
#loc1303 = loc(callsite(#loc716 at #loc1210))
#loc1304 = loc(callsite(#loc716 at #loc1211))
#loc1305 = loc(callsite(#loc817 at #loc1212))
#loc1306 = loc(callsite(#loc748 at #loc1213))
#loc1307 = loc(callsite(#loc822 at #loc1214))
#loc1308 = loc(callsite(#loc716 at #loc1215))
#loc1309 = loc(callsite(#loc716 at #loc1216))
#loc1310 = loc(callsite(#loc716 at #loc1217))
#loc1311 = loc(callsite(#loc716 at #loc1218))
#loc1312 = loc(callsite(#loc716 at #loc1219))
#loc1313 = loc(callsite(#loc716 at #loc1220))
#loc1314 = loc(callsite(#loc716 at #loc1221))
#loc1315 = loc(callsite(#loc716 at #loc1222))
#loc1316 = loc(callsite(#loc716 at #loc1223))
#loc1317 = loc(callsite(#loc716 at #loc1224))
#loc1318 = loc(callsite(#loc716 at #loc1225))
#loc1319 = loc(callsite(#loc716 at #loc1226))
#loc1320 = loc(callsite(#loc809 at #loc1227))
#loc1321 = loc(callsite(#loc808 at #loc1228))
#loc1322 = loc(callsite(#loc830 at #loc1229))
#loc1323 = loc(callsite(#loc832 at #loc1230))
#loc1324 = loc(callsite(#loc834 at #loc1231))
#loc1325 = loc(callsite(#loc829 at #loc1232))
#loc1326 = loc(callsite(#loc820 at #loc1233))
#loc1327 = loc(callsite(#loc853 at #loc1234))
#loc1328 = loc(callsite(#loc868 at #loc1235))
#loc1329 = loc(callsite(#loc874 at #loc1235))
#loc1331 = loc(callsite(#loc873 at #loc1237))
#loc1332 = loc(callsite(#loc875 at #loc1238))
#loc1333 = loc(callsite(#loc715 at #loc1239))
#loc1334 = loc(callsite(#loc725 at #loc1240))
#loc1335 = loc(callsite(#loc715 at #loc1241))
#loc1336 = loc(callsite(#loc715 at #loc1242))
#loc1337 = loc(callsite(#loc715 at #loc1243))
#loc1338 = loc(callsite(#loc741 at #loc1244))
#loc1339 = loc(callsite(#loc715 at #loc1245))
#loc1340 = loc(callsite(#loc725 at #loc1246))
#loc1341 = loc(callsite(#loc715 at #loc1247))
#loc1342 = loc(callsite(#loc748 at #loc1248))
#loc1343 = loc(callsite(#loc753 at #loc1249))
#loc1344 = loc(callsite(#loc758 at #loc1250))
#loc1345 = loc(callsite(#loc715 at #loc1251))
#loc1346 = loc(callsite(#loc715 at #loc1252))
#loc1347 = loc(callsite(#loc715 at #loc1253))
#loc1348 = loc(callsite(#loc715 at #loc1254))
#loc1349 = loc(callsite(#loc715 at #loc1255))
#loc1350 = loc(callsite(#loc715 at #loc1256))
#loc1351 = loc(callsite(#loc715 at #loc1257))
#loc1352 = loc(callsite(#loc715 at #loc1258))
#loc1353 = loc(callsite(#loc715 at #loc1259))
#loc1354 = loc(callsite(#loc715 at #loc1260))
#loc1355 = loc(callsite(#loc715 at #loc1261))
#loc1356 = loc(callsite(#loc715 at #loc1262))
#loc1357 = loc(callsite(#loc715 at #loc1263))
#loc1358 = loc(callsite(#loc715 at #loc1264))
#loc1359 = loc(callsite(#loc770 at #loc1265))
#loc1360 = loc(callsite(#loc758 at #loc1266))
#loc1361 = loc(callsite(#loc715 at #loc1267))
#loc1362 = loc(callsite(#loc715 at #loc1268))
#loc1363 = loc(callsite(#loc715 at #loc1269))
#loc1364 = loc(callsite(#loc758 at #loc1270))
#loc1365 = loc(callsite(#loc715 at #loc1271))
#loc1366 = loc(callsite(#loc715 at #loc1272))
#loc1367 = loc(callsite(#loc715 at #loc1273))
#loc1368 = loc(callsite(#loc715 at #loc1274))
#loc1369 = loc(callsite(#loc715 at #loc1275))
#loc1370 = loc(callsite(#loc715 at #loc1276))
#loc1371 = loc(callsite(#loc715 at #loc1277))
#loc1372 = loc(callsite(#loc793 at #loc1278))
#loc1373 = loc(callsite(#loc715 at #loc1279))
#loc1374 = loc(callsite(#loc715 at #loc1280))
#loc1375 = loc(callsite(#loc715 at #loc1281))
#loc1376 = loc(callsite(#loc715 at #loc1282))
#loc1377 = loc(callsite(#loc715 at #loc1283))
#loc1378 = loc(callsite(#loc758 at #loc1284))
#loc1379 = loc(callsite(#loc715 at #loc1285))
#loc1380 = loc(callsite(#loc715 at #loc1286))
#loc1381 = loc(callsite(#loc715 at #loc1287))
#loc1382 = loc(callsite(#loc715 at #loc1288))
#loc1383 = loc(callsite(#loc715 at #loc1289))
#loc1384 = loc(callsite(#loc715 at #loc1290))
#loc1385 = loc(callsite(#loc715 at #loc1291))
#loc1386 = loc(callsite(#loc715 at #loc1292))
#loc1387 = loc(callsite(#loc715 at #loc1293))
#loc1388 = loc(callsite(#loc809 at #loc1294))
#loc1389 = loc(callsite(#loc748 at #loc1295))
#loc1390 = loc(callsite(#loc753 at #loc1296))
#loc1391 = loc(callsite(#loc725 at #loc1297))
#loc1392 = loc(callsite(#loc715 at #loc1298))
#loc1393 = loc(callsite(#loc725 at #loc1299))
#loc1394 = loc(callsite(#loc715 at #loc1300))
#loc1395 = loc(callsite(#loc715 at #loc1301))
#loc1396 = loc(callsite(#loc715 at #loc1302))
#loc1397 = loc(callsite(#loc715 at #loc1303))
#loc1398 = loc(callsite(#loc715 at #loc1304))
#loc1399 = loc(callsite(#loc748 at #loc1305))
#loc1400 = loc(callsite(#loc753 at #loc1306))
#loc1401 = loc(callsite(#loc821 at #loc1307))
#loc1402 = loc(callsite(#loc715 at #loc1308))
#loc1403 = loc(callsite(#loc715 at #loc1309))
#loc1404 = loc(callsite(#loc715 at #loc1310))
#loc1405 = loc(callsite(#loc715 at #loc1311))
#loc1406 = loc(callsite(#loc715 at #loc1312))
#loc1407 = loc(callsite(#loc715 at #loc1313))
#loc1408 = loc(callsite(#loc715 at #loc1314))
#loc1409 = loc(callsite(#loc715 at #loc1315))
#loc1410 = loc(callsite(#loc715 at #loc1316))
#loc1411 = loc(callsite(#loc715 at #loc1317))
#loc1412 = loc(callsite(#loc715 at #loc1318))
#loc1413 = loc(callsite(#loc715 at #loc1319))
#loc1414 = loc(callsite(#loc808 at #loc1320))
#loc1415 = loc(callsite(#loc830 at #loc1321))
#loc1416 = loc(callsite(#loc829 at #loc1322))
#loc1417 = loc(callsite(#loc834 at #loc1323))
#loc1418 = loc(callsite(#loc839 at #loc1324))
#loc1419 = loc(callsite(#loc832 at #loc1325))
#loc1420 = loc(callsite(#loc849 at #loc1323))
#loc1421 = loc(callsite(#loc853 at #loc1326))
#loc1422 = loc(callsite(#loc868 at #loc1327))
#loc1423 = loc(callsite(#loc869 at #loc1328))
#loc1424 = loc(callsite(#loc874 at #loc1327))
#loc1425 = loc(callsite(#loc873 at #loc1329))
#loc1426 = loc(callsite(#loc867 at #loc1328))
#loc1428 = loc(callsite(#loc875 at #loc1331))
#loc1429 = loc(callsite(#loc886 at #loc1332))
#loc1430 = loc(callsite(#loc714 at #loc1333))
#loc1431 = loc(callsite(#loc724 at #loc1334))
#loc1432 = loc(callsite(#loc733 at #loc1335))
#loc1433 = loc(callsite(#loc736 at #loc1336))
#loc1434 = loc(callsite(#loc738 at #loc1337))
#loc1435 = loc(callsite(#loc740 at #loc1338))
#loc1436 = loc(callsite(#loc743 at #loc1339))
#loc1437 = loc(callsite(#loc724 at #loc1340))
#loc1438 = loc(callsite(#loc743 at #loc1341))
#loc1439 = loc(callsite(#loc747 at #loc1342))
#loc1440 = loc(callsite(#loc752 at #loc1343))
#loc1441 = loc(callsite(#loc754 at #loc1343))
#loc1442 = loc(callsite(#loc755 at #loc1343))
#loc1443 = loc(callsite(#loc756 at #loc1342))
#loc1444 = loc(callsite(#loc757 at #loc1344))
#loc1445 = loc(callsite(#loc762 at #loc1345))
#loc1446 = loc(callsite(#loc733 at #loc1346))
#loc1447 = loc(callsite(#loc736 at #loc1347))
#loc1448 = loc(callsite(#loc738 at #loc1348))
#loc1449 = loc(callsite(#loc743 at #loc1349))
#loc1450 = loc(callsite(#loc743 at #loc1350))
#loc1451 = loc(callsite(#loc762 at #loc1351))
#loc1452 = loc(callsite(#loc762 at #loc1352))
#loc1453 = loc(callsite(#loc733 at #loc1353))
#loc1454 = loc(callsite(#loc736 at #loc1354))
#loc1455 = loc(callsite(#loc738 at #loc1355))
#loc1456 = loc(callsite(#loc743 at #loc1356))
#loc1457 = loc(callsite(#loc743 at #loc1357))
#loc1458 = loc(callsite(#loc762 at #loc1358))
#loc1459 = loc(callsite(#loc769 at #loc1359))
#loc1460 = loc(callsite(#loc757 at #loc1360))
#loc1461 = loc(callsite(#loc762 at #loc1361))
#loc1462 = loc(callsite(#loc775 at #loc1362))
#loc1463 = loc(callsite(#loc775 at #loc1363))
#loc1464 = loc(callsite(#loc757 at #loc1364))
#loc1465 = loc(callsite(#loc743 at #loc1365))
#loc1466 = loc(callsite(#loc743 at #loc1366))
#loc1467 = loc(callsite(#loc783 at #loc1367))
#loc1468 = loc(callsite(#loc785 at #loc1367))
#loc1469 = loc(callsite(#loc743 at #loc1368))
#loc1470 = loc(callsite(#loc743 at #loc1369))
#loc1471 = loc(callsite(#loc788 at #loc1370))
#loc1472 = loc(callsite(#loc738 at #loc1370))
#loc1473 = loc(callsite(#loc790 at #loc1371))
#loc1474 = loc(callsite(#loc792 at #loc1372))
#loc1475 = loc(callsite(#loc790 at #loc1373))
#loc1476 = loc(callsite(#loc762 at #loc1374))
#loc1477 = loc(callsite(#loc762 at #loc1375))
#loc1478 = loc(callsite(#loc775 at #loc1376))
#loc1479 = loc(callsite(#loc775 at #loc1377))
#loc1480 = loc(callsite(#loc757 at #loc1378))
#loc1481 = loc(callsite(#loc743 at #loc1379))
#loc1482 = loc(callsite(#loc743 at #loc1380))
#loc1483 = loc(callsite(#loc783 at #loc1381))
#loc1484 = loc(callsite(#loc785 at #loc1381))
#loc1485 = loc(callsite(#loc743 at #loc1382))
#loc1486 = loc(callsite(#loc743 at #loc1383))
#loc1487 = loc(callsite(#loc788 at #loc1384))
#loc1488 = loc(callsite(#loc738 at #loc1384))
#loc1489 = loc(callsite(#loc790 at #loc1385))
#loc1490 = loc(callsite(#loc790 at #loc1386))
#loc1491 = loc(callsite(#loc762 at #loc1387))
#loc1492 = loc(callsite(#loc808 at #loc1388))
#loc1493 = loc(callsite(#loc747 at #loc1389))
#loc1494 = loc(callsite(#loc752 at #loc1390))
#loc1495 = loc(callsite(#loc755 at #loc1390))
#loc1496 = loc(callsite(#loc724 at #loc1391))
#loc1497 = loc(callsite(#loc788 at #loc1392))
#loc1498 = loc(callsite(#loc738 at #loc1392))
#loc1499 = loc(callsite(#loc724 at #loc1393))
#loc1500 = loc(callsite(#loc733 at #loc1394))
#loc1501 = loc(callsite(#loc736 at #loc1395))
#loc1502 = loc(callsite(#loc738 at #loc1396))
#loc1503 = loc(callsite(#loc743 at #loc1397))
#loc1504 = loc(callsite(#loc743 at #loc1398))
#loc1505 = loc(callsite(#loc747 at #loc1399))
#loc1506 = loc(callsite(#loc752 at #loc1400))
#loc1507 = loc(callsite(#loc755 at #loc1400))
#loc1508 = loc(callsite(#loc820 at #loc1401))
#loc1509 = loc(callsite(#loc788 at #loc1402))
#loc1510 = loc(callsite(#loc738 at #loc1402))
#loc1511 = loc(callsite(#loc733 at #loc1403))
#loc1512 = loc(callsite(#loc736 at #loc1404))
#loc1513 = loc(callsite(#loc738 at #loc1405))
#loc1514 = loc(callsite(#loc743 at #loc1406))
#loc1515 = loc(callsite(#loc743 at #loc1407))
#loc1516 = loc(callsite(#loc788 at #loc1408))
#loc1517 = loc(callsite(#loc738 at #loc1408))
#loc1518 = loc(callsite(#loc733 at #loc1409))
#loc1519 = loc(callsite(#loc736 at #loc1410))
#loc1520 = loc(callsite(#loc738 at #loc1411))
#loc1521 = loc(callsite(#loc743 at #loc1412))
#loc1522 = loc(callsite(#loc743 at #loc1413))
#loc1523 = loc(callsite(#loc826 at #loc1414))
#loc1524 = loc(callsite(#loc827 at #loc1414))
#loc1525 = loc(callsite(#loc828 at #loc1414))
#loc1526 = loc(callsite(#loc829 at #loc1415))
#loc1527 = loc(callsite(#loc831 at #loc1414))
#loc1528 = loc(callsite(#loc832 at #loc1416))
#loc1529 = loc(callsite(#loc833 at #loc1417))
#loc1530 = loc(callsite(#loc835 at #loc1417))
#loc1531 = loc(callsite(#loc836 at #loc1417))
#loc1532 = loc(callsite(#loc837 at #loc1417))
#loc1533 = loc(callsite(#loc838 at #loc1418))
#loc1534 = loc(callsite(#loc840 at #loc1418))
#loc1535 = loc(callsite(#loc841 at #loc1418))
#loc1536 = loc(callsite(#loc842 at #loc1418))
#loc1537 = loc(callsite(#loc843 at #loc1419))
#loc1538 = loc(callsite(#loc844 at #loc1419))
#loc1539 = loc(callsite(#loc845 at #loc1419))
#loc1540 = loc(callsite(#loc846 at #loc1419))
#loc1541 = loc(callsite(#loc847 at #loc1419))
#loc1542 = loc(callsite(#loc848 at #loc1420))
#loc1543 = loc(callsite(#loc850 at #loc1420))
#loc1544 = loc(callsite(#loc851 at #loc1420))
#loc1545 = loc(callsite(#loc852 at #loc1421))
#loc1546 = loc(callsite(#loc854 at #loc1421))
#loc1547 = loc(callsite(#loc855 at #loc1421))
#loc1548 = loc(callsite(#loc856 at #loc1421))
#loc1549 = loc(callsite(#loc857 at #loc1421))
#loc1550 = loc(callsite(#loc858 at #loc1421))
#loc1551 = loc(callsite(#loc859 at #loc1421))
#loc1552 = loc(callsite(#loc860 at #loc1421))
#loc1553 = loc(callsite(#loc861 at #loc1421))
#loc1554 = loc(callsite(#loc862 at #loc1421))
#loc1555 = loc(callsite(#loc863 at #loc1421))
#loc1556 = loc(callsite(#loc864 at #loc1421))
#loc1557 = loc(callsite(#loc865 at #loc1421))
#loc1558 = loc(callsite(#loc866 at #loc1421))
#loc1559 = loc(callsite(#loc867 at #loc1422))
#loc1560 = loc(callsite(#loc752 at #loc1423))
#loc1561 = loc(callsite(#loc754 at #loc1423))
#loc1562 = loc(callsite(#loc755 at #loc1423))
#loc1563 = loc(callsite(#loc870 at #loc1421))
#loc1564 = loc(callsite(#loc871 at #loc1421))
#loc1565 = loc(callsite(#loc872 at #loc1421))
#loc1566 = loc(callsite(#loc873 at #loc1424))
#loc1567 = loc(callsite(#loc875 at #loc1425))
#loc1568 = loc(callsite(#loc876 at #loc1425))
#loc1569 = loc(callsite(#loc877 at #loc1421))
#loc1570 = loc(callsite(#loc878 at #loc1421))
#loc1571 = loc(callsite(#loc879 at #loc1421))
#loc1572 = loc(callsite(#loc880 at #loc1421))
#loc1573 = loc(callsite(#loc881 at #loc1421))
#loc1574 = loc(callsite(#loc882 at #loc1421))
#loc1575 = loc(callsite(#loc883 at #loc1426))
#loc1576 = loc(callsite(#loc884 at #loc1426))
#loc1577 = loc(callsite(#loc885 at #loc1427))
#loc1578 = loc(callsite(#loc887 at #loc1427))
#loc1580 = loc(callsite(#loc889 at #loc1427))
#loc1581 = loc(callsite(#loc890 at #loc1427))
#loc1582 = loc(callsite(#loc891 at #loc1427))
#loc1583 = loc(callsite(#loc892 at #loc1427))
#loc1584 = loc(callsite(#loc893 at #loc1427))
#loc1585 = loc(callsite(#loc894 at #loc1427))
#loc1586 = loc(callsite(#loc895 at #loc1427))
#loc1587 = loc(callsite(#loc896 at #loc1427))
#loc1588 = loc(callsite(#loc897 at #loc1427))
#loc1589 = loc(callsite(#loc898 at #loc1427))
#loc1590 = loc(callsite(#loc899 at #loc1427))
#loc1591 = loc(callsite(#loc900 at #loc1427))
#loc1592 = loc(callsite(#loc901 at #loc1427))
#loc1593 = loc(callsite(#loc902 at #loc1427))
#loc1594 = loc(callsite(#loc903 at #loc1427))
#loc1595 = loc(callsite(#loc904 at #loc1427))
#loc1596 = loc(callsite(#loc905 at #loc1427))
#loc1597 = loc(callsite(#loc906 at #loc1427))
#loc1598 = loc(callsite(#loc907 at #loc1427))
#loc1599 = loc(callsite(#loc908 at #loc1427))
#loc1600 = loc(callsite(#loc909 at #loc1427))
#loc1601 = loc(callsite(#loc910 at #loc1427))
#loc1602 = loc(callsite(#loc911 at #loc1427))
#loc1603 = loc(callsite(#loc912 at #loc1427))
#loc1604 = loc(callsite(#loc913 at #loc1427))
#loc1605 = loc(callsite(#loc914 at #loc1427))
#loc1606 = loc(callsite(#loc915 at #loc1427))
#loc1607 = loc(callsite(#loc916 at #loc1427))
#loc1608 = loc(callsite(#loc917 at #loc1427))
#loc1610 = loc(callsite(#loc919 at #loc1427))
#loc1611 = loc(callsite(#loc920 at #loc1427))
#loc1612 = loc(callsite(#loc921 at #loc1427))
#loc1613 = loc(callsite(#loc922 at #loc1427))
#loc1614 = loc(callsite(#loc923 at #loc1427))
#loc1615 = loc(callsite(#loc924 at #loc1427))
#loc1616 = loc(callsite(#loc925 at #loc1427))
#loc1617 = loc(callsite(#loc926 at #loc1427))
#loc1618 = loc(callsite(#loc927 at #loc1427))
#loc1619 = loc(callsite(#loc928 at #loc1427))
#loc1620 = loc(callsite(#loc929 at #loc1427))
#loc1621 = loc(callsite(#loc930 at #loc1427))
#loc1622 = loc(callsite(#loc931 at #loc1426))
#loc1623 = loc(callsite(#loc883 at #loc1428))
#loc1624 = loc(callsite(#loc884 at #loc1428))
#loc1625 = loc(callsite(#loc885 at #loc1429))
#loc1626 = loc(callsite(#loc887 at #loc1429))
#loc1627 = loc(callsite(#loc888 at #loc1429))
#loc1628 = loc(callsite(#loc889 at #loc1429))
#loc1629 = loc(callsite(#loc890 at #loc1429))
#loc1630 = loc(callsite(#loc891 at #loc1429))
#loc1631 = loc(callsite(#loc892 at #loc1429))
#loc1632 = loc(callsite(#loc893 at #loc1429))
#loc1633 = loc(callsite(#loc894 at #loc1429))
#loc1634 = loc(callsite(#loc895 at #loc1429))
#loc1635 = loc(callsite(#loc896 at #loc1429))
#loc1636 = loc(callsite(#loc897 at #loc1429))
#loc1637 = loc(callsite(#loc898 at #loc1429))
#loc1638 = loc(callsite(#loc899 at #loc1429))
#loc1639 = loc(callsite(#loc900 at #loc1429))
#loc1640 = loc(callsite(#loc901 at #loc1429))
#loc1641 = loc(callsite(#loc902 at #loc1429))
#loc1642 = loc(callsite(#loc903 at #loc1429))
#loc1643 = loc(callsite(#loc904 at #loc1429))
#loc1644 = loc(callsite(#loc905 at #loc1429))
#loc1645 = loc(callsite(#loc906 at #loc1429))
#loc1646 = loc(callsite(#loc907 at #loc1429))
#loc1647 = loc(callsite(#loc908 at #loc1429))
#loc1648 = loc(callsite(#loc909 at #loc1429))
#loc1649 = loc(callsite(#loc910 at #loc1429))
#loc1650 = loc(callsite(#loc911 at #loc1429))
#loc1651 = loc(callsite(#loc912 at #loc1429))
#loc1652 = loc(callsite(#loc913 at #loc1429))
#loc1653 = loc(callsite(#loc914 at #loc1429))
#loc1654 = loc(callsite(#loc915 at #loc1429))
#loc1655 = loc(callsite(#loc916 at #loc1429))
#loc1656 = loc(callsite(#loc917 at #loc1429))
#loc1657 = loc(callsite(#loc918 at #loc1429))
#loc1658 = loc(callsite(#loc919 at #loc1429))
#loc1659 = loc(callsite(#loc920 at #loc1429))
#loc1660 = loc(callsite(#loc921 at #loc1429))
#loc1661 = loc(callsite(#loc922 at #loc1429))
#loc1662 = loc(callsite(#loc923 at #loc1429))
#loc1663 = loc(callsite(#loc924 at #loc1429))
#loc1664 = loc(callsite(#loc925 at #loc1429))
#loc1665 = loc(callsite(#loc926 at #loc1429))
#loc1666 = loc(callsite(#loc927 at #loc1429))
#loc1667 = loc(callsite(#loc928 at #loc1429))
#loc1668 = loc(callsite(#loc929 at #loc1429))
#loc1669 = loc(callsite(#loc930 at #loc1429))
#loc1670 = loc(callsite(#loc931 at #loc1428))
#loc1671 = loc("jit(step_fun)/aten::embedding/jit(_take)"(#loc1430))
#loc1672 = loc("jit(step_fun)/convert_element_type"(#loc1431))
#loc1673 = loc("jit(step_fun)/aten::pow.Tensor_Scalar/pow"(#loc1432))
#loc1674 = loc("jit(step_fun)/aten::mean.dim/reduce_sum"(#loc1433))
#loc1675 = loc("jit(step_fun)/aten::mean.dim/broadcast_in_dim"(#loc1433))
#loc1676 = loc("jit(step_fun)/aten::mean.dim/div"(#loc1433))
#loc1677 = loc("jit(step_fun)/aten::add.Tensor/add"(#loc1434))
#loc1678 = loc("jit(step_fun)/aten::rsqrt/rsqrt"(#loc1435))
#loc1679 = loc("jit(step_fun)/aten::mul.Tensor/mul"(#loc1436))
#loc1680 = loc("jit(step_fun)/convert_element_type"(#loc1437))
#loc1681 = loc("jit(step_fun)/aten::mul.Tensor/broadcast_in_dim"(#loc1438))
#loc1682 = loc("jit(step_fun)/aten::mul.Tensor/mul"(#loc1438))
#loc1683 = loc("jit(step_fun)/QKVParallelLinear/mn,pn->mp/dot_general"(#loc1439))
#loc1684 = loc("jit(step_fun)/QKVParallelLinear/reshape"(#loc1440))
#loc1685 = loc("jit(step_fun)/QKVParallelLinear/slice"(#loc1441))
#loc1686 = loc("jit(step_fun)/QKVParallelLinear/reshape"(#loc1442))
#loc1687 = loc("jit(step_fun)/QKVParallelLinear/concatenate"(#loc1443))
#loc1688 = loc("jit(step_fun)/aten::split_with_sizes/slice"(#loc1444))
#loc1689 = loc("jit(step_fun)/aten::view/reshape"(#loc1445))
#loc1690 = loc("jit(step_fun)/aten::pow.Tensor_Scalar/pow"(#loc1446))
#loc1691 = loc("jit(step_fun)/aten::mean.dim/reduce_sum"(#loc1447))
#loc1692 = loc("jit(step_fun)/aten::mean.dim/broadcast_in_dim"(#loc1447))
#loc1693 = loc("jit(step_fun)/aten::mean.dim/div"(#loc1447))
#loc1694 = loc("jit(step_fun)/aten::add.Tensor/add"(#loc1448))
#loc1695 = loc("jit(step_fun)/aten::mul.Tensor/mul"(#loc1449))
#loc1696 = loc("jit(step_fun)/aten::mul.Tensor/broadcast_in_dim"(#loc1450))
#loc1697 = loc("jit(step_fun)/aten::mul.Tensor/mul"(#loc1450))
#loc1698 = loc("jit(step_fun)/aten::view/reshape"(#loc1451))
#loc1699 = loc("jit(step_fun)/aten::view/reshape"(#loc1452))
#loc1700 = loc("jit(step_fun)/aten::pow.Tensor_Scalar/pow"(#loc1453))
#loc1701 = loc("jit(step_fun)/aten::mean.dim/reduce_sum"(#loc1454))
#loc1702 = loc("jit(step_fun)/aten::mean.dim/broadcast_in_dim"(#loc1454))
#loc1703 = loc("jit(step_fun)/aten::mean.dim/div"(#loc1454))
#loc1704 = loc("jit(step_fun)/aten::add.Tensor/add"(#loc1455))
#loc1705 = loc("jit(step_fun)/aten::mul.Tensor/mul"(#loc1456))
#loc1706 = loc("jit(step_fun)/aten::mul.Tensor/broadcast_in_dim"(#loc1457))
#loc1707 = loc("jit(step_fun)/aten::mul.Tensor/mul"(#loc1457))
#loc1708 = loc("jit(step_fun)/aten::view/reshape"(#loc1458))
#loc1709 = loc("jit(step_fun)/__getitem__/lt"(#loc1459))
#loc1710 = loc("jit(step_fun)/__getitem__/add"(#loc1459))
#loc1711 = loc("jit(step_fun)/__getitem__/select_n"(#loc1459))
#loc1712 = loc("jit(step_fun)/__getitem__/broadcast_in_dim"(#loc1459))
#loc1713 = loc("jit(step_fun)/__getitem__/gather"(#loc1459))
#loc1714 = loc("jit(step_fun)/aten::split.Tensor/slice"(#loc1460))
#loc1715 = loc("jit(step_fun)/aten::view/reshape"(#loc1461))
#loc1716 = loc("jit(step_fun)/aten::unsqueeze/broadcast_in_dim"(#loc1462))
#loc1717 = loc("jit(step_fun)/aten::unsqueeze/broadcast_in_dim"(#loc1463))
#loc1718 = loc("jit(step_fun)/aten::split.Tensor/slice"(#loc1464))
#loc1719 = loc("jit(step_fun)/aten::mul.Tensor/mul"(#loc1465))
#loc1720 = loc("jit(step_fun)/aten::mul.Tensor/mul"(#loc1466))
#loc1721 = loc("jit(step_fun)/aten::sub.Tensor/mul"(#loc1467))
#loc1722 = loc("jit(step_fun)/aten::sub.Tensor/sub"(#loc1468))
#loc1723 = loc("jit(step_fun)/aten::mul.Tensor/mul"(#loc1469))
#loc1724 = loc("jit(step_fun)/aten::mul.Tensor/mul"(#loc1470))
#loc1725 = loc("jit(step_fun)/aten::add.Tensor/mul"(#loc1471))
#loc1726 = loc("jit(step_fun)/aten::add.Tensor/add"(#loc1472))
#loc1727 = loc("jit(step_fun)/aten::cat/concatenate"(#loc1473))
#loc1728 = loc("jit(step_fun)/aten::cat/slice"(#loc1474))
#loc1729 = loc("jit(step_fun)/aten::cat/concatenate"(#loc1475))
#loc1730 = loc("jit(step_fun)/aten::view/reshape"(#loc1476))
#loc1731 = loc("jit(step_fun)/aten::view/reshape"(#loc1477))
#loc1732 = loc("jit(step_fun)/aten::unsqueeze/broadcast_in_dim"(#loc1478))
#loc1733 = loc("jit(step_fun)/aten::unsqueeze/broadcast_in_dim"(#loc1479))
#loc1734 = loc("jit(step_fun)/aten::split.Tensor/slice"(#loc1480))
#loc1735 = loc("jit(step_fun)/aten::mul.Tensor/mul"(#loc1481))
#loc1736 = loc("jit(step_fun)/aten::mul.Tensor/mul"(#loc1482))
#loc1737 = loc("jit(step_fun)/aten::sub.Tensor/mul"(#loc1483))
#loc1738 = loc("jit(step_fun)/aten::sub.Tensor/sub"(#loc1484))
#loc1739 = loc("jit(step_fun)/aten::mul.Tensor/mul"(#loc1485))
#loc1740 = loc("jit(step_fun)/aten::mul.Tensor/mul"(#loc1486))
#loc1741 = loc("jit(step_fun)/aten::add.Tensor/mul"(#loc1487))
#loc1742 = loc("jit(step_fun)/aten::add.Tensor/add"(#loc1488))
#loc1743 = loc("jit(step_fun)/aten::cat/concatenate"(#loc1489))
#loc1744 = loc("jit(step_fun)/aten::cat/concatenate"(#loc1490))
#loc1745 = loc("jit(step_fun)/aten::view/reshape"(#loc1491))
#loc1746 = loc("jit(step_fun)/jit(_jax_attn_func)"(#loc1492))
#loc1747 = loc("jit(step_fun)/RowParallelLinear/mn,pn->mp/dot_general"(#loc1493))
#loc1748 = loc("jit(step_fun)/RowParallelLinear/reshape"(#loc1494))
#loc1749 = loc("jit(step_fun)/RowParallelLinear/reshape"(#loc1495))
#loc1750 = loc("jit(step_fun)/convert_element_type"(#loc1496))
#loc1751 = loc("jit(step_fun)/aten::add.Tensor/mul"(#loc1497))
#loc1752 = loc("jit(step_fun)/aten::add.Tensor/add"(#loc1498))
#loc1753 = loc("jit(step_fun)/convert_element_type"(#loc1499))
#loc1754 = loc("jit(step_fun)/aten::pow.Tensor_Scalar/pow"(#loc1500))
#loc1755 = loc("jit(step_fun)/aten::mean.dim/reduce_sum"(#loc1501))
#loc1756 = loc("jit(step_fun)/aten::mean.dim/broadcast_in_dim"(#loc1501))
#loc1757 = loc("jit(step_fun)/aten::mean.dim/div"(#loc1501))
#loc1758 = loc("jit(step_fun)/aten::add.Tensor/add"(#loc1502))
#loc1759 = loc("jit(step_fun)/aten::mul.Tensor/mul"(#loc1503))
#loc1760 = loc("jit(step_fun)/aten::mul.Tensor/broadcast_in_dim"(#loc1504))
#loc1761 = loc("jit(step_fun)/aten::mul.Tensor/mul"(#loc1504))
#loc1762 = loc("jit(step_fun)/ReplicatedLinear/mn,pn->mp/dot_general"(#loc1505))
#loc1763 = loc("jit(step_fun)/ReplicatedLinear/reshape"(#loc1506))
#loc1764 = loc("jit(step_fun)/ReplicatedLinear/reshape"(#loc1507))
#loc1765 = loc("jit(step_fun)/jit(jax_fused_moe_func_padded)"(#loc1508))
#loc1766 = loc("jit(step_fun)/aten::add.Tensor/mul"(#loc1509))
#loc1767 = loc("jit(step_fun)/aten::add.Tensor/add"(#loc1510))
#loc1768 = loc("jit(step_fun)/aten::pow.Tensor_Scalar/pow"(#loc1511))
#loc1769 = loc("jit(step_fun)/aten::mean.dim/reduce_sum"(#loc1512))
#loc1770 = loc("jit(step_fun)/aten::mean.dim/broadcast_in_dim"(#loc1512))
#loc1771 = loc("jit(step_fun)/aten::mean.dim/div"(#loc1512))
#loc1772 = loc("jit(step_fun)/aten::add.Tensor/add"(#loc1513))
#loc1773 = loc("jit(step_fun)/aten::mul.Tensor/mul"(#loc1514))
#loc1774 = loc("jit(step_fun)/aten::mul.Tensor/broadcast_in_dim"(#loc1515))
#loc1775 = loc("jit(step_fun)/aten::mul.Tensor/mul"(#loc1515))
#loc1776 = loc("jit(step_fun)/aten::add.Tensor/mul"(#loc1516))
#loc1777 = loc("jit(step_fun)/aten::add.Tensor/add"(#loc1517))
#loc1778 = loc("jit(step_fun)/aten::pow.Tensor_Scalar/pow"(#loc1518))
#loc1779 = loc("jit(step_fun)/aten::mean.dim/reduce_sum"(#loc1519))
#loc1780 = loc("jit(step_fun)/aten::mean.dim/broadcast_in_dim"(#loc1519))
#loc1781 = loc("jit(step_fun)/aten::mean.dim/div"(#loc1519))
#loc1782 = loc("jit(step_fun)/aten::add.Tensor/add"(#loc1520))
#loc1783 = loc("jit(step_fun)/aten::mul.Tensor/mul"(#loc1521))
#loc1784 = loc("jit(step_fun)/aten::mul.Tensor/broadcast_in_dim"(#loc1522))
#loc1785 = loc("jit(step_fun)/aten::mul.Tensor/mul"(#loc1522))
#loc1786 = loc("jit(step_fun)/aten::embedding/jit"(#loc1430))
#loc1787 = loc("gather"(#loc1430))
#loc1788 = loc("reduce_and"(#loc1430))
#loc1789 = loc("lt"(#loc1430))
#loc1790 = loc("add"(#loc1430))
#loc1791 = loc("jit(_where)"(#loc1430))
#loc1792 = loc("broadcast_in_dim"(#loc1430))
#loc1793 = loc("ge"(#loc1430))
#loc1794 = loc("le"(#loc1430))
#loc1795 = loc("and"(#loc1430))
#loc1796 = loc("select_n"(#loc1430))
#loc1797 = loc("jit"(#loc1430))
#loc1798 = loc("jit(step_fun)/jit"(#loc1492))
#loc1799 = loc("reshape"(#loc1523))
#loc1800 = loc("reshape"(#loc1524))
#loc1801 = loc("reshape"(#loc1525))
#loc1802 = loc("jit(_ragged_paged_attention)"(#loc1526))
#loc1803 = loc("reshape"(#loc1527))
#loc1804 = loc("jit"(#loc1526))
#loc1805 = loc("shard_map"(#loc1526))
#loc1806 = loc("jit(ragged_paged_attention)"(#loc1528))
#loc1807 = loc("jit"(#loc1528))
#loc1808 = loc("reshape"(#loc1529))
#loc1809 = loc("jit(_pad)"(#loc1530))
#loc1810 = loc("reshape"(#loc1531))
#loc1811 = loc("transpose"(#loc1532))
#loc1812 = loc("concatenate"(#loc1533))
#loc1813 = loc("reshape"(#loc1534))
#loc1814 = loc("jit(_pad)"(#loc1535))
#loc1815 = loc("reshape"(#loc1536))
#loc1816 = loc("slice"(#loc1537))
#loc1817 = loc("squeeze"(#loc1537))
#loc1818 = loc("broadcast_in_dim"(#loc1538))
#loc1819 = loc("broadcast_in_dim"(#loc1539))
#loc1820 = loc("broadcast_in_dim"(#loc1540))
#loc1821 = loc("RPA-bq_16-bkvp_64-p_32/pallas_call"(#loc1541))
#loc1822 = loc("transpose"(#loc1542))
#loc1823 = loc("reshape"(#loc1543))
#loc1824 = loc("reshape"(#loc1544))
#loc1825 = loc("jit"(#loc1530))
#loc1826 = loc("convert_element_type"(#loc1530))
#loc1827 = loc("pad"(#loc1530))
#loc1828 = loc("jit"(#loc1535))
#loc1829 = loc("convert_element_type"(#loc1535))
#loc1830 = loc("pad"(#loc1535))
#loc1831 = loc("jit(step_fun)/jit"(#loc1508))
#loc1832 = loc("convert_element_type"(#loc1545))
#loc1833 = loc("reduce_max"(#loc1546))
#loc1834 = loc("max"(#loc1546))
#loc1835 = loc("broadcast_in_dim"(#loc1546))
#loc1836 = loc("sub"(#loc1546))
#loc1837 = loc("exp"(#loc1546))
#loc1838 = loc("reduce_sum"(#loc1546))
#loc1839 = loc("div"(#loc1546))
#loc1840 = loc("top_k"(#loc1547))
#loc1841 = loc("reduce_sum"(#loc1548))
#loc1842 = loc("broadcast_in_dim"(#loc1548))
#loc1843 = loc("div"(#loc1549))
#loc1844 = loc("convert_element_type"(#loc1550))
#loc1845 = loc("reshape"(#loc1551))
#loc1846 = loc("jit(argsort)"(#loc1552))
#loc1847 = loc("jit(argsort)"(#loc1553))
#loc1848 = loc("iota"(#loc1554))
#loc1849 = loc("broadcast_in_dim"(#loc1555))
#loc1850 = loc("reshape"(#loc1555))
#loc1851 = loc("lt"(#loc1556))
#loc1852 = loc("add"(#loc1556))
#loc1853 = loc("select_n"(#loc1556))
#loc1854 = loc("broadcast_in_dim"(#loc1556))
#loc1855 = loc("gather"(#loc1556))
#loc1856 = loc("broadcast_in_dim"(#loc1557))
#loc1857 = loc("jit(clip)"(#loc1557))
#loc1858 = loc("lt"(#loc1557))
#loc1859 = loc("add"(#loc1557))
#loc1860 = loc("select_n"(#loc1557))
#loc1861 = loc("scatter-add"(#loc1557))
#loc1862 = loc("lt"(#loc1558))
#loc1863 = loc("add"(#loc1558))
#loc1864 = loc("select_n"(#loc1558))
#loc1865 = loc("broadcast_in_dim"(#loc1558))
#loc1866 = loc("gather"(#loc1558))
#loc1867 = loc("shard_map"(#loc1559))
#loc1868 = loc("jit(gmm)"(#loc1559))
#loc1869 = loc("reshape"(#loc1560))
#loc1870 = loc("slice"(#loc1561))
#loc1871 = loc("reshape"(#loc1562))
#loc1872 = loc("jit(silu)"(#loc1563))
#loc1873 = loc("mul"(#loc1564))
#loc1874 = loc("sharding_constraint"(#loc1565))
#loc1875 = loc("shard_map"(#loc1566))
#loc1876 = loc("jit(gmm)"(#loc1567))
#loc1877 = loc("psum"(#loc1568))
#loc1878 = loc("add"(#loc1568))
#loc1879 = loc("lt"(#loc1569))
#loc1880 = loc("add"(#loc1569))
#loc1881 = loc("select_n"(#loc1569))
#loc1882 = loc("broadcast_in_dim"(#loc1569))
#loc1883 = loc("gather"(#loc1569))
#loc1884 = loc("reshape"(#loc1570))
#loc1885 = loc("broadcast_in_dim"(#loc1571))
#loc1886 = loc("mul"(#loc1572))
#loc1887 = loc("convert_element_type"(#loc1573))
#loc1888 = loc("reduce_sum"(#loc1573))
#loc1889 = loc("sharding_constraint"(#loc1574))
#loc1890 = loc("jit"(#loc1552))
#loc1891 = loc("iota"(#loc1552))
#loc1892 = loc("sort"(#loc1552))
#loc1893 = loc("lt_to"(#loc1552))
#loc1894 = loc("jit"(#loc1553))
#loc1895 = loc("iota"(#loc1553))
#loc1896 = loc("sort"(#loc1553))
#loc1897 = loc("lt_to"(#loc1553))
#loc1898 = loc("jit"(#loc1557))
#loc1899 = loc("convert_element_type"(#loc1557))
#loc1900 = loc("max"(#loc1557))
#loc1901 = loc("jit"(#loc1559))
#loc1902 = loc("broadcast_in_dim"(#loc1575))
#loc1903 = loc("slice"(#loc1576))
#loc1904 = loc("squeeze"(#loc1576))
#loc1905 = loc("add"(#loc1577))
#loc1906 = loc("sub"(#loc1578))
#loc1907 = loc("jit(cumsum)"(#loc1579))
#loc1908 = loc("broadcast_in_dim"(#loc1580))
#loc1909 = loc("concatenate"(#loc1581))
#loc1910 = loc("add"(#loc1582))
#loc1911 = loc("sub"(#loc1583))
#loc1912 = loc("jit(floor_divide)"(#loc1584))
#loc1913 = loc("mul"(#loc1585))
#loc1914 = loc("broadcast_in_dim"(#loc1586))
#loc1915 = loc("slice"(#loc1587))
#loc1916 = loc("concatenate"(#loc1588))
#loc1917 = loc("jit(floor_divide)"(#loc1589))
#loc1918 = loc("mul"(#loc1590))
#loc1919 = loc("sub"(#loc1591))
#loc1920 = loc("eq"(#loc1592))
#loc1921 = loc("jit(_where)"(#loc1593))
#loc1922 = loc("jit(floor_divide)"(#loc1594))
#loc1923 = loc("iota"(#loc1595))
#loc1924 = loc("jit(_roll_static)"(#loc1596))
#loc1925 = loc("broadcast_in_dim"(#loc1596))
#loc1926 = loc("scatter"(#loc1596))
#loc1927 = loc("jit(cumsum)"(#loc1596))
#loc1928 = loc("lt"(#loc1596))
#loc1929 = loc("add"(#loc1596))
#loc1930 = loc("select_n"(#loc1596))
#loc1931 = loc("scatter-add"(#loc1596))
#loc1932 = loc("sub"(#loc1596))
#loc1933 = loc("jit(_take)"(#loc1596))
#loc1934 = loc("slice"(#loc1597))
#loc1935 = loc("jit(remainder)"(#loc1598))
#loc1936 = loc("eq"(#loc1599))
#loc1937 = loc("eq"(#loc1600))
#loc1938 = loc("or"(#loc1601))
#loc1939 = loc("slice"(#loc1602))
#loc1940 = loc("jit(floor_divide)"(#loc1603))
#loc1941 = loc("jit(_where)"(#loc1604))
#loc1942 = loc("convert_element_type"(#loc1605))
#loc1943 = loc("broadcast_in_dim"(#loc1605))
#loc1944 = loc("jit(_ptp)"(#loc1605))
#loc1945 = loc("eq"(#loc1605))
#loc1946 = loc("slice"(#loc1605))
#loc1947 = loc("squeeze"(#loc1605))
#loc1948 = loc("sub"(#loc1605))
#loc1949 = loc("jit(_where)"(#loc1605))
#loc1950 = loc("add"(#loc1605))
#loc1951 = loc("jit(_linspace)"(#loc1605))
#loc1952 = loc("jit(searchsorted)"(#loc1605))
#loc1953 = loc("dynamic_slice"(#loc1605))
#loc1954 = loc("lt"(#loc1605))
#loc1955 = loc("select_n"(#loc1605))
#loc1956 = loc("scatter-add"(#loc1605))
#loc1957 = loc("add"(#loc1606))
#loc1958 = loc("iota"(#loc1607))
#loc1959 = loc("convert_element_type"(#loc1608))
#loc1960 = loc("jit(_roll_static)"(#loc1609))
#loc1961 = loc("broadcast_in_dim"(#loc1609))
#loc1962 = loc("scatter"(#loc1609))
#loc1963 = loc("jit(cumsum)"(#loc1609))
#loc1964 = loc("lt"(#loc1609))
#loc1965 = loc("add"(#loc1609))
#loc1966 = loc("select_n"(#loc1609))
#loc1967 = loc("scatter-add"(#loc1609))
#loc1968 = loc("sub"(#loc1609))
#loc1969 = loc("jit(_take)"(#loc1609))
#loc1970 = loc("convert_element_type"(#loc1610))
#loc1971 = loc("lt"(#loc1610))
#loc1972 = loc("convert_element_type"(#loc1611))
#loc1973 = loc("reduce_sum"(#loc1611))
#loc1974 = loc("neg"(#loc1612))
#loc1975 = loc("jit(_roll_dynamic)"(#loc1613))
#loc1976 = loc("neg"(#loc1614))
#loc1977 = loc("jit(_roll_dynamic)"(#loc1615))
#loc1978 = loc("iota"(#loc1616))
#loc1979 = loc("convert_element_type"(#loc1617))
#loc1980 = loc("le"(#loc1617))
#loc1981 = loc("convert_element_type"(#loc1618))
#loc1982 = loc("ge"(#loc1618))
#loc1983 = loc("and"(#loc1619))
#loc1984 = loc("jit(_where)"(#loc1620))
#loc1985 = loc("reduce_sum"(#loc1621))
#loc1986 = loc("pallas_call"(#loc1622))
#loc1987 = loc("jit"(#loc1579))
#loc1988 = loc("reduce_window_sum"(#loc1579))
#loc1989 = loc("jit"(#loc1584))
#loc1990 = loc("convert_element_type"(#loc1584))
#loc1991 = loc("div"(#loc1584))
#loc1992 = loc("sign"(#loc1584))
#loc1993 = loc("ne"(#loc1584))
#loc1994 = loc("rem"(#loc1584))
#loc1995 = loc("and"(#loc1584))
#loc1996 = loc("sub"(#loc1584))
#loc1997 = loc("jit(_where)"(#loc1584))
#loc1998 = loc("select_n"(#loc1584))
#loc1999 = loc("jit"(#loc1593))
#loc2000 = loc("convert_element_type"(#loc1593))
#loc2001 = loc("broadcast_in_dim"(#loc1593))
#loc2002 = loc("select_n"(#loc1593))
#loc2003 = loc("jit"(#loc1596))
#loc2004 = loc("slice"(#loc1596))
#loc2005 = loc("concatenate"(#loc1596))
#loc2006 = loc("gather"(#loc1596))
#loc2007 = loc("reduce_and"(#loc1596))
#loc2008 = loc("jit(_where)"(#loc1596))
#loc2009 = loc("ge"(#loc1596))
#loc2010 = loc("le"(#loc1596))
#loc2011 = loc("and"(#loc1596))
#loc2012 = loc("jit"(#loc1598))
#loc2013 = loc("convert_element_type"(#loc1598))
#loc2014 = loc("eq"(#loc1598))
#loc2015 = loc("broadcast_in_dim"(#loc1598))
#loc2016 = loc("jit(_where)"(#loc1598))
#loc2017 = loc("rem"(#loc1598))
#loc2018 = loc("ne"(#loc1598))
#loc2019 = loc("lt"(#loc1598))
#loc2020 = loc("and"(#loc1598))
#loc2021 = loc("add"(#loc1598))
#loc2022 = loc("select_n"(#loc1598))
#loc2023 = loc("jit"(#loc1605))
#loc2024 = loc("reduce_min"(#loc1605))
#loc2025 = loc("reduce_max"(#loc1605))
#loc2026 = loc("iota"(#loc1605))
#loc2027 = loc("div"(#loc1605))
#loc2028 = loc("reshape"(#loc1605))
#loc2029 = loc("mul"(#loc1605))
#loc2030 = loc("concatenate"(#loc1605))
#loc2031 = loc("vmap()/broadcast_in_dim"(#loc1605))
#loc2032 = loc("vmap()/while"(#loc1605))
#loc2033 = loc("vmap()/while/cond/lt"(#loc1605))
#loc2034 = loc("vmap()/while/body/closed_call"(#loc1605))
#loc2035 = loc("vmap()/while/body/add"(#loc1605))
#loc2036 = loc("while/body/closed_call"(#loc1605))
#loc2037 = loc("gather"(#loc1605))
#loc2038 = loc("ne"(#loc1605))
#loc2039 = loc("lt_to"(#loc1605))
#loc2040 = loc("jit"(#loc1609))
#loc2041 = loc("slice"(#loc1609))
#loc2042 = loc("concatenate"(#loc1609))
#loc2043 = loc("reduce_window_sum"(#loc1609))
#loc2044 = loc("gather"(#loc1609))
#loc2045 = loc("reduce_and"(#loc1609))
#loc2046 = loc("jit(_where)"(#loc1609))
#loc2047 = loc("ge"(#loc1609))
#loc2048 = loc("le"(#loc1609))
#loc2049 = loc("and"(#loc1609))
#loc2050 = loc("jit"(#loc1613))
#loc2051 = loc("broadcast_in_dim"(#loc1613))
#loc2052 = loc("slice"(#loc1613))
#loc2053 = loc("squeeze"(#loc1613))
#loc2054 = loc("max"(#loc1613))
#loc2055 = loc("jit(remainder)"(#loc1613))
#loc2056 = loc("concatenate"(#loc1613))
#loc2057 = loc("sub"(#loc1613))
#loc2058 = loc("lt"(#loc1613))
#loc2059 = loc("add"(#loc1613))
#loc2060 = loc("select_n"(#loc1613))
#loc2061 = loc("dynamic_slice"(#loc1613))
#loc2062 = loc("eq"(#loc1613))
#loc2063 = loc("jit(_where)"(#loc1613))
#loc2064 = loc("rem"(#loc1613))
#loc2065 = loc("ne"(#loc1613))
#loc2066 = loc("and"(#loc1613))
#loc2067 = loc("jit"(#loc1620))
#loc2068 = loc("convert_element_type"(#loc1620))
#loc2069 = loc("broadcast_in_dim"(#loc1620))
#loc2070 = loc("select_n"(#loc1620))
#loc2071 = loc("jit"(#loc1563))
#loc2072 = loc("neg"(#loc1563))
#loc2073 = loc("exp"(#loc1563))
#loc2074 = loc("add"(#loc1563))
#loc2075 = loc("div"(#loc1563))
#loc2076 = loc("mul"(#loc1563))
#loc2077 = loc("jit"(#loc1567))
#loc2078 = loc("broadcast_in_dim"(#loc1623))
#loc2079 = loc("slice"(#loc1624))
#loc2080 = loc("squeeze"(#loc1624))
#loc2081 = loc("add"(#loc1625))
#loc2082 = loc("sub"(#loc1626))
#loc2083 = loc("jit(cumsum)"(#loc1627))
#loc2084 = loc("broadcast_in_dim"(#loc1628))
#loc2085 = loc("concatenate"(#loc1629))
#loc2086 = loc("add"(#loc1630))
#loc2087 = loc("sub"(#loc1631))
#loc2088 = loc("jit(floor_divide)"(#loc1632))
#loc2089 = loc("mul"(#loc1633))
#loc2090 = loc("broadcast_in_dim"(#loc1634))
#loc2091 = loc("slice"(#loc1635))
#loc2092 = loc("concatenate"(#loc1636))
#loc2093 = loc("jit(floor_divide)"(#loc1637))
#loc2094 = loc("mul"(#loc1638))
#loc2095 = loc("sub"(#loc1639))
#loc2096 = loc("eq"(#loc1640))
#loc2097 = loc("jit(_where)"(#loc1641))
#loc2098 = loc("jit(floor_divide)"(#loc1642))
#loc2099 = loc("iota"(#loc1643))
#loc2100 = loc("jit(_roll_static)"(#loc1644))
#loc2101 = loc("broadcast_in_dim"(#loc1644))
#loc2102 = loc("scatter"(#loc1644))
#loc2103 = loc("jit(cumsum)"(#loc1644))
#loc2104 = loc("lt"(#loc1644))
#loc2105 = loc("add"(#loc1644))
#loc2106 = loc("select_n"(#loc1644))
#loc2107 = loc("scatter-add"(#loc1644))
#loc2108 = loc("sub"(#loc1644))
#loc2109 = loc("jit(_take)"(#loc1644))
#loc2110 = loc("slice"(#loc1645))
#loc2111 = loc("jit(remainder)"(#loc1646))
#loc2112 = loc("eq"(#loc1647))
#loc2113 = loc("eq"(#loc1648))
#loc2114 = loc("or"(#loc1649))
#loc2115 = loc("slice"(#loc1650))
#loc2116 = loc("jit(floor_divide)"(#loc1651))
#loc2117 = loc("jit(_where)"(#loc1652))
#loc2118 = loc("convert_element_type"(#loc1653))
#loc2119 = loc("broadcast_in_dim"(#loc1653))
#loc2120 = loc("jit(_ptp)"(#loc1653))
#loc2121 = loc("eq"(#loc1653))
#loc2122 = loc("slice"(#loc1653))
#loc2123 = loc("squeeze"(#loc1653))
#loc2124 = loc("sub"(#loc1653))
#loc2125 = loc("jit(_where)"(#loc1653))
#loc2126 = loc("add"(#loc1653))
#loc2127 = loc("jit(_linspace)"(#loc1653))
#loc2128 = loc("jit(searchsorted)"(#loc1653))
#loc2129 = loc("dynamic_slice"(#loc1653))
#loc2130 = loc("lt"(#loc1653))
#loc2131 = loc("select_n"(#loc1653))
#loc2132 = loc("scatter-add"(#loc1653))
#loc2133 = loc("add"(#loc1654))
#loc2134 = loc("iota"(#loc1655))
#loc2135 = loc("convert_element_type"(#loc1656))
#loc2136 = loc("jit(_roll_static)"(#loc1657))
#loc2137 = loc("broadcast_in_dim"(#loc1657))
#loc2138 = loc("scatter"(#loc1657))
#loc2139 = loc("jit(cumsum)"(#loc1657))
#loc2140 = loc("lt"(#loc1657))
#loc2141 = loc("add"(#loc1657))
#loc2142 = loc("select_n"(#loc1657))
#loc2143 = loc("scatter-add"(#loc1657))
#loc2144 = loc("sub"(#loc1657))
#loc2145 = loc("jit(_take)"(#loc1657))
#loc2146 = loc("convert_element_type"(#loc1658))
#loc2147 = loc("lt"(#loc1658))
#loc2148 = loc("convert_element_type"(#loc1659))
#loc2149 = loc("reduce_sum"(#loc1659))
#loc2150 = loc("neg"(#loc1660))
#loc2151 = loc("jit(_roll_dynamic)"(#loc1661))
#loc2152 = loc("neg"(#loc1662))
#loc2153 = loc("jit(_roll_dynamic)"(#loc1663))
#loc2154 = loc("iota"(#loc1664))
#loc2155 = loc("convert_element_type"(#loc1665))
#loc2156 = loc("le"(#loc1665))
#loc2157 = loc("convert_element_type"(#loc1666))
#loc2158 = loc("ge"(#loc1666))
#loc2159 = loc("and"(#loc1667))
#loc2160 = loc("jit(_where)"(#loc1668))
#loc2161 = loc("reduce_sum"(#loc1669))
#loc2162 = loc("pallas_call"(#loc1670))
#loc2163 = loc("jit:"(#loc1786))
#loc2164 = loc("jit:"(#loc1797))
#loc2165 = loc("jit:"(#loc1798))
#loc2166 = loc("jit:"(#loc1804))
#loc2167 = loc("jit:"(#loc1807))
#loc2168 = loc("jit:"(#loc1825))
#loc2169 = loc("jit:"(#loc1828))
#loc2170 = loc("jit:"(#loc1831))
#loc2171 = loc("jit:"(#loc1890))
#loc2172 = loc("jit:"(#loc1894))
#loc2173 = loc("jit:"(#loc1898))
#loc2174 = loc("jit:"(#loc1901))
#loc2175 = loc("jit:"(#loc1987))
#loc2176 = loc("jit:"(#loc1989))
#loc2177 = loc("jit:"(#loc1999))
#loc2178 = loc("jit:"(#loc2003))
#loc2179 = loc("jit:"(#loc2012))
#loc2180 = loc("jit:"(#loc2023))
#loc2181 = loc("closed_call:"(#loc2036))
#loc2182 = loc("jit:"(#loc2040))
#loc2183 = loc("jit:"(#loc2050))
#loc2184 = loc("jit:"(#loc2067))
#loc2185 = loc("jit:"(#loc2071))
#loc2186 = loc("jit:"(#loc2077))
